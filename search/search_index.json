{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenue !","text":"<p>Je m'appelle Anas, je suis consultant data scientist sp\u00e9cialis\u00e9 en NLP et l'IA g\u00e9n\u00e9rative. Mon travail consiste \u00e0 apporter des solutions concr\u00e8tes \u00e0 des probl\u00e9matiques m\u00e9tier en utilisant l'IA. Car je suis convaincu que l'IA peut apporter une v\u00e9ritable valeur ajout\u00e9e \u00e0 chaque profession. </p> <p>Mon objectif au quotidien est d'accompagner les entreprises afin qu'elles exploitent pleinement le potentiel de leurs donn\u00e9es pour :</p> <ul> <li>Am\u00e9liorer les prises de d\u00e9cisions strat\u00e9giques.</li> <li>Optimiser les processus op\u00e9rationnels.</li> <li>Augmenter la productivit\u00e9 et la rentabilit\u00e9.</li> </ul> <p>Partager mes connaissances est une vraie passion, que ce soit via des formations, des audits ou des articles de blog. Mes contenus visent \u00e0 vulgariser l\u2019IA, proposer des solutions concr\u00e8tes et partager des retours d\u2019exp\u00e9rience pour faciliter l\u2019int\u00e9gration de l\u2019intelligence artificielle au quotidien.</p> <p>Lisez mes articles en suivant le lien ci-dessous : Mes articles de blog</p> <p>Voici quelques exemples de projets r\u00e9alis\u00e9s et des probl\u00e9matiques auxquelles j'ai pu apporter des solutions :</p> <ul> <li> <p>Agent IA pour l'\u00e9criture de r\u00e9ponses \u00e0 des appels d'offres : Cet agent a automatis\u00e9 la r\u00e9daction des r\u00e9ponses, r\u00e9duisant consid\u00e9rablement le temps n\u00e9cessaire. R\u00e9sultat : 75 % de temps \u00e9conomis\u00e9 avec un retour sur investissement (ROI) de 300 %.</p> </li> <li> <p>Assistant IA pour un site d'e-commerce : Cet assistant a permis de r\u00e9pondre aux questions sur les produits, facilitant ainsi la vente. R\u00e9sultat : Am\u00e9lioration de la qualit\u00e9 de service client.</p> </li> <li> <p>Agent IA pour l'extraction de donn\u00e9es : D\u00e9velopp\u00e9 pour extraire des donn\u00e9es de diff\u00e9rents types de documents avec une grande pr\u00e9cision. R\u00e9sultat : le temps de traitement a \u00e9t\u00e9 divis\u00e9 par deux.</p> </li> <li> <p>Assistant IA pour une usine de production : Aide \u00e0 identifier et r\u00e9soudre les erreurs affich\u00e9es sur les machines. R\u00e9sultat : 60% de temps \u00e9conomis\u00e9.</p> </li> <li> <p>Agent IA pour l'\u00e9criture de tests unitaires : Automatisation de la cr\u00e9ation de tests unitaires \u00e0 partir du code source.</p> </li> <li> <p>IA souveraine sp\u00e9cialis\u00e9e dans les donn\u00e9es spatiales : Mise en place d'un RAG (Retrieval-Augmented Generation) sur des donn\u00e9es confidentielles avec des LLMs souverains.</p> </li> <li> <p>Agents IA personnalis\u00e9s sur site web : Cr\u00e9ation d'un outil qui permet de faciliter le support client (https://heeya.fr/)</p> </li> </ul> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou juste envie de discuter de vos propres cas d'usages, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime bien \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter"},{"location":"blog/","title":"Blog","text":"<p>Bienvenue sur mon blog !</p> <p>J'essaye de partager r\u00e9guli\u00e8rement mes r\u00e9flexions, mes d\u00e9couvertes et mes exp\u00e9riences autour de l'intelligence artificielle, un domaine passionnant avec beaucoup de choses \u00e0 d\u00e9couvrir.</p> <p>Pourquoi je me suis lanc\u00e9 dans ce blog ? En tant que consultant data scientist sp\u00e9cialis\u00e9 en IA, je suis r\u00e9guli\u00e8rement confront\u00e9 \u00e0 des probl\u00e9matiques m\u00e9tier que je r\u00e9sous en utilisant l'intelligence artificielle. Dans ce processus, j'apprends beaucoup sur les enjeux m\u00e9tier, sur la mani\u00e8re d\u2019int\u00e9grer l\u2019IA de fa\u00e7on pertinente selon chaque contexte, et sur comment rendre l\u2019IA plus accessible \u00e0 tous.</p> <p>Mon objectif \u00e0 travers mes articles est de vulgariser des concepts IA, proposer des solutions techniques concr\u00e8tes, partager des retours d'exp\u00e9rience issus de mes projets, et donner des conseils pratiques pour int\u00e9grer l'intelligence artificielle dans votre quotidien professionnel.</p> <p>Bonne lecture, et n'h\u00e9sitez pas \u00e0 \u00e9changer avec moi : anas0rabhi@gmail.com !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["Agents","RAG","Intelligence artificielle","blog","IA"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/","title":"Comment am\u00e9liorer le RAG","text":"<p>Souvent, pour am\u00e9liorer une application d'IA comme un RAG ou un agent, il est plus judicieux de se concentrer sur l'analyse fine des erreurs plut\u00f4t que de c\u00e9der \u00e0 la tentation d'ajouter syst\u00e9matiquement de nouveaux outils. Voyons pourquoi cette approche pragmatique est souvent la plus efficace.</p>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#la-course-aux-outils-une-fausse-bonne-idee","title":"La course aux outils : Une fausse bonne id\u00e9e ?","text":"<p>Lorsqu'on cherche \u00e0 am\u00e9liorer une application IA, qu'il s'agisse d'un syst\u00e8me RAG (Retrieval-Augmented Generation) ou d'un agent conversationnel plus complexe, l'\u00e9cosyst\u00e8me technologique nous pr\u00e9sente une multitude d'outils. Frameworks, bases de donn\u00e9es vectorielles, mod\u00e8les d'embedding, techniques de r\u00e9\u00e9criture de prompt... chacun promettant d'am\u00e9liorer significativement les performances.</p> <p>Pourtant, c\u00e9der \u00e0 cette \"course aux outils\" sans une compr\u00e9hension claire du probl\u00e8me \u00e0 r\u00e9soudre peut s'av\u00e9rer contre-productif. L'approche la plus pragmatique, et souvent la plus efficace sur le long terme, repose moins sur l'accumulation de nouvelles briques technologiques que sur une analyse rigoureuse des erreurs et l'am\u00e9lioration continue de l'architecture existante.</p>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#decortiquer-les-erreurs-dun-rag-ou-chercher","title":"D\u00e9cortiquer les erreurs d'un RAG : Ou chercher ?","text":"<p>Prenons l'exemple concret d'un syst\u00e8me RAG, tr\u00e8s populaire aujourd'hui. Lorsqu'il fournit une r\u00e9ponse incorrecte ou d\u00e9cevante, les causes potentielles sont nombreuses et vari\u00e9es :</p> <ol> <li>Le LLM lui-m\u00eame : Le mod\u00e8le de langage peut \"halluciner\", c'est-\u00e0-dire inventer des informations qui ne sont pas pr\u00e9sentes dans les documents r\u00e9cup\u00e9r\u00e9s.</li> <li>L'\u00e9tape de r\u00e9cup\u00e9ration (Retrieve) : Les documents pertinents ne sont pas correctement identifi\u00e9s et remont\u00e9s. Le probl\u00e8me peut venir du mod\u00e8le d'embedding utilis\u00e9, de la requ\u00eate de recherche, ou de l'indexation.</li> <li>Le d\u00e9coupage (Chunking) : La mani\u00e8re dont les documents originaux sont segment\u00e9s en morceaux (chunks) peut \u00eatre inadapt\u00e9e, coupant des informations importantes ou ne fournissant pas assez de contexte.</li> <li>Le pr\u00e9-traitement des donn\u00e9es : La qualit\u00e9 initiale des documents (nettoyage, formatage) peut impacter l'ensemble du processus.</li> </ol> <p>Face \u00e0 une erreur, ajouter un nouvel outil (par exemple, un module de \"re-ranking\") sans avoir identifi\u00e9 laquelle de ces \u00e9tapes est d\u00e9faillante revient souvent \u00e0 ajouter de la complexit\u00e9 inutile, voire \u00e0 masquer le probl\u00e8me initial sans le r\u00e9soudre.</p>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#levaluation-la-cle-pour-comprendre-et-cibler","title":"L'\u00c9valuation : La cl\u00e9 pour comprendre et cibler","text":"<p>Identifier pr\u00e9cis\u00e9ment lequel de ces \u00e9l\u00e9ments est en cause est donc crucial pour une am\u00e9lioration efficace. C'est l\u00e0 qu'intervient l'\u00e9tape indispensable de l'\u00e9valuation. Il ne s'agit pas seulement de mesurer un score de performance global, mais bien d'analyser m\u00e9thodiquement les erreurs sp\u00e9cifiques pour comprendre pourquoi elles se produisent.</p> <p>Cette analyse d\u00e9taill\u00e9e permet de cibler pr\u00e9cis\u00e9ment les actions correctives n\u00e9cessaires, \u00e9vitant ainsi les modifications \u00e0 l'aveugle.</p>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#evaluation-humaine-vs-automatisee-trouver-le-bon-equilibre","title":"\u00c9valuation humaine vs automatis\u00e9e : Trouver le bon \u00e9quilibre","text":"<p>Dans les premi\u00e8res phases de d\u00e9veloppement, la tentation d'automatiser enti\u00e8rement l'\u00e9valuation est forte, notamment pour traiter de grands volumes de donn\u00e9es et gagner du temps. Les m\u00e9triques automatiques (BLEU, ROUGE, pertinence s\u00e9mantique calcul\u00e9e, LLM as a judge, etc.) ont leur utilit\u00e9.</p> <p>Cependant, surtout au d\u00e9but du projet, rien ne remplace une \u00e9valuation humaine. Pourquoi ? Parce qu'un humain peut comprendre les nuances, le contexte, et identifier des types d'erreurs subtiles ou inattendues qu'une m\u00e9trique automatique pourrait manquer. Investir ce temps pour d\u00e9cortiquer manuellement un \u00e9chantillon repr\u00e9sentatif de cas probl\u00e9matiques est souvent plus rentable \u00e0 long terme, car cela fournit des insights qualitatifs pr\u00e9cieux pour orienter les am\u00e9liorations. L'automatisation peut ensuite prendre le relais pour v\u00e9rifier l'impact des corrections \u00e0 plus grande \u00e9chelle.</p>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#corriger-les-problemes-a-la-source","title":"Corriger les probl\u00e8mes \u00e0 la source","text":"<p>Une fois l'erreur comprise gr\u00e2ce \u00e0 l'\u00e9valuation (humaine et/ou automatis\u00e9e), l'am\u00e9lioration devient cibl\u00e9e. Chaque erreur identifi\u00e9e, en lien avec les donn\u00e9es sp\u00e9cifiques trait\u00e9es, peut alors \u00eatre corrig\u00e9e ou, du moins, att\u00e9nu\u00e9e en ajustant le composant d\u00e9faillant :</p> <ul> <li>Am\u00e9liorer le prompt envoy\u00e9 au LLM.</li> <li>Affiner la strat\u00e9gie de d\u00e9coupage (chunking).</li> <li>Tester et choisir un meilleur mod\u00e8le d'embedding pour la r\u00e9cup\u00e9ration.</li> <li>Nettoyer ou enrichir les donn\u00e9es sources.</li> <li>Ajuster les param\u00e8tres de l'algorithme de recherche.</li> <li>Parfois, oui, int\u00e9grer un nouvel outil sp\u00e9cifiquement pour r\u00e9soudre un probl\u00e8me identifi\u00e9 (mais seulement apr\u00e8s analyse !).</li> </ul>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#lamelioration-continue-en-production","title":"L'Am\u00e9lioration continue en production","text":"<p>L'\u00e9valuation et l'am\u00e9lioration ne s'arr\u00eatent pas une fois l'application d\u00e9velopp\u00e9e et d\u00e9ploy\u00e9e. En production, il est essentiel de continuer \u00e0 monitorer les performances de mani\u00e8re continue. Les donn\u00e9es peuvent \u00e9voluer (data drift), de nouveaux cas d'usage peuvent appara\u00eetre, et les attentes des utilisateurs peuvent changer.</p> <p>Id\u00e9alement, il faut mettre en place des m\u00e9canismes pour int\u00e9grer les retours utilisateurs (par exemple, un syst\u00e8me de pouce lev\u00e9/baiss\u00e9 sur les r\u00e9ponses) dans le cycle d'am\u00e9lioration. C'est un cycle continu : observer, analyser, am\u00e9liorer, jusqu'\u00e0 atteindre et maintenir un niveau de performance et de fiabilit\u00e9 satisfaisant pour l'usage vis\u00e9.</p>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#conclusion-une-approche-methodique-et-iterative","title":"Conclusion : Une approche m\u00e9thodique et it\u00e9rative","text":"<p>En r\u00e9sum\u00e9, plut\u00f4t que de c\u00e9der syst\u00e9matiquement \u00e0 l'attrait du dernier outil \u00e0 la mode, une d\u00e9marche plus pragmatique et souvent plus payante consiste \u00e0 adopter une approche it\u00e9rative et m\u00e9thodique :</p> <ol> <li>\u00c9valuer rigoureusement les sorties de l'application.</li> <li>Analyser en profondeur les erreurs pour en comprendre la cause racine.</li> <li>Am\u00e9liorer de mani\u00e8re cibl\u00e9e le ou les composants identifi\u00e9s comme d\u00e9faillants.</li> <li>Monitorer en continu et int\u00e9grer les retours.</li> </ol> <p>C'est souvent la voie la plus directe vers des applications IA (RAG, agents) r\u00e9ellement performantes, fiables et utiles au quotidien.</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/","title":"Comprendre l'intelligence artificielle : L'IA g\u00e9n\u00e9rative (Partie 2)","text":"","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#introduction","title":"Introduction","text":"<p>Dans la premi\u00e8re partie de cette exploration de l'intelligence artificielle, j'ai cherch\u00e9 \u00e0 rendre accessibles des notions souvent floues comme le machine learning et le deep learning. Dans cet article, je vous propose de plonger ensemble dans un domaine dont tout le monde parle depuis deux ans : l'IA g\u00e9n\u00e9rative.</p> <p>Avec l'essor impressionnant d'outils comme ChatGPT, Perplexity ou Midjourney, l'IA g\u00e9n\u00e9rative s'est impos\u00e9e sur le devant de la sc\u00e8ne. Contrairement aux autres formes d'IA qui se contentent d'analyser ou de classer des donn\u00e9es, cette technologie est capable de cr\u00e9er de nouvelles donn\u00e9es. Que ce soit pour g\u00e9n\u00e9rer du texte, des images, ou m\u00eame de l'audio. En gros, tout r\u00e9side dans le mot \"G\u00e9n\u00e9rative\" : pour faire simple c'est une forme ou domaine d'IA qui va permettre de g\u00e9n\u00e9rer un nouveau contenu.</p> <p>Dans cette partie, je vais simplement expliquer ce qu'est l'IA g\u00e9n\u00e9rative, comment \u00e7a marche, donner des exemples concrets, et montrer \u00e0 quoi \u00e7a sert vraiment. L'id\u00e9e, c'est de voir comment les machines ne se contentent plus de comprendre ce qu'on leur donne, mais arrivent carr\u00e9ment \u00e0 cr\u00e9er du nouveau contenu.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#les-premiers-pas","title":"Les premiers pas","text":"<p>Une des choses qui m'a le plus impressionn\u00e9 dans le domaine de l'IA g\u00e9n\u00e9rative a \u00e9t\u00e9 l'\u00e9mergence de mod\u00e8les performants tels que GPT-3 (ChatGPT en 2022). Qu'on a appel\u00e9 les LLM, large language mod\u00e8les ou \"grands mod\u00e8les de langage\" en fran\u00e7ais. La premi\u00e8re chose remarquable, c'est l'impression de dialoguer avec une intelligence artificielle qui non seulement comprend nos propos, mais parvient \u00e9galement \u00e0 maintenir une conversation coh\u00e9rente.</p> <p>Ayant d\u00e9j\u00e0 travaill\u00e9 sur des projets de traitement du langage naturel (NLP) (cf. la partie 1 : Comprendre l'intelligence artificielle - Guide pratique simple), je savais bien que la performance de GPT-3 n'\u00e9tait pas aussi simple \u00e0 obtenir. Si c'\u00e9tait aussi facile, des g\u00e9ants comme Google auraient sorti des IA aussi puissantes depuis longtemps ! Mais \u00e0 l'\u00e9poque (avant 2022), personne ne savait vraiment comment s'y prendre pour atteindre ce niveau.</p> <p>L'arriv\u00e9e de ChatGPT a vraiment chang\u00e9 la donne dans le monde de l'IA. Contrairement \u00e0 ce qu'on pourrait croire, le principe g\u00e9n\u00e9ral derri\u00e8re ces avanc\u00e9es n'est pas si compliqu\u00e9 : il s'agit principalement d'utiliser une immense quantit\u00e9 de donn\u00e9es et une puissance de calcul colossale pour entra\u00eener des mod\u00e8les tr\u00e8s grands, qu'on appelle des LLMs. Ces mod\u00e8les existent depuis 2017, mais ce qui a tout boulevers\u00e9, c'est l'\u00e9chelle \u00e0 laquelle ils ont \u00e9t\u00e9 d\u00e9ploy\u00e9s. Cela dit, il faut nuancer : m\u00eame si l'id\u00e9e para\u00eet simple sur le papier, la mise en \u0153uvre concr\u00e8te reste extr\u00eamement complexe et co\u00fbteuse, tant sur le plan technique que financier.</p> <p>C'est un peu comme si, pour gagner une course de voiture, au lieu d'optimiser la forme de la voiture ou d'am\u00e9liorer la technique du pilote, on d\u00e9cidait simplement d'installer le plus de moteurs puissants possible tout en agrandissant la voiture, en esp\u00e9rant que la voiture ira forc\u00e9ment plus vite gr\u00e2ce \u00e0 toute cette puissance, m\u00eame si ce n'est pas la solution la plus \u00e9l\u00e9gante. Dans notre cas, la taille du mod\u00e8le correspond \u00e0 la taille de la voiture, et le moteur de la voiture fait r\u00e9f\u00e9rence \u00e0 la donn\u00e9e ing\u00e9r\u00e9e par le mod\u00e8le.</p> <p>Pour vous donner une id\u00e9e du co\u00fbt, environ 4 millions de dollars ont \u00e9t\u00e9 n\u00e9cessaires pour entra\u00eener l'un des premiers mod\u00e8les d'IA \u00e0 atteindre une performance satisfaisante : GPT-3 (voir : Co\u00fbt d'entra\u00eenement de GPT-3), sans compter le salaire des chercheurs et les ann\u00e9es de recherche n\u00e9cessaires pour y parvenir.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#un-changement-de-paradigme-ia","title":"Un changement de paradigme IA","text":"<p>Vous l'avez compris, l'intelligence artificielle a connu un v\u00e9ritable tournant avec l'arriv\u00e9e de cette approche \u00ab\u202fbourrin\u202f\u00bb \ud83d\ude05 : apr\u00e8s cette d\u00e9couverte, plus rien n'a \u00e9t\u00e9 comme avant dans le domaine de l'IA.</p> <p>Mais revenons \u00e0 l'essentiel. Dans la partie 1, j'expliquais que les mod\u00e8les sont des algorithmes qui apprennent \u00e0 partir des donn\u00e9es qu'on leur donne. Les mod\u00e8les Transformers (utilis\u00e9s pour ChatGPT), c'est juste un nouveau type de mod\u00e8le architectur\u00e9s de mani\u00e8re diff\u00e9rente. Pour faire simple\u202f: imaginez un b\u00e2timent. Selon ce qu'on veut en faire (habiter, travailler...), on ne va pas le construire pareil. En IA, c'est pareil\u202f: selon le besoin, on choisit une \"forme\" de mod\u00e8le diff\u00e9rente. Les transformers, c'est une architecture qui a tr\u00e8s bien march\u00e9 pour g\u00e9n\u00e9rer du texte, des images, etc.</p> <p>Dans tous les mod\u00e8les d'IA, il y a plein de chiffres \u00e0 l'int\u00e9rieur, qu'on appelle \u00ab\u202fparam\u00e8tres\u202f\u00bb. Imagine-les comme des petits boutons qu'on peut tourner. Pris tout seuls, ces chiffres ne veulent rien dire. Mais quand on les r\u00e8gle tous ensemble, c'est \u00e7a qui permet au mod\u00e8le d'apprendre et de faire son travail.</p> <p>Pour illustrer un peu l'id\u00e9e, sur cette image, on voit une machine qui permet de r\u00e9gler les sonorit\u00e9s : dans un mod\u00e8le d'IA, chaque param\u00e8tre peut \u00eatre imagin\u00e9 comme un petit bouton que l'on tourne pour ajuster le comportement du mod\u00e8le. Par exemple, un param\u00e8tre peut influencer la fa\u00e7on dont l'IA accorde de l'importance \u00e0 certains mots ou \u00e0 certaines images. Comme sur une table de mixage audio o\u00f9 chaque bouton modifie un aspect du son, les param\u00e8tres d'un mod\u00e8le d'IA sont ajust\u00e9s pour obtenir le meilleur r\u00e9sultat possible lors de l'apprentissage.</p> <p></p> <p>Les param\u00e8tres sont ajust\u00e9s gr\u00e2ce aux donn\u00e9es qu'on fournit au mod\u00e8le lors de son apprentissage. Les donn\u00e9es permettent de trouver la combinaison de param\u00e8tres optimale pour que l'IA soit la plus performante possible. Le dernier \u00e9l\u00e9ment \u00e0 comprendre dans un mod\u00e8le d'IA, c'est que tous ces param\u00e8tres sont connect\u00e9s entre eux par des op\u00e9rations math\u00e9matiques. C'est ce qui permet de calculer le r\u00e9sultat final de l'IA. C'est tout ? C'est juste des chiffres et des op\u00e9rations qui permettent \u00e0 ChatGPT d'\u00eatre aussi fort ? Eh bien, aussi simple que cela puisse para\u00eetre =&gt; OUI. Bien \u00e9videmment, l'id\u00e9e globale est simple, mais lorsqu'on s'y met, \u00e7a peut vite devenir complexe.</p> <p>S'il y a une chose \u00e0 retenir en IA g\u00e9n\u00e9rative, c'est qu'on utilise massivement des donn\u00e9es pour entra\u00eener de tr\u00e8s gros mod\u00e8les pour qu'ils continuent de s'am\u00e9liorer en termes de performance comme on peut le voir sur le graphique suivant : le mod\u00e8le de 70 milliards de param\u00e8tres (Llama 3 70b) a une des meilleures performances, alors que ceux ayant 7 ou 8 milliards sont en dessous. \u00c9videmment, on arrive de plus en plus \u00e0 am\u00e9liorer l'efficacit\u00e9 de plus petits mod\u00e8les mais nous y viendrons plus tard.</p> <p></p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#lentrainement-des-modeles-generatifs","title":"L'entra\u00eenement des mod\u00e8les g\u00e9n\u00e9ratifs","text":"<p>L'entra\u00eenement d'un mod\u00e8le g\u00e9n\u00e9ratif, c'est un peu comme apprendre \u00e0 un enfant \u00e0 \u00e9crire ou \u00e0 dessiner\u202f: on lui montre des millions d'exemples, et il finit par comprendre comment cr\u00e9er quelque chose de nouveau \u00e0 partir de ce qu'il a vu. Mais ici, l'\u00ab\u202fenfant\u202f\u00bb est un algorithme, et les exemples sont des montagnes de textes, d'images, de sons ou de vid\u00e9os collect\u00e9s sur Internet.</p> <p>Concr\u00e8tement, l'entra\u00eenement consiste \u00e0 pr\u00e9senter au mod\u00e8le d'innombrables morceaux de donn\u00e9es et \u00e0 lui demander de deviner la suite logique\u202f: le prochain mot dans une phrase, le pixel suivant dans une image, la note suivante dans une m\u00e9lodie. \u00c0 chaque essai, le mod\u00e8le compare sa proposition \u00e0 la bonne r\u00e9ponse, puis ajuste ses fameux \u00ab\u202fparam\u00e8tres\u202f\u00bb pour s'am\u00e9liorer. Ce processus, appel\u00e9 optimisation, se r\u00e9p\u00e8te des milliards de fois jusqu'\u00e0 ce que le mod\u00e8le devienne suffisamment bon pour g\u00e9n\u00e9rer du contenu cr\u00e9dible.</p> <p>Ce qui distingue l'IA g\u00e9n\u00e9rative moderne, c'est l'\u00e9chelle\u202f: on ne parle plus de quelques milliers d'exemples, mais de milliards. L'entra\u00eenement d'un mod\u00e8le comme GPT-3 ou Llama 3 n\u00e9cessite des semaines, voire des mois, sur des ordinateurs \u00e9quip\u00e9s de milliers de cartes graphiques. C'est cette d\u00e9mesure qui permet d'obtenir des r\u00e9sultats bluffants, mais qui explique aussi pourquoi seuls quelques acteurs majeurs peuvent se permettre de cr\u00e9er de tels mod\u00e8les.</p> <p>Un autre point cl\u00e9\u202f: plus le mod\u00e8le est grand (c'est-\u00e0-dire plus il a de param\u00e8tres), plus il a de capacit\u00e9 \u00e0 apprendre des subtilit\u00e9s et \u00e0 g\u00e9n\u00e9rer des contenus vari\u00e9s. Mais cela ne veut pas dire que \u00ab\u202fplus gros\u202f\u00bb est toujours \u00ab\u202fmieux\u202f\u00bb\u202f: il faut aussi des donn\u00e9es de qualit\u00e9, et il existe aujourd'hui des recherches pour rendre les mod\u00e8les plus efficaces, m\u00eame \u00e0 taille r\u00e9duite.</p> <p>Enfin, il ne faut pas oublier que l'entra\u00eenement n'est qu'une \u00e9tape. Une fois le mod\u00e8le pr\u00eat, il peut \u00eatre \u00ab\u202faffin\u00e9\u202f\u00bb (fine-tuned) sur des t\u00e2ches sp\u00e9cifiques, ou mis \u00e0 jour pour corriger ses erreurs et s'adapter \u00e0 de nouveaux usages. C'est ce qui permet, par exemple, d'avoir des IA sp\u00e9cialis\u00e9es dans la r\u00e9daction, la traduction, la cr\u00e9ation d'images, etc.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#comment-ca-marche-concretement","title":"Comment \u00e7a marche concr\u00e8tement\u202f?","text":"<p>Maintenant qu'on sait comment on entra\u00eene un mod\u00e8le g\u00e9n\u00e9ratif, on peut se demander : comment fait le mod\u00e8le, une fois entra\u00een\u00e9, pour inventer du texte ? Prenons l'exemple d'un mod\u00e8le de texte comme ChatGPT. Lorsqu'on lui pose une question ou que l'on commence une phrase, il ne fait rien d'autre que de deviner, mot apr\u00e8s mot, ce qui a le plus de chances de venir ensuite. C'est un peu comme un jeu du \u00ab\u202fcompl\u00e8te la phrase\u202f\u00bb\u202f: on \u00e9crit \u00ab\u202fLe chat grimpe sur...\u202f\u00bb, et l'IA va proposer \u00ab\u202fle toit\u202f\u00bb, \u00ab\u202fl'arbre\u202f\u00bb, ou autre, en fonction de ce qu'elle a vu des millions de fois dans ses donn\u00e9es d'entra\u00eenement. Elle choisit \u00e0 chaque \u00e9tape le mot qui lui semble le plus logique, puis recommence, encore et encore, jusqu'\u00e0 former une r\u00e9ponse compl\u00e8te. Ce qui est fascinant, c'est que le mod\u00e8le ne comprend pas vraiment ce qu'il \u00e9crit ou dessine\u202f: il se base uniquement sur des probabilit\u00e9s, en essayant de coller au mieux \u00e0 ce qu'il a vu dans ses donn\u00e9es. C'est pour cela qu'il peut parfois inventer des choses qui n'existent pas (\u00ab\u202fhalluciner\u202f\u00bb), ou se tromper compl\u00e8tement si la question sort de l'ordinaire.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#exemples-dia-generative","title":"Exemples d'IA g\u00e9n\u00e9rative","text":"<p>Pour mieux comprendre l'impact de l'IA g\u00e9n\u00e9rative, voici quelques exemples d'applications qui transforment d\u00e9j\u00e0 notre quotidien\u202f:</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#generation-de-texte","title":"G\u00e9n\u00e9ration de texte","text":"<ul> <li>ChatGPT, Gemini, Mistral, Llama : Ces assistants conversationnels peuvent r\u00e9pondre \u00e0 des questions, r\u00e9diger des emails, r\u00e9sumer des documents, traduire des textes, ou m\u00eame \u00e9crire des histoires et des po\u00e8mes. Ils sont utilis\u00e9s dans le support client, l'aide \u00e0 la r\u00e9daction, l'\u00e9ducation, etc.</li> </ul>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#generation-dimages","title":"G\u00e9n\u00e9ration d'images","text":"<ul> <li>Midjourney, DALL\u00b7E, Stable Diffusion, reve.art : Ces outils transforment une simple description textuelle (\u00ab\u202fun chat qui joue de la guitare sur la lune\u202f\u00bb) en image r\u00e9aliste ou artistique. Ils sont utilis\u00e9s par des artistes, des designers, des publicitaires, ou simplement pour s'amuser.</li> </ul>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#generation-de-code","title":"G\u00e9n\u00e9ration de code","text":"<ul> <li>GitHub Copilot, Claude code, Cursor : Ces IA assistent les d\u00e9veloppeurs en g\u00e9n\u00e9rant automatiquement du code, en sugg\u00e9rant des corrections ou en expliquant des fonctions. Elles acc\u00e9l\u00e8rent le d\u00e9veloppement logiciel et aident \u00e0 l'apprentissage de la programmation.</li> </ul>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#generation-de-musique-et-daudio","title":"G\u00e9n\u00e9ration de musique et d'audio","text":"<ul> <li>Suno, MusicLM : Ces mod\u00e8les peuvent composer de la musique originale dans diff\u00e9rents styles, g\u00e9n\u00e9rer des voix synth\u00e9tiques ou cr\u00e9er des effets sonores \u00e0 partir d'une simple consigne.</li> </ul>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#generation-de-videos","title":"G\u00e9n\u00e9ration de vid\u00e9os","text":"<ul> <li>Sora, RunwayML, Veo : Ces outils permettent de cr\u00e9er des vid\u00e9os courtes \u00e0 partir d'un texte descriptif, d'animer des images ou de g\u00e9n\u00e9rer des effets sp\u00e9ciaux.</li> </ul>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#autres-usages-de-lia-generative","title":"Autres usages de l'IA g\u00e9n\u00e9rative","text":"<ul> <li>RAG (Retrieval-Augmented Generation) : Le RAG, c'est un peu comme avoir un ChatGPT personnalis\u00e9 qui r\u00e9pond \u00e0 partir de vos documents, sans avoir besoin de r\u00e9entra\u00eener l'IA. Concr\u00e8tement, au lieu de s'appuyer uniquement sur ce qu'il a appris lors de son entra\u00eenement, le mod\u00e8le va d'abord aller chercher des informations pertinentes dans une base de donn\u00e9es ou un ensemble de documents que vous lui fournissez (par exemple, vos manuels internes, FAQ, rapports, etc.). Ensuite, il utilise ces informations pour g\u00e9n\u00e9rer une r\u00e9ponse adapt\u00e9e et contextualis\u00e9e. Cette m\u00e9thode permet d'obtenir des r\u00e9ponses pr\u00e9cises, \u00e0 jour et vraiment align\u00e9es sur votre contexte, tout en limitant les risques d'hallucinations ou d'erreurs. C'est une fa\u00e7on simple et puissante de mettre l'IA au service de vos besoins, sans avoir \u00e0 manipuler des mod\u00e8les complexes ou \u00e0 g\u00e9rer de longs entra\u00eenements avec des co\u00fbts assez importants. </li> </ul> <p>Le RAG a \u00e9t\u00e9 l'une des technologies les plus populaires depuis le d\u00e9but de l'IA g\u00e9n\u00e9rative. </p> <ul> <li> <p>Agents autonomes : Les agents sont l'une des principales fa\u00e7ons d'augmenter les capacit\u00e9s d'un mod\u00e8le de langage (LLM) comme ChatGPT. Un LLM seul se contente de g\u00e9n\u00e9rer du texte en r\u00e9ponse \u00e0 une consigne, mais un agent va plus loin\u202f: il utilise le LLM comme \u00ab\u202fcerveau\u202f\u00bb pour planifier, prendre des d\u00e9cisions et interagir avec le monde ext\u00e9rieur (applications, sites web, emails, etc.). Par exemple, un agent peut organiser un voyage complet, r\u00e9pondre \u00e0 des emails, ou automatiser des t\u00e2ches m\u00e9tiers en combinant le raisonnement du LLM avec l'acc\u00e8s \u00e0 des outils ou des bases de donn\u00e9es. Cette approche permet de rendre l'IA vraiment utile dans des situations complexes, en lui donnant la capacit\u00e9 d'agir, de s'adapter au contexte et m\u00eame d'apprendre de nouvelles comp\u00e9tences au fil du temps. La performance des agents autonomes n'est pas encore parfaite, mais \u00e7a progresse rapidement.</p> </li> <li> <p>Synth\u00e8se et r\u00e9sum\u00e9 d'informations : Les mod\u00e8les g\u00e9n\u00e9ratifs sont capables de r\u00e9sumer automatiquement de longs documents, d'extraire les points cl\u00e9s d'un rapport, ou de g\u00e9n\u00e9rer des comptes rendus personnalis\u00e9s \u00e0 partir de multiples sources. Cela facilite la veille, l'analyse de donn\u00e9es et la prise de d\u00e9cision rapide.</p> </li> </ul> <p>Ces exemples montrent que l'IA g\u00e9n\u00e9rative est assez vaste et grand public : elle s'invite dans de nombreux secteurs (\u00e9ducation, sant\u00e9, cr\u00e9ation artistique, industrie, etc.) et ouvre la porte \u00e0 de nouveaux usages, parfois encore inimaginables il y a deux-trois ans en arri\u00e8re.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#bien-utiliser-lia-generative","title":"Bien utiliser l'IA g\u00e9n\u00e9rative","text":"<p>L'IA g\u00e9n\u00e9rative est un outil puissant, mais pour en tirer le meilleur parti, il faut savoir l'utiliser de la bonne fa\u00e7on. Voici quelques conseils simples pour l'utiliser efficacement :</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#1-donner-des-consignes-claires-le-prompting","title":"1. Donner des consignes claires (le \u00ab\u202fprompting\u202f\u00bb)","text":"<p>La qualit\u00e9 des r\u00e9sultats d\u00e9pend beaucoup de la fa\u00e7on dont on formule la demande. Plus la consigne est claire, pr\u00e9cise et d\u00e9taill\u00e9e, plus l'IA a de chances de donner une r\u00e9ponse pertinente. Il ne faut pas h\u00e9siter \u00e0 donner du contexte, \u00e0 pr\u00e9ciser le style ou le format attendu, ou \u00e0 demander plusieurs propositions si besoin. Pour \u00e9crire les meilleurs prompts, il existe des guides selon l'IA utilis\u00e9e. Voici par exemple le guide d'OpenAI pour le mod\u00e8le GPT-4.1 : Guide.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#2-relire-et-verifier-les-resultats","title":"2. Relire et v\u00e9rifier les r\u00e9sultats","text":"<p>M\u00eame si l'IA est impressionnante, elle peut se tromper ou inventer des informations. Il est donc essentiel de toujours relire ce qu'elle produit, de v\u00e9rifier les faits importants et de corriger les \u00e9ventuelles erreurs ou incoh\u00e9rences. L'IA doit rester un assistant, pas un rempla\u00e7ant du jugement humain.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#3-experimenter-et-affiner","title":"3. Exp\u00e9rimenter et affiner","text":"<p>N'h\u00e9site pas \u00e0 tester diff\u00e9rentes formulations, \u00e0 demander des variantes ou \u00e0 affiner ta demande si le r\u00e9sultat ne te convient pas du premier coup. L'IA ne \"comprend\" pas comme un humain, mais elle peut s'adapter \u00e0 tes besoins au fil des \u00e9changes.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#limites-de-lia-generative","title":"Limites de l'IA g\u00e9n\u00e9rative","text":"<p>L'IA g\u00e9n\u00e9rative a des limites importantes\u202f: les conna\u00eetre permet de mieux l'utiliser au quotidien.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#1-pas-de-veritable-invention-ni-de-comprehension","title":"1. Pas de v\u00e9ritable invention ni de compr\u00e9hension","text":"<p>L'IA ne cr\u00e9e rien de totalement nouveau\u202f: elle se contente de r\u00e9assembler, reformuler ou combiner ce qu'elle a d\u00e9j\u00e0 vu dans ses donn\u00e9es d'entra\u00eenement. Si une id\u00e9e, une information ou un style n'a jamais \u00e9t\u00e9 rencontr\u00e9 pendant l'entra\u00eenement, l'IA ne pourra pas l'inventer ni le deviner. Elle ne comprend pas r\u00e9ellement le sens de ce qu'elle g\u00e9n\u00e8re\u202f: il n'y a ni conscience, ni intention, ni r\u00e9flexion derri\u00e8re ses r\u00e9ponses. L'IA manipule des probabilit\u00e9s.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#2-les-hallucinations","title":"2. Les hallucinations","text":"<p>L'IA g\u00e9n\u00e9rative peut produire des \u00ab\u202fhallucinations\u202f\u00bb\u202f: elle invente parfois des informations qui semblent cr\u00e9dibles, mais qui sont fausses, inexactes ou trompeuses. Cela peut concerner des faits, des citations, des r\u00e9f\u00e9rences, des chiffres, etc. M\u00eame si vous demandez \u00e0 l'IA de ne pas inventer ou de citer ses sources, il n'y a aucune garantie que le r\u00e9sultat soit fiable \u00e0 100\u202f%. C'est pourquoi il faut toujours relire et v\u00e9rifier les r\u00e9ponses, surtout pour des sujets sensibles, professionnels ou lorsque l'exactitude est cruciale.</p> <p>La seule fa\u00e7on d'\u00eatre certain de la v\u00e9racit\u00e9 d'une information produite par l'IA est de d\u00e9j\u00e0 conna\u00eetre la r\u00e9ponse ou de pouvoir la v\u00e9rifier soi-m\u00eame \u00e0 partir d'une source fiable.</p> <p>Conseil\u202f: N'utilisez pas l'IA pour obtenir des informations que vous ne pouvez pas v\u00e9rifier, ou alors uniquement dans des contextes o\u00f9 une \u00e9ventuelle erreur n'aura pas de cons\u00e9quences importantes.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#3-biais-et-stereotypes","title":"3. Biais et st\u00e9r\u00e9otypes","text":"<p>Les mod\u00e8les d'IA sont entra\u00een\u00e9s sur de grandes quantit\u00e9s de donn\u00e9es issues d'Internet ou d'autres sources. Si ces donn\u00e9es contiennent des biais, des st\u00e9r\u00e9otypes ou des pr\u00e9jug\u00e9s, l'IA risque de les reproduire ou m\u00eame de les amplifier dans ses r\u00e9ponses. Il est donc important de garder un esprit critique et de ne pas prendre au s\u00e9rieux tout ce que l'IA g\u00e9n\u00e8re, notamment sur des sujets sociaux, culturels ou sensibles.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#4-confidentialite-et-securite","title":"4. Confidentialit\u00e9 et s\u00e9curit\u00e9","text":"<p>Les informations saisies dans une IA peuvent \u00eatre stock\u00e9es temporairement ou utilis\u00e9es pour am\u00e9liorer le mod\u00e8le. Il est donc d\u00e9conseill\u00e9 d'y partager des donn\u00e9es personnelles, confidentielles ou sensibles. Soyez vigilant sur ce que vous communiquez \u00e0 l'IA, surtout dans un cadre professionnel.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#conclusion","title":"Conclusion","text":"<p>L'IA g\u00e9n\u00e9rative n'en est qu'\u00e0 ses d\u00e9buts et son \u00e9volution promet de transformer encore davantage notre quotidien. \u00c0 terme, on peut imaginer l'\u00e9mergence de mod\u00e8les plus compacts et plus efficaces, capables pour certains usages de fonctionner directement sur nos appareils (ordinateurs, smartphones, objets connect\u00e9s), sans d\u00e9pendre du cloud. Pour l'instant, la plupart des mod\u00e8les les plus puissants restent accessibles uniquement via le cloud, mais cette miniaturisation ouvrira progressivement la voie \u00e0 des usages plus priv\u00e9s, plus rapides et mieux adapt\u00e9s \u00e0 chaque utilisateur.</p> <p>Parall\u00e8lement, la question de la r\u00e9gulation et de l'\u00e9thique deviendra centrale\u202f: il faudra encadrer l'utilisation de ces technologies pour limiter les risques de d\u00e9sinformation, de biais ou d'atteinte \u00e0 la vie priv\u00e9e. Les gouvernements, les entreprises et la soci\u00e9t\u00e9 civile devront collaborer pour d\u00e9finir des r\u00e8gles claires et garantir un usage responsable de l'IA.</p> <p>Enfin, l'IA g\u00e9n\u00e9rative va progressivement s'ins\u00e9rer de fa\u00e7on personnalis\u00e9e dans chaque m\u00e9tier\u202f: elle viendra assister les professionnels au quotidien, en s'adaptant aux besoins sp\u00e9cifiques de chaque secteur. Que ce soit pour aider \u00e0 la prise de d\u00e9cision, automatiser des t\u00e2ches r\u00e9p\u00e9titives ou lib\u00e9rer du temps pour se concentrer sur des activit\u00e9s \u00e0 plus forte valeur ajout\u00e9e, l'IA deviendra un v\u00e9ritable partenaire de travail. Elle ne remplacera pas la cr\u00e9ativit\u00e9 ou l'expertise humaine, mais agira comme un outil puissant pour les amplifier et les enrichir. L'essentiel sera de rester curieux, critique et ouvert face \u00e0 ces \u00e9volutions, afin d'en tirer le meilleur pour chacun.</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/","title":"Comprendre l'intelligence artificielle : Guide Simple (Partie 1)","text":"","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#introduction","title":"Introduction","text":"<p>L'intelligence artificielle s\u00e9duit de plus en plus de curieux et de professionnels, gr\u00e2ce \u00e0 des outils r\u00e9volutionnaires comme ChatGPT. Ces avanc\u00e9es ont non seulement transform\u00e9 notre mani\u00e8re d'interagir avec la technologie, mais ont aussi rendu l'IA incontournable dans les discussions contemporaines. Avec l'IA g\u00e9n\u00e9rative, nous pouvons d\u00e9sormais produire du texte, des images et bien d'autres contenus gr\u00e2ce \u00e0 de puissants mod\u00e8les d'apprentissage.</p> <p>Cependant, se plonger dans le monde de l'IA peut s'av\u00e9rer d\u00e9routant. Entre les vid\u00e9os explicatives superficielles et un flot de ressources sans v\u00e9ritable explication de fond, la confusion r\u00e8gne. De nombreuses id\u00e9es re\u00e7ues circulent, amplifiant la perception que l'IA est un domaine imp\u00e9n\u00e9trable.</p> <p>L'objectif de cet article est de dissiper cette brume en fournissant une introduction claire et accessible \u00e0 l'IA, ses composantes et ses applications. Que vous soyez juste curieux ou que vous cherchiez \u00e0 int\u00e9grer l'IA dans votre domaine, ce guide est con\u00e7u pour vous donner les cl\u00e9s n\u00e9cessaires pour comprendre l'intelligence artificielle et naviguer dans cet univers fascinant.</p> <p>Dans cette partie 1, je vais me concentrer sur les bases. Et dans la deuxieme partie je me focaliserai sur l'IA G\u00e9n\u00e9rative.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#comprendre-les-bases-de-lia","title":"Comprendre les Bases de l'IA","text":"<p>L'intelligence artificielle est un terme que l'on entend partout ces jours-ci, mais que signifie-t-il vraiment ? Pour faire simple, l'IA repose sur des algorithmes capables d'apprendre et d'effectuer des t\u00e2ches sp\u00e9cifiques \u00e0 partir de donn\u00e9es historiques. Les donn\u00e9es, donc, sont la pierre angulaire de toute application d'IA. Sans elles, l'algorithme n'a rien sur quoi s'appuyer.</p> <p>Pour comprendre l'intelligence artificielle, il faut d'abord saisir l'importance des donn\u00e9es et des mod\u00e8les qui les exploitent.</p> <p>L'id\u00e9e fondamentale est d'utiliser ces donn\u00e9es pour cr\u00e9er des mod\u00e8les capables de r\u00e9aliser de nouvelles t\u00e2ches ou de pr\u00e9dire des r\u00e9sultats. Par exemple, identifier des fraudes \u00e0 partir de caract\u00e9ristiques d\u00e9j\u00e0 observ\u00e9es. Sans ces donn\u00e9es pr\u00e9existantes, notre \"intelligence\" serait sans rep\u00e8re, comme un chef talentueux mais sans ingr\u00e9dients.</p> <p>L'IA n'est pas une baguette magique, elle ne fonctionne pas dans le vide. L'algorithme, souvent appel\u00e9 \"mod\u00e8le\", \"r\u00e9seau\" ou m\u00eame simplement \"une IA\", doit \u00eatre nourri. Sans donn\u00e9es, il ne peut pas fonctionner correctement.</p> <p>En somme, alors que beaucoup courent apr\u00e8s la nouveaut\u00e9 des applications IA, il est crucial de toujours revenir \u00e0 la source : les donn\u00e9es historiques. Elles ne sont pas seulement un point de d\u00e9part, mais le socle ind\u00e9fectible sur lequel toute l'IA repose. Commen\u00e7ons donc par balayer les mythes et fondons notre compr\u00e9hension sur ce qui est vraiment essentiel.</p> <p>Il est important de noter que l'IA n\u00e9cessite beaucoup plus de donn\u00e9es et d'\u00e9nergie qu'un humain pour apprendre. Prenons l'exemple d'AlphaGo, l'IA c\u00e9l\u00e8bre pour avoir battu les meilleurs joueurs de Go au monde. Elle a \u00e9t\u00e9 entra\u00een\u00e9e gr\u00e2ce \u00e0 des dizaines de millions de parties, simul\u00e9es et jou\u00e9es, alors qu'un humain parvient \u00e0 ma\u00eetriser le jeu avec beaucoup moins de pratique. Cette d\u00e9pendance massive aux donn\u00e9es et aux ressources \u00e9nerg\u00e9tiques montre bien la diff\u00e9rence entre l'apprentissage humain et celui des machines.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#les-composantes-de-lia","title":"Les composantes de l'IA","text":"<p>Dans le vaste univers de l'intelligence artificielle, deux sous-domaines se distinguent particuli\u00e8rement : le Machine Learning (ML) et le Deep Learning (DL). Ces techniques sont les fondations sur lesquelles reposent de nombreuses applications d'IA que nous utilisons aujourd'hui.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#machine-learning","title":"Machine Learning","text":"<p>Le Machine Learning est une technique d'apprentissage statistique qui permet aux machines d\u2019apprendre \u00e0 partir de donn\u00e9es, sans \u00eatre explicitement programm\u00e9es pour chaque t\u00e2che. Cela se traduit par des mod\u00e8les capables d'effectuer des pr\u00e9dictions ou des classifications dans une multitude de domaines.</p> <p>Prenons l'exemple de la pr\u00e9diction des prix immobiliers : un mod\u00e8le ML peut analyser des donn\u00e9es historiques sur les ventes de maisons pour d\u00e9terminer le prix probable d'un bien donn\u00e9 en se basant sur diff\u00e9rentes caract\u00e9ristiques (surface, nombre de chambres, etc.). De m\u00eame, dans le domaine de la reconnaissance faciale, ces syst\u00e8mes sont form\u00e9s \u00e0 partir de milliers d'images pour identifier avec pr\u00e9cision un visage dans une foule.</p> <p>L'essentiel ici est de comprendre que le Machine Learning n'est pas juste un outil magique. C'est une approche m\u00e9thodique qui n\u00e9cessite des donn\u00e9es de qualit\u00e9 et une \u00e9valuation continue pour s'assurer que le mod\u00e8le reste performant et pertinent.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#deep-learning","title":"Deep Learning","text":"<p>Le Deep Learning est une technique avanc\u00e9e de machine learning qui utilise des r\u00e9seaux de neurones (une sorte d'algorithme math\u00e9matique avec de nombreux chiffres et \u00e9quations) pour analyser des donn\u00e9es complexes. Sa force r\u00e9side dans sa capacit\u00e9 \u00e0 traiter les donn\u00e9es de fa\u00e7on hi\u00e9rarchique, un peu comme notre cerveau (ou du moins, on essaie d'imiter math\u00e9matiquement son fonctionnement).</p> <p>Les r\u00e9seaux de neurones profonds ont permis des avanc\u00e9es spectaculaires dans des t\u00e2ches n\u00e9cessitant une compr\u00e9hension fine, comme la reconnaissance de la voix ou la compr\u00e9hension du langage naturel. Cependant, cette sophistication a un co\u00fbt : les ressources n\u00e9cessaires sont cons\u00e9quentes, que ce soit en termes de puissance de calcul ou de volume de donn\u00e9es requis pour l'entra\u00eenement (= les donn\u00e9es historiques).</p> <p>Ainsi, lorsqu'on envisage d'utiliser le deep learning, il est crucial d'\u00e9valuer si l'investissement en ressources est justifi\u00e9 par les b\u00e9n\u00e9fices escompt\u00e9s. Pour beaucoup d'applications, il pourrait \u00eatre plus pragmatique de commencer avec des techniques de Machine Learning plus simples, avant d'\u00e9voluer vers le Deep Learning si n\u00e9cessaire.</p> <p>Ces deux composantes, bien que puissantes, n\u00e9cessitent une approche r\u00e9fl\u00e9chie et strat\u00e9gique pour \u00eatre int\u00e9gr\u00e9es efficacement dans vos projets. Une compr\u00e9hension claire de chaque technique, combin\u00e9e \u00e0 une \u00e9valuation m\u00e9thodique des erreurs et des performances, est la cl\u00e9 pour r\u00e9ussir dans le monde passionnant de l'IA.</p> <p>Pour mieux comprendre les relations entre l'Intelligence Artificielle, le Machine Learning et le Deep Learning, voici une repr\u00e9sentation visuelle qui illustre comment ces domaines s'imbriquent les uns dans les autres :</p> <p></p> <p>Cette image montre clairement que le Deep Learning est un sous-ensemble du Machine Learning, qui est lui-m\u00eame une branche de l'Intelligence Artificielle. Chaque couche repr\u00e9sente une approche plus sp\u00e9cialis\u00e9e et sophistiqu\u00e9e, avec des techniques et des applications sp\u00e9cifiques.</p> <p>Cette hi\u00e9rarchie nous rappelle que, bien que le Deep Learning soit actuellement au c\u0153ur de nombreuses innovations spectaculaires, il s'inscrit dans un \u00e9cosyst\u00e8me plus large de m\u00e9thodes et d'approches qui constituent ensemble le domaine de l'Intelligence Artificielle.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#les-domaines-de-lia","title":"Les domaines de l'IA","text":"<p>Dans le vaste univers de l'intelligence artificielle, il est crucial de comprendre que diff\u00e9rents domaines existent, chacun ayant des applications sp\u00e9cifiques et des relations \u00e9troites avec les composantes du machine learning et du deep learning. Dans cette section, je vais pr\u00e9senter un aper\u00e7u de chaque domaine majeur de l'IA, en expliquant comment ils s'appuient sur le machine learning et le deep learning pour fonctionner efficacement.</p> <p>On peut voir un domaine de l'IA comme une sp\u00e9cialit\u00e9. Imaginez que l'IA est la m\u00e9decine. Les diff\u00e9rents domaines de la m\u00e9decine sont la cardiologie, la dentisterie, etc. C'est pareil pour l'IA. Les domaines de l'IA peuvent \u00eatre l'imagerie, les moteurs de recommandation, le traitement du langage, etc. Et dans chaque domaine, on peut appliquer du machine learning ou du deep learning.</p> <p>Comprendre l'intelligence artificielle, c'est aussi d\u00e9couvrir la diversit\u00e9 de ses domaines d'application et la fa\u00e7on dont ils transforment notre quotidien.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#vision-par-ordinateur-voir-pour-comprendre","title":"Vision par ordinateur : Voir pour comprendre","text":"<p>Le domaine de la vision par ordinateur se concentre sur la capacit\u00e9 des machines \u00e0 analyser et interpr\u00e9ter des images et des vid\u00e9os. Que ce soit pour reconna\u00eetre des visages sur Facebook ou pour permettre \u00e0 une voiture autonome de \"voir\" la route, les applications sont vari\u00e9es et en constante \u00e9volution.</p> <p>L'analyse vid\u00e9o en temps r\u00e9el, essentielle pour des applications telles que la s\u00e9curit\u00e9 ou le sport, d\u00e9pend fortement de cette technologie. Mais attention, toute cette magie visuelle repose sur des techniques avanc\u00e9es d'apprentage supervis\u00e9 (o\u00f9 l'algorithme apprend \u00e0 partir d'exemples \u00e9tiquet\u00e9s par des humains) et d'\u00e9normes ensembles de donn\u00e9es annot\u00e9es. Sans ces fondations solides, m\u00eame les algorithmes les plus prometteurs risquent de tr\u00e9bucher.</p> <p>Il est important de souligner que la vision par ordinateur est un domaine extr\u00eamement vaste et complexe. Ma\u00eetriser ses diff\u00e9rentes facettes, de la d\u00e9tection d'objets \u00e0 la segmentation d'images, et la reconnaissance faciale (et beaucoup d'autres...) peut n\u00e9cessiter plusieurs ann\u00e9es d'\u00e9tude et de pratique. Chaque sous-domaine poss\u00e8de ses propres d\u00e9fis techniques et m\u00e9thodologiques, et les chercheurs continuent d'innover constamment pour repousser les limites de ce que les machines peuvent \"voir\" et comprendre.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#traitement-du-langage-naturel-nlp-communiquer-avec-les-machines","title":"Traitement du Langage Naturel (NLP) : Communiquer avec les machines","text":"<p>Le NLP est la branche de l'IA qui se penche sur la compr\u00e9hension et la g\u00e9n\u00e9ration du langage humain par les machines. Que ce soit pour traduire un texte ou interagir avec un assistant vocal, la capacit\u00e9 \u00e0 comprendre le langage est primordiale.</p> <p>Pourtant, comme dans toute relation, la communication n'est pas d\u00e9pourvue de d\u00e9fis. Les nuances, les contextes culturels, et les langues rendent la t\u00e2che complexe. Des avanc\u00e9es comme les mod\u00e8les de langage GPT tentent d'apprivoiser cette complexit\u00e9, mais un mod\u00e8le mal entra\u00een\u00e9 peut \"halluciner\" (inventer des informations) ou produire des r\u00e9ponses inappropri\u00e9es.</p> <p>Comme le domaine de la vision par ordinateur, le traitement du langage naturel est un domaine extr\u00eamement vaste et complexe. Ma\u00eetriser ses diff\u00e9rentes facettes et apporter des solutions adapt\u00e9es \u00e0 chaque probl\u00e9matique n\u00e9cessitent plusieurs ann\u00e9es d'expertise.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#series-temporelles-predire-lavenir","title":"S\u00e9ries temporelles : Pr\u00e9dire l'avenir","text":"<p>Les s\u00e9ries temporelles constituent \u00e9galement un domaine \u00e0 part enti\u00e8re. Lorsqu'il s'agit d'analyser des donn\u00e9es ordonn\u00e9es dans le temps, comme les pr\u00e9visions m\u00e9t\u00e9orologiques ou les cours de la bourse, les s\u00e9ries temporelles sont essentielles.</p> <p>Ces analyses permettent non seulement de comprendre des tendances pass\u00e9es mais aussi de pr\u00e9dire des \u00e9v\u00e9nements futurs. Mais attention, pr\u00e9dire l'avenir reste risqu\u00e9 avec une part d'erreur (ce qui n'emp\u00eache pas les pr\u00e9dictions de ramener de la valeur). Les mod\u00e8les doivent \u00eatre constamment ajust\u00e9s, notant que des approches rigides peuvent manquer de flexibilit\u00e9 face \u00e0 des \u00e9v\u00e9nements impr\u00e9vus.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#systemes-de-recommandation-personnaliser-lexperience-utilisateur","title":"Syst\u00e8mes de Recommandation : Personnaliser l'exp\u00e9rience utilisateur","text":"<p>Les syst\u00e8mes de recommandation sont un domaine \u00e0 part enti\u00e8re de l'IA, focalis\u00e9 sur la cr\u00e9ation d'exp\u00e9riences personnalis\u00e9es. Ils ne se contentent pas d'analyser vos habitudes, ils utilisent des m\u00e9thodes uniques pour anticiper vos d\u00e9sirs.</p> <p>Prenons l'exemple de Netflix : lorsque vous terminez une s\u00e9rie, le syst\u00e8me analyse vos pr\u00e9f\u00e9rences et propose imm\u00e9diatement des films ou s\u00e9ries qui pourraient vous plaire, en tenant compte de ce que d'autres utilisateurs ayant des go\u00fbts similaires ont appr\u00e9ci\u00e9. Ce domaine se distingue par son approche d\u00e9di\u00e9e \u00e0 enrichir continuellement votre interaction avec les plateformes.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#apprentissage-sur-donnees-tabulaires-analyser-les-tableaux","title":"Apprentissage sur donn\u00e9es tabulaires : Analyser les tableaux","text":"<p>L'apprentissage sur donn\u00e9es tabulaires (comme les donn\u00e9es Excel) est une discipline qui se concentre sur l'utilisation des donn\u00e9es structur\u00e9es pour prendre des d\u00e9cisions claires. Imaginez une entreprise qui cherche \u00e0 pr\u00e9voir ses ventes futures : en analysant des tableaux de donn\u00e9es contenant l'historique des ventes, les prix, et les p\u00e9riodes de l'ann\u00e9e, elle peut anticiper les tendances et ajuster ses strat\u00e9gies de marketing.</p> <p>Ce domaine se distingue par sa capacit\u00e9 \u00e0 transformer des donn\u00e9es en actions concr\u00e8tes, offrant un support essentiel aux d\u00e9cisions strat\u00e9giques des entreprises.</p> <p>Ces domaines, bien qu'ils partagent certains concepts avec d'autres branches de l'IA, n\u00e9cessitent une approche sp\u00e9cialis\u00e9e et une compr\u00e9hension approfondie des d\u00e9fis singuliers qu'ils pr\u00e9sentent.</p> <p>Une chose \u00e0 retenir : il est essentiel de comprendre que l'IA est intrins\u00e8quement li\u00e9e aux types de donn\u00e9es et aux probl\u00e9matiques sp\u00e9cifiques qu'elle aborde. Comme soulign\u00e9 pr\u00e9c\u00e9demment, l'IA ne peut exister sans donn\u00e9es. Chaque domaine de l'IA peut tirer parti des techniques de machine learning et de deep learning, selon les avanc\u00e9es scientifiques. Par exemple, pour les donn\u00e9es tabulaires, telles que la pr\u00e9diction des prix immobiliers, le machine learning et les analyses statistiques sont souvent privil\u00e9gi\u00e9s. \u00c0 l'inverse, pour la vision par ordinateur, qui implique le traitement d'images comme la reconnaissance faciale, le deep learning est g\u00e9n\u00e9ralement plus adapt\u00e9. Ces choix sont guid\u00e9s par les recherches actuelles et les m\u00e9thodes \u00e9prouv\u00e9es par la communaut\u00e9 scientifique.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#conclusion","title":"Conclusion","text":"<p>Pour conclure cette premi\u00e8re partie, nous avons explor\u00e9 ensemble les grands domaines qui composent l\u2019intelligence artificielle. Chacun poss\u00e8de ses sp\u00e9cificit\u00e9s, ses d\u00e9fis et ses applications, ce qui rend l\u2019IA aussi riche que passionnante. Bien entendu, il existe de nombreux autres sous-domaines, parfois plus discrets ou \u00e9mergents, que je n\u2019ai pas pu aborder ici. L\u2019essentiel \u00e0 retenir est que, finalement, comprendre l\u2019intelligence artificielle consiste \u00e0 comprendre quelques concepts de base qui sont les fondations de l'IA.</p> <p>Dans la suite, nous plongerons dans l'univers passionnant de l'IA g\u00e9n\u00e9rative. En attendant la partie deux, n'h\u00e9sitez pas \u00e0 vous abonner \u00e0 ma newsletter \ud83d\ude00 \u00c0 bient\u00f4t ! </p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/","title":"Mais c'est quoi le RAG vraiment ? D\u00e9finition, fonctionnement, limites et conseils","text":"","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/#introduction-au-rag-retrieval-augmented-generation","title":"Introduction au RAG (Retrieval-Augmented Generation)","text":"<p>Tout le monde a plus ou moins entendu parler du RAG (Retrieval-Augmented Generation). Mais c'est quoi le RAG exactement ? Beaucoup l'ont m\u00eame d\u00e9j\u00e0 impl\u00e9ment\u00e9, parfois avec des outils \"no-code\" ou des librairies Python comme LangChain ou LlamaIndex. C'est simple \u00e0 mettre en place, mais je vois aussi pas mal de gens d\u00e9\u00e7us du r\u00e9sultat. En r\u00e9alit\u00e9, il faut surtout comprendre \u00e0 quoi \u00e7a sert et comment \u00e7a fonctionne pour savoir si c'est adapt\u00e9 \u00e0 votre besoin.</p> <p>Au d\u00e9but, je ne comptais pas r\u00e9expliquer le RAG ici, il existe d\u00e9j\u00e0 plein de ressources sur le sujet. Mais en discutant avec des personnes qui veulent l'utiliser en entreprise, je me rends compte qu'on passe souvent \u00e0 c\u00f4t\u00e9 de l'essentiel : \u00e0 quoi \u00e7a sert vraiment un RAG, et comment \u00e7a marche concr\u00e8tement.</p> <p>Je vais donc essayer de revenir sur les points que j'ai l'habitude d'\u00e9claircir quand on me pose la question.</p>","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/#la-facilite-dimplementation-du-rag","title":"La facilit\u00e9 d'impl\u00e9mentation du RAG","text":"<p>Mettre en place un RAG, c'est facile. C'est m\u00eame trop facile : on suit un tuto, on branche deux librairies, et hop, \u00e7a tourne. Mais attention, le r\u00e9sultat n'est pas toujours \u00e0 la hauteur des attentes (spoiler : souvent, on est d\u00e9\u00e7u).</p> <p>Justement, cette simplicit\u00e9 cache un pi\u00e8ge. Pour une question tr\u00e8s basique, le RAG peut donner l'impression que tout fonctionne parfaitement. Mais d\u00e8s qu'on sort un peu du cadre, on se rend vite compte que les r\u00e9ponses ne suivent plus.</p> <p>C'est l\u00e0 qu'on peut passer beaucoup de temps \u00e0 bricoler, \u00e0 optimiser les mauvaises briques, sans forc\u00e9ment comprendre o\u00f9 est le vrai probl\u00e8me. Et c'est normal\u202f: comme tout projet d'IA, le RAG est plus complexe qu'il n'y para\u00eet. Il faut vraiment comprendre comment chaque brique fonctionne pour \u00e9viter de tourner en rond. J'ai d\u00e9j\u00e0 \u00e9crit des articles sur comment am\u00e9liorer le RAG ici ou sur l'analyse d'erreur pour comprendre ce qui coince ici.</p> <p>Mais revenons aux bases du RAG dans cet article.</p>","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/#cest-quoi-le-rag-et-pourquoi-lutiliser-en-entreprise","title":"C'est quoi le RAG et pourquoi l'utiliser en entreprise ?","text":"<p>Avant de parler du RAG, il faut d\u00e9j\u00e0 comprendre c'est quoi le RAG et pourquoi on en a besoin. Depuis l'arriv\u00e9e de ChatGPT, on a tous vu \u00e0 quel point les mod\u00e8les de langage (appel\u00e9s LLM) sont puissants. Mais il y a une limite : ils ne connaissent pas nos donn\u00e9es \u00e0 nous, ni les infos internes d'une entreprise.</p> <p>Prenons un exemple tout simple : j'ai une documentation sur une page Word. Si je veux poser des questions \u00e0 l'IA sur ce document, je peux copier le texte dans ChatGPT : l'IA va le lire et r\u00e9pondre, parce que j'ai mis le texte directement dans ce qu'on appelle le contexte du mod\u00e8le (ou le prompt). </p> <p>Sauf que la fen\u00eatre de contexte est limit\u00e9e \u00e0 un certain nombre de mots (ou plus pr\u00e9cis\u00e9ment de tokens). On peut charger une page, mais pas 10 000 d'un coup. En entreprise, on a souvent des documents tr\u00e8s volumineux, et on aimerait qu'une IA puisse r\u00e9pondre \u00e0 des questions sur l'ensemble de ces documents.</p> <p>La solution qui a \u00e9merg\u00e9 est la suivante : \u00e0 chaque question, on s\u00e9lectionne seulement quelques extraits de la documentation qui sont pertinents, et on les ins\u00e8re dans le contexte (ou prompt) du mod\u00e8le.</p> <p>C'est exactement ce que fait le RAG : il permet, pour chaque question, de choisir les extraits pertinents et de les ajouter au contexte pour que l'IA puisse r\u00e9pondre, m\u00eame sur de tr\u00e8s gros volumes de documents, si vous aimez lire, AWS en parle tr\u00e8s bien ici aussi : (aws.amazon.com)</p>","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/#comment-fonctionne-un-systeme-rag","title":"Comment fonctionne un syst\u00e8me RAG ?","text":"<p>Pour faire simple, la premi\u00e8re \u00e9tape du RAG, c'est de stocker les documents dont on a besoin dans une base de donn\u00e9es vectorielle. Mais comme un mod\u00e8le de langage ne peut pas tout lire d'un coup (\u00e0 cause de la fameuse limite de contexte), on d\u00e9coupe chaque document en petits morceaux, qu'on appelle des chunks. Ensuite, on transforme chaque chunk en un vecteur qui capture sa signification (sa s\u00e9mantique). C'est ce qu'on appelle la vectorisation\u202f: \u00e7a permet de comparer rapidement la question de l'utilisateur avec tous les morceaux de documents, pour trouver ceux qui sont les plus proches.</p> <p>Vectoriser un texte consiste \u00e0 le transformer en un vecteur qui capture sa signification (sa s\u00e9mantique). Cela permet de comparer facilement la similarit\u00e9 entre la question de l'utilisateur et les diff\u00e9rents fragments de documents. Par exemple, si \"un chat blanc\" est repr\u00e9sent\u00e9 par [1, 1], \"un chat noir\" par [1, 0], et \"un chien noir\" par [7, 1], on voit que \"un chat noir\" est plus proche de \"un chat blanc\" que de \"un chien noir\". Ce principe permet d'identifier rapidement les passages les plus pertinents \u00e0 ins\u00e9rer dans le contexte du mod\u00e8le.</p> <p>L'\u00e9tape de vectorisation est cruciale\u202f: c'est elle qui permet de retrouver les bons documents quand on pose une question. Pour \u00e7a, on utilise ce qu'on appelle des mod\u00e8les d'embeddings. Leur r\u00f4le\u202fest de transformer le texte en un vecteur qui capture sa signification, sa \"s\u00e9mantique\". Ces mod\u00e8les sont eux-m\u00eames des IA, entra\u00een\u00e9es sp\u00e9cialement pour cette t\u00e2che, pour en savoir plus.</p> <p>Comme on peut le voir, il y a d\u00e9j\u00e0 un vrai travail en amont\u202f: il faut bien pr\u00e9parer les documents. La performance du RAG d\u00e9pend beaucoup de deux choses\u202f: comment on d\u00e9coupe les documents (la taille et la m\u00e9thode de d\u00e9coupage, ce qu'on appelle le chunking), et la qualit\u00e9 du mod\u00e8le d'embeddings qu'on utilise pour transformer ces morceaux en vecteurs. Rien que sur ces deux points, on a d\u00e9j\u00e0 de quoi am\u00e9liorer les futurs r\u00e9sultats.</p> <p>Une fois la base de donn\u00e9es pr\u00eate, on passe \u00e0 l'\u00e9tape suivante\u202f: quand un utilisateur pose une question, on transforme cette question en vecteur, puis on cherche dans la base les morceaux (chunks) les plus proches, c'est-\u00e0-dire ceux qui ressemblent le plus \u00e0 la question. Cette recherche se fait en comparant la distance entre le vecteur de la question et ceux des chunks d\u00e9j\u00e0 stock\u00e9s. On r\u00e9cup\u00e8re alors quelques chunks (le nombre est d\u00e9fini \u00e0 l'avance), et on les envoie au mod\u00e8le de langage pour qu'il puisse r\u00e9pondre. Au final, le prompt envoy\u00e9 \u00e0 l'IA ressemble \u00e0 \u00e7a\u202f:</p> <pre><code>Voici les chunks pertinents pour la question : \nChunk 1 : &lt;TEXT DU CHUNK 1&gt;\nChunk 2 : &lt;TEXT DU CHUNK 2&gt;\n...\n\nVoici la question de l'utilisateur : Dans quel document je peux trouver des informations concernant le planning du moteur avec la r\u00e9f\u00e9rence X2D2E?\n</code></pre> <p>Dans cette deuxi\u00e8me phase, plusieurs param\u00e8tres peuvent \u00eatre optimis\u00e9s pour am\u00e9liorer le RAG : - La recherche en utilisant la question peut ne pas \u00eatre suffisante.  - Le nombre de chunks r\u00e9cup\u00e9r\u00e9s peut \u00eatre trop faible ou trop grand. - La qualit\u00e9 du mod\u00e8le LLM qui g\u00e9n\u00e8re la r\u00e9ponse peut \u00eatre am\u00e9lior\u00e9e.</p> <p>\u00c0 partir de cette base, on peut commencer \u00e0 exp\u00e9rimenter et \u00e9valuer le RAG. Pour ensuite l'am\u00e9liorer en analysant les erreurs et en changeant les param\u00e8tres. Si vous \u00eates \u00e0 l'\u00e9tape de l'\u00e9valuation, je vous invite \u00e0 lire mon article sur l'analyse d'erreur pour comprendre ce qui coince ici.</p>","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/#a-quoi-ca-sert-vraiment-le-rag-quelles-sont-ses-limites","title":"\u00c0 quoi \u00e7a sert vraiment le RAG ? Quelles sont ses limites ?","text":"<p>Je pense que c'est la question la plus importante. Le RAG ne permet pas de r\u00e9pondre \u00e0 toutes les questions que vous pouvez lui poser, en raison de certaines limites. Dans sa version la plus basique et la plus simple, celle que je d\u00e9taille ici, il ne permet de r\u00e9pondre qu'\u00e0 des questions directes qui ciblent un contenu limit\u00e9.</p> <p>Je m'explique. Si on lui demande de r\u00e9pondre \u00e0 une question tr\u00e8s large, il est possible que le nombre de chunks r\u00e9cup\u00e9r\u00e9s ne soit pas suffisant pour r\u00e9pondre \u00e0 la question.</p> <p>O\u00f9 le RAG fonctionne bien : - Acc\u00e8s \u00e0 des informations sp\u00e9cialis\u00e9es : documents internes ou connaissances m\u00e9tier non disponibles dans les mod\u00e8les de base - Fournir une information pr\u00e9cise : le RAG est particuli\u00e8rement efficace pour r\u00e9pondre \u00e0 des questions cibl\u00e9es en allant chercher directement l'information demand\u00e9e dans les documents fournis</p> <p>O\u00f9 le RAG montre ses limites : - Raisonnement it\u00e9ratif limit\u00e9 : Le RAG ne sait pas raisonner en plusieurs \u00e9tapes pour affiner sa recherche. Il ne v\u00e9rifie pas si les documents r\u00e9cup\u00e9r\u00e9s sont vraiment les plus pertinents ou si l'information est compl\u00e8te. Par exemple, pour une question complexe, il va juste ramener les passages les plus proches s\u00e9mantiquement, sans \"comprendre\" le contexte global comme le ferait un humain. - D\u00e9pendance \u00e0 l'organisation des donn\u00e9es : Si les documents sont mal structur\u00e9s ou mal index\u00e9s, la recherche sera inefficace. Une bonne organisation, des m\u00e9tadonn\u00e9es et une structuration claire sont essentiels. - Qualit\u00e9 et biais des sources : Le RAG ne fait que transmettre ce qu'il trouve. Si les documents sont incomplets, obsol\u00e8tes ou biais\u00e9s, la r\u00e9ponse le sera aussi, pour en savoir plus : elastic.co</p> <p>\u00c0 retenir : - Le RAG ne garantit pas la compl\u00e9tude ni la v\u00e9racit\u00e9 des r\u00e9ponses. Il y a toujours un risque d'hallucination ou d'erreur. - Pour fiabiliser un RAG, il faut : bien organiser les donn\u00e9es, surveiller la qualit\u00e9 des sources, calibrer les param\u00e8tres (chunking, embeddings, etc.), et permettre la citation des sources pour pouvoir v\u00e9rifier les informations si besoin. - L'\u00e9valuation et l'am\u00e9lioration d'un RAG se font surtout en conditions r\u00e9elles, car les probl\u00e8mes apparaissent \u00e0 l'usage.</p>","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/#conclusion-le-rag-est-il-utile-pour-vos-projets-dia","title":"Conclusion : Le RAG est-il utile pour vos projets d'IA ?","text":"<p>Le RAG est vraiment utile, \u00e0 condition de prendre le temps de bien le mettre en place et de l'am\u00e9liorer au fil du temps. Ce n'est pas un outil qu'on impl\u00e9mente en vitesse pour le laisser tourner tout seul\u202f: il faut l'\u00e9valuer, l'ajuster, et corriger ce qui ne va pas pour qu'il soit vraiment efficace.</p> <p>Les limites que j'ai \u00e9voqu\u00e9es ne sont pas simples \u00e0 \u00e9liminer, mais il existe des solutions pour les att\u00e9nuer. On parle souvent d'Agentic RAG par exemple pour am\u00e9liorer certains aspects du RAG. Si vous cherchez un syst\u00e8me qui donne 100\u202f% de bonnes r\u00e9ponses, le RAG (et l'IA en g\u00e9n\u00e9ral) n'est pas faite pour vous. Mais si vous \u00eates pr\u00eat \u00e0 viser 90-95\u202f% de r\u00e9ponses correctes, et \u00e0 investir un peu de temps pour bien l'impl\u00e9menter, alors le RAG peut vraiment devenir votre meilleur alli\u00e9.</p> <p>Si vous voulez en savoir plus sur le RAG, m\u00eame le gouvernement a publi\u00e9 un guide pour faire du RAG : Guide de la g\u00e9n\u00e9ration augment\u00e9e par r\u00e9cup\u00e9ration (RAG).</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/","title":"Mon RAG ne marche pas : pourquoi l\u2019analyse d\u2019erreur change tout","text":"","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#introduction","title":"Introduction","text":"<p>J\u2019ai d\u00e9j\u00e0 \u00e9crit un article sur comment am\u00e9liorer le RAG ici, mais le sujet est tellement vaste qu\u2019il y a toujours de nouvelles choses \u00e0 partager. D\u2019autant plus que j'entends souvent des remarques comme\u202f: \"Je ne comprends pas, pourtant j\u2019ai ajout\u00e9 [la techno \u00e0 la mode], mais le r\u00e9sultat n\u2019est pas bon.\"</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#le-rag-nest-pas-magique-et-cest-normal","title":"Le RAG n\u2019est pas magique (et c\u2019est normal)","text":"<p>Le RAG, c\u2019est un peu LE projet \u00e0 la mode depuis le d\u00e9but de l\u2019IA g\u00e9n\u00e9rative. Tout le monde veut son assistant boost\u00e9 \u00e0 l\u2019IA, capable de r\u00e9pondre \u00e0 n\u2019importe quelle question sur ses donn\u00e9es. On trouve des tutos \"RAG en deux lignes\", des outils \"no-code\", et \u00e7a donne l\u2019impression que c\u2019est simple. Mais la r\u00e9alit\u00e9, c\u2019est qu\u2019une fois le projet en place, les tests sont rarement aussi magiques qu\u2019esp\u00e9r\u00e9. L\u2019IA ne r\u00e9pond pas \u00e0 tout, hallucine parfois, ou passe compl\u00e8tement \u00e0 c\u00f4t\u00e9 d\u2019une question basique. Et l\u00e0, grosse frustration.</p> <p>Premi\u00e8re chose \u00e0 retenir\u202f: aucun syst\u00e8me IA ne peut avoir 100\u202f% de bonnes r\u00e9ponses. Il faut surtout se poser la question: quel niveau d\u2019erreur je suis pr\u00eat \u00e0 accepter. La vraie valeur, on l\u2019obtient en comprenant bien le probl\u00e8me qu\u2019on veut r\u00e9soudre, pas en cherchant la perfection.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#quand-on-veut-vraiment-ameliorer-le-rag","title":"Quand on veut vraiment am\u00e9liorer le RAG","text":"<p>Si vous \u00eates convaincu que le RAG est le bon choix, alors il faut investir du temps... mais pas n\u2019importe comment. La premi\u00e8re version est souvent bluffante, mais tr\u00e8s vite, on voit les limites. Les retours utilisateurs sont clairs\u202f: - certaines questions restent sans r\u00e9ponse, - il y a des erreurs \"b\u00eates\", - et l\u2019utilisation devient frustrante.</p> <p>\u00c0 chaque fois, la question c\u2019est\u202f: \"Qu\u2019est-ce qu\u2019on fait maintenant\u202f? On ajoute une nouvelle techno\u202f? On change de mod\u00e8le\u202f?\"</p> <p>Ma r\u00e9ponse\u202f: on analyse. Analyser, ce n\u2019est pas juste regarder les logs ou changer des param\u00e8tres au hasard. C\u2019est d\u00e9cortiquer chaque \u00e9chec pour comprendre d\u2019o\u00f9 il vient. Avant d\u2019ajouter quoi que ce soit, il faut comprendre\u202f: c\u2019est la base du m\u00e9tier, que ce soit en data science, en ML, ou en stats.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#exemples-concrets-danalyse-derreur","title":"Exemples concrets d\u2019analyse d\u2019erreur","text":"<p>Parce que c\u2019est plus parlant, voici deux exemples v\u00e9cus\u202f:</p> <p>1. Quand la recherche vectorielle fait d\u00e9faut Sur un projet, tout semblait marcher... sauf que certaines requ\u00eates avec des mots-cl\u00e9s pr\u00e9cis ne donnaient rien, alors que la r\u00e9ponse \u00e9tait bien dans la base. Apr\u00e8s analyse, on a vu que la recherche vectorielle ne captait pas certains synonymes ou formulations. On a donc ajout\u00e9 une recherche BM25 (bas\u00e9e sur les mots-cl\u00e9s) en plus du vectoriel. R\u00e9sultat\u202f: les questions \"difficiles\" trouvaient enfin des r\u00e9ponses.</p> <p>2. Les attributs m\u00e9tiers oubli\u00e9s Dans un e-commerce, impossible de sortir les produits d\u2019une couleur pr\u00e9cise (\"je veux un t-shirt rouge\"), alors que les donn\u00e9es \u00e9taient l\u00e0. L\u2019analyse a montr\u00e9 que la s\u00e9mantique de la couleur n\u2019\u00e9tait pas bien captur\u00e9e dans les vecteurs d'embeddings. On a simplement ajout\u00e9 un filtrage par m\u00e9tadonn\u00e9e avant de passer \u00e0 l\u2019IA : probl\u00e8me r\u00e9gl\u00e9.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#comment-mener-lanalyse-derreur","title":"Comment mener l\u2019analyse d\u2019erreur\u202f?","text":"<p>Voil\u00e0 comment je m\u2019y prends, et franchement, \u00e7a marche dans 90\u202f% des cas\u202f:</p> <p>1 : Prendre un \u00e9chantillon d\u2019exemples o\u00f9 le RAG se plante. 2 : Pour chaque cas, se demander\u202f:    - Est-ce que le retrieval trouve quelque chose ou non\u202f?    - Est-ce que la g\u00e9n\u00e9ration hallucine\u202f?    - Est-ce que l\u2019info existe vraiment dans la base\u202f?    - Est-ce un probl\u00e8me de format, de m\u00e9tadonn\u00e9e, de formulation\u202f? 3 : Cat\u00e9goriser les erreurs (retrieval, ranking, hallucination, data...). 4 : Tester des corrections simples... avant de tout changer.</p> <p>Et surtout\u202f: noter ce qui revient le plus souvent, pour prioriser les vraies am\u00e9liorations.</p> <p>Pour commencer, toutes les analyses d\u2019erreur doivent se faire \u00e0 la main. C\u2019est indispensable pour vraiment comprendre d\u2019o\u00f9 viennent les probl\u00e8mes et comment fonctionnent les diff\u00e9rents frameworks RAG. Mais soyons honn\u00eates : \u00e0 un moment, quand le volume de requ\u00eates augmente, \u00e7a devient vite ing\u00e9rable. C\u2019est l\u00e0 que de bons outils deviennent indispensables pour garder une vision claire de ce qui se passe \u00e0 chaque \u00e9tape.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#quels-outils-choisir","title":"Quels outils choisir ?","text":"<p>LangFuse est probablement l\u2019un des plus pratiques (et open-source) pour tracer tout le pipeline RAG\u202f: on visualise chaque \u00e9tape, de la requ\u00eate originale aux chunks r\u00e9cup\u00e9r\u00e9s, le prompt final envoy\u00e9 au LLM, et la r\u00e9ponse g\u00e9n\u00e9r\u00e9e. Id\u00e9al pour rep\u00e9rer pr\u00e9cis\u00e9ment o\u00f9 \u00e7a d\u00e9raille.</p> <p>LangSmith fait la m\u00eame chose que Langfuse, avec une interface diff\u00e9rente. L\u2019avantage\u202f: si vous utilisez d\u00e9j\u00e0 LangChain, l\u2019int\u00e9gration est naturelle.</p> <p>Pour aller plus loin dans le suivi, Weights &amp; Biases permet de tracker les m\u00e9triques de performance dans le temps et de comparer diff\u00e9rentes versions du syst\u00e8me. Pratique pour v\u00e9rifier si une \"am\u00e9lioration\" n\u2019a pas cass\u00e9 autre chose ailleurs.</p> <p>Et puis il y a les solutions maison,\u202fqui permettent de garder le contr\u00f4le sur tout : un logging JSON bien structur\u00e9, qui enregistre les prompts, les chunks r\u00e9cup\u00e9r\u00e9s et les r\u00e9ponses du LLM.</p> <p>L\u2019id\u00e9e, c\u2019est de ne pas se perdre dans la surench\u00e8re de dashboards\u202f: il faut juste assez de visibilit\u00e9 pour comprendre rapidement o\u00f9 chercher quand quelque chose cloche.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#ce-quil-faut-retenir","title":"Ce qu\u2019il faut retenir","text":"<p>Le RAG, ce n\u2019est ni magique, ni parfait. Ce qui fait la diff\u00e9rence, ce n\u2019est pas la derni\u00e8re techno ajout\u00e9e, mais la capacit\u00e9 \u00e0 comprendre pourquoi \u00e7a rate et \u00e0 it\u00e9rer intelligemment. L\u2019analyse d\u2019erreur, c\u2019est la base\u202f: avant d\u2019empiler les couches de complexit\u00e9, prenez le temps de regarder, d\u2019\u00e9couter les utilisateurs, et de corriger \u00e0 la source.</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/","title":"Comment r\u00e9ussir \u00e0 rester \u00e0 jour en IA g\u00e9n\u00e9rative ?","text":"","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#introduction","title":"Introduction","text":"<p>Aborder l'IA en g\u00e9n\u00e9ral est d\u00e9j\u00e0 assez complexe. D\u2019un point de vue externe, on n\u2019a pas id\u00e9e \u00e0 quel point le domaine est vaste. Et une fois qu\u2019on est dedans, c\u2019est infini. On peut passer toute une carri\u00e8re sur une petite sp\u00e9cialit\u00e9 de l\u2019IA, comme par exemple les s\u00e9ries temporelles, ou encore travailler sur le traitement du langage (NLP), sans forc\u00e9ment savoir ce qui se passe dans les autres cat\u00e9gories d\u2019IA comme les syst\u00e8mes de recommandation, la vision par ordinateur, etc.</p> <p>Vous vous dites que c\u2019est d\u00e9j\u00e0 compliqu\u00e9, mais imaginez maintenant qu\u2019il y ait des nouveaut\u00e9s toutes les semaines. C\u2019est exactement ce qu\u2019on vit depuis 2-3 ans avec l\u2019IA g\u00e9n\u00e9rative, une cat\u00e9gorie de l\u2019IA qui \u00e9volue extr\u00eamement vite, avec de nombreux acteurs : OpenAI, Anthropic, Deepseek, Alibaba, XAi, HuggingFace, Pleias, Mistral... Bref, je pense que cela pourrait faire l\u2019objet d\u2019un article de blog \u00e0 part enti\u00e8re.</p> <p>Suivre tout cela est donc tr\u00e8s complexe : des mod\u00e8les sortent presque tous les jours, de nouvelles techniques, des produits... 24 heures ne suffisent clairement pas pour tout voir.</p> <p>Comme c\u2019est mon m\u00e9tier et que je suis passionn\u00e9 par le sujet, j\u2019ai pris l\u2019habitude de suivre tout \u00e7a au quotidien, en essayant de ne pas trop en faire et en gardant du plaisir. Je me suis donc dit qu'il serait int\u00e9ressant d\u2019\u00e9crire un article sur les sources qui me permettent de rester \u00e0 jour.</p>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#prerequis","title":"Pr\u00e9requis","text":"<p>Il y a tout de m\u00eame quelques pr\u00e9requis n\u00e9cessaires pour pouvoir suivre ce qui se passe en IA g\u00e9n\u00e9rative, en particulier sur le plan technique.</p> <p>Si vous souhaitez comprendre l'IA en g\u00e9n\u00e9ral et l'IA g\u00e9n\u00e9rative en particulier et avoir une vision globale pour commencer, je vous conseille de lire mes articles sur comprendre l'IA et comprendre l'IA g\u00e9n\u00e9rative.</p> <p>Si vous voulez aller plus loin et que vous ne savez pas par o\u00f9 commencer, voici quelques \u00e9l\u00e9ments :</p> <ul> <li>Pour avoir des notions g\u00e9n\u00e9rales en IA, un cours d'introduction peut \u00eatre pertinent : Introduction to AI</li> <li>Pour avoir des connaissances en NLP (natural language processing) faciliteront aussi la t\u00e2che. Si cela vous int\u00e9resse, je vous conseille ce cours sur le NLP</li> <li>Les cours d'Andrew Ng sur Youtube sont \u00e9galement une tr\u00e8s bonne introduction \u00e0 l'IA.</li> </ul>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#les-differentes-sources-dinformation","title":"Les diff\u00e9rentes sources d'information","text":"<p>Le contenu sera compl\u00e9t\u00e9 au fur et \u00e0 mesure, et sera aussi s\u00fbrement amen\u00e9 \u00e0 \u00e9voluer au fil du temps, en fonction de ce qui me revient en t\u00eate. Il y a des sources que j'utilise au quotidien, et d'autres que je consulte de temps en temps, donc la premi\u00e8re version de cet article ne sera pas compl\u00e8te.</p>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#les-reseaux-sociaux","title":"Les r\u00e9seaux sociaux","text":"<p>Rien d'\u00e9tonnant, les r\u00e9seaux sociaux occupent une partie tr\u00e8s importante de notre temps quotidien. S'il y a une source assez simple d'acc\u00e8s pour des informations d'actualit\u00e9 qui reprennent les derni\u00e8res nouveaut\u00e9s, et aussi pour avoir une premi\u00e8re piste de o\u00f9 chercher, c'est bien les r\u00e9seaux sociaux. </p> <p>Je dirais que les deux r\u00e9seaux sociaux que j'essaie de consulter un peu tous les jours et dont le contenu est tr\u00e8s qualitatif sont : Twitter (X) et Reddit. Sur ces r\u00e9seaux, on peut trouver pas mal de chercheurs qui travaillent \u00e0 Stanford, Google, OpenAI, Anthropic, etc. et qui partagent leurs travaux, d\u00e9couvertes, ainsi que leurs id\u00e9es presque tous les jours. Ils r\u00e9pondent souvent aux questions pour expliquer et interagissent beaucoup pour confronter les id\u00e9es. Rien qu'avec Twitter, on peut rester \u00e0 jour sur tout ce qui se fait dans l'IA g\u00e9n\u00e9rative (ou presque).</p> <p>Je passe aussi un peu de temps sur Linkedin, c'est un r\u00e9seau qui est utile pour l'actualit\u00e9 mais avec un peu moins de profondeur dans le contenu.</p> <p>Twitter (X) :</p> <ul> <li>@HamelHusain : ML Engineer qui a travaill\u00e9 \u00e0 Github et qui partage pas mal de choses sur l'IA g\u00e9n\u00e9rative, donne des cours et des conf\u00e9rences.</li> <li>@Dorialexander : CEO de pleiasfr, entreprise fran\u00e7aise peu connue mais dont les travaux de recherche b\u00e9n\u00e9ficient beaucoup \u00e0 la communaut\u00e9 open-source.</li> <li>@jeremyphoward : Chercheur, CEO, prof... Une personne avec une \u00e9norme contribution \u00e0 l'IA depuis des ann\u00e9es. Si vous connaissez la librairie FastAI qui date de quelques ann\u00e9es, c'est lui le cr\u00e9ateur.</li> <li>@karpathy : Quand je pense \u00e0 ChatGPT, je pense \u00e0 Karpathy. Il a travaill\u00e9 sur les premi\u00e8res versions de ChatGPT, il a fait des vid\u00e9os explicatives sur YouTube, et je pense que c'est une des personnes les plus influentes dans cette vague (il a lanc\u00e9 le vibe coding et d'autres vagues d'IA g\u00e9n\u00e9rative).</li> <li>@DFintelligence : ML Engineer, dont le contenu est en fran\u00e7ais et tr\u00e8s qualitatif. Il parle surtout de l'IA dans sa globalit\u00e9, beaucoup de news et de contenu tr\u00e8s bien vulgaris\u00e9 !</li> <li>@jxnlco : ML Engineer, anciennement chez Meta. Il cr\u00e9e pas mal de contenu sur les diff\u00e9rentes m\u00e9thodes en IA g\u00e9n\u00e9rative, il fait des cours, des podcasts et plein de contenu sur le sujet.</li> <li>@simonw : Cr\u00e9ateur de Django, mais fait beaucoup de contenu sur l'IA g\u00e9n\u00e9rative. Les derni\u00e8res d\u00e9couvertes, techniques, benchmarks, etc.</li> </ul> <p>Je garde un oeil aussi sur les diff\u00e9rents comptes des entreprises ou organisations dont le produit est int\u00e9ressant ou qui publient des news, des articles de recherche, etc. comme : @aiDotEngineer, @googleai, @anthropic, @openai, @huggingface, @mistralai, @xai, @deepseek_ai, @Alibaba_Qwen,</p> <p>J\u2019ai essay\u00e9 de diversifier les profils, mais il en manque encore beaucoup. J\u2019en ajouterai s\u00fbrement d\u2019autres au fil du temps, sans trop surcharger l\u2019article. Si vous souhaitez voir directement la liste compl\u00e8te des comptes que je suis sur Twitter, vous pouvez consulter mon profil.</p> <p>Reddit :</p> <p>J'ai commenc\u00e9 \u00e0 utiliser Reddit principalement pour une seule page : LocalLLaMA. Cette page regroupe de nombreuses discussions sur les nouveaut\u00e9s de l'IA g\u00e9n\u00e9rative, mais attention, elle se concentre surtout sur les mod\u00e8les open source. Le contenu y est vraiment tr\u00e8s qualitatif, et c'est la seule page Reddit que je consulte presque quotidiennement.</p> <p>Par la suite, j'ai d\u00e9couvert d'autres canaux tr\u00e8s int\u00e9ressants, que je consulte de temps en temps : - ArtificialIntelligence  - ClaudeAI  - LangChain  - OpenAI - PromptEngineering</p> <p>Github :</p> <p>Si vous voulez d\u00e9couvrir des librairies open-source autour de l'IA g\u00e9n\u00e9rative, github peut \u00eatre un bon endroit. Il y a les plus connues Langchain, HuggingFace, LLamaindex, etc. Mais la liste est vraiment tr\u00e8s longue, et vous pouvez la trouver dans mes favoris github : Generative AI</p> <p>Si vous avez un besoin bien pr\u00e9cis on peut discuter des diff\u00e9rents libraires par message sur LinkedIn ! :)</p>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#youtube","title":"Youtube","text":"<p>YouTube, c'est l'endroit o\u00f9 je passe pas mal de temps de mon c\u00f4t\u00e9. Que je sois en d\u00e9placement, en train de marcher, de faire du sport ou une t\u00e2che secondaire, j'ai toujours une vid\u00e9o \u00e0 regarder. Parmi les vid\u00e9os que je regarde, certaines sont li\u00e9es \u00e0 l'actualit\u00e9 de l'IA, d'autres \u00e0 des techniques plus avanc\u00e9es ou \u00e0 des contenus \u00e9ducatifs pour continuer \u00e0 apprendre. Les podcasts je les mets \u00e9galement dans cette cat\u00e9gorie.</p> <ul> <li>Andrej Karpathy : J'ai d\u00e9j\u00e0 mentionn\u00e9 son Twitter, mais ses vid\u00e9os sont vraiment tr\u00e8s bien faites et p\u00e9dagogiques.</li> <li>aiDotEngineer : Cha\u00eene tr\u00e8s int\u00e9ressante avec des invit\u00e9s qui partagent beaucoup d'informations sur l'IA g\u00e9n\u00e9rative et les tendances du domaine. Il s'agit d'une conf\u00e9rence o\u00f9 se rencontrent les plus grands labs d'IA, fondateurs, CTOs du Fortune 500 et ing\u00e9nieurs IA.</li> <li>DiscoverAI : Des vid\u00e9os o\u00f9 le youtubeur d\u00e9cortique les derniers papiers de mani\u00e8re tr\u00e8s claire et p\u00e9dagogique.</li> <li>Elvis Saravia : Beaucoup de tutoriels, de pr\u00e9sentations de papiers et de contenu autour de l'IA g\u00e9n\u00e9rative, toujours de qualit\u00e9.</li> <li>Flint : Podcasts IA avec des invit\u00e9s vari\u00e9s et du contenu qualitatif.</li> <li>Anyscale : \u00c0 la base, Anyscale est une entreprise, mais leur cha\u00eene YouTube propose du contenu tr\u00e8s int\u00e9ressant.</li> <li>HuggingFace : Les rois de l'open-source, avec des revues de papiers, tutoriels, podcasts, etc.</li> <li>vanishinggradients : Excellente cha\u00eene de podcasts avec des invit\u00e9s tr\u00e8s reconnus dans le domaine.</li> <li>Alexandre TL : Cha\u00eene d'un chercheur fran\u00e7ais, avec un contenu tr\u00e8s bien vulgaris\u00e9.</li> <li>3blue1brown : Une des cha\u00eenes les plus int\u00e9ressantes et populaires de ces derni\u00e8res ann\u00e9es. Une cha\u00eene similaire est StatQuest.</li> <li>MLOps.community : Discussions autour de l'IA en g\u00e9n\u00e9ral et du MLOps.</li> <li>MachineLearningStreetTalk : Podcasts avec diff\u00e9rents invit\u00e9s et du contenu tr\u00e8s enrichissant.</li> <li>Jason Liu : Contenu ax\u00e9 IA g\u00e9n\u00e9rative et retours sur des projets concrets, avec des discussions sur l'\u00e9valuation, les techniques, etc.</li> <li>Hamel Hussain : Un peu comme jxnlco (ils font partie de la m\u00eame \u00e9quipe), mais la cha\u00eene vaut vraiment le d\u00e9tour.</li> <li>EfficientNLP : Cha\u00eene d'un chercheur sp\u00e9cialis\u00e9 en NLP, avec des vid\u00e9os \u00e9ducatives sur le sujet.</li> <li>Sebastian Raschka : Chercheur qui propose des vid\u00e9os \u00e9ducatives. Auteur de plusieurs livres dont \"Build a Large Language Model From Scratch\".</li> </ul> <p>S'il y en a d'autres qui me reviennent en t\u00eate ou que je d\u00e9couvre, je les ajouterai au fil du temps. En g\u00e9n\u00e9ral, je d\u00e9couvre de nouvelles cha\u00eenes via les recommandations ou les r\u00e9seaux. Si vous en avez \u00e0 partager, envoyez-moi un message sur LinkedIn !</p>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#newsletters","title":"Newsletters","text":"<p>Les newsletters, c'est en quelque sorte le r\u00e9capitulatif de tout ce qui circule d\u00e9j\u00e0 sur Twitter/X. Les annonces des chercheurs ou des entreprises se font le plus souvent sur X et sont ensuite relay\u00e9es sur les autres r\u00e9seaux.</p> <ul> <li> <p>TLDR AI : newsletter avec les derni\u00e8res actualit\u00e9s IA.</p> </li> <li> <p>SMOL : Newsletter avec actualit\u00e9s vari\u00e9es comme les nouveaux mod\u00e8les, les annonces ou encore les nouveaux outils IA.</p> </li> </ul>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#blogs","title":"Blogs","text":"<ul> <li>Le blog de J\u00e9r\u00e9my Howard (fast.ai) : Une r\u00e9f\u00e9rence incontournable pour apprendre l\u2019IA et le deep learning de fa\u00e7on pratique et accessible, avec des articles et des cours de grande qualit\u00e9.</li> </ul>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#articles-de-recherche","title":"Articles de recherche","text":"<p>Les articles de recherche sont tr\u00e8s importants dans le domaine de l\u2019IA. Cela dit, il y en a \u00e9norm\u00e9ment qui sont publi\u00e9s, et souvent certains ne remplissent pas tous les crit\u00e8res, comme la relecture par les pairs. Il devient donc tr\u00e8s difficile de distinguer le vrai du faux parmi tous ces articles. Une des sources que je trouve les plus int\u00e9ressantes est Huggingface Papers, car plusieurs personnes prennent le temps de revoir les articles, ce qui permet d\u2019effectuer un premier filtre.</p> <p>Pour chaque article lu, il est important de garder un esprit critique et de v\u00e9rifier la cr\u00e9dibilit\u00e9 des auteurs ainsi que celle des diff\u00e9rentes personnes ayant examin\u00e9 l\u2019article.</p>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#sites-et-autres","title":"Sites et autres","text":"<p>La documentation d\u2019OpenAI, Anthropic, Google ou Mistral peut contenir des informations tr\u00e8s int\u00e9ressantes sur la fa\u00e7on dont ils utilisent leurs produits. De temps en temps, j\u2019y fais un saut pour lire ce qu\u2019ils publient et j\u2019apprends presque toujours quelque chose.</p> <p>Le cookbook d\u2019OpenAI est aussi une excellente ressource c\u00f4t\u00e9 code. Il montre comment utiliser leurs mod\u00e8les via l\u2019API et propose de nombreuses techniques concr\u00e8tes \u00e0 explorer.</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2024/12/11/utiliser-chatgpt-efficacement/","title":"Utiliser ChatGPT efficacement","text":"<p>Utiliser ChatGPT efficacement pour automatiser les t\u00e2ches r\u00e9p\u00e9titives est d\u00e9j\u00e0 une r\u00e9alit\u00e9 dans de nombreuses entreprises. Gr\u00e2ce \u00e0 l'intelligence artificielle g\u00e9n\u00e9rative, il est possible d'am\u00e9liorer la productivit\u00e9, de gagner du temps et d'optimiser les processus quotidiens. Dans cet article, je vais essayer de vous donner quelques pistes pour utiliser ChatGPT efficacement.</p> <p>C'est la premi\u00e8re fois qu'une IA poss\u00e8de la capacit\u00e9 de communiquer par \u00e9crit de mani\u00e8re claire, au point qu'on puisse lui demander d'\u00e9crire un article marketing, r\u00e9diger un mail, automatiser des t\u00e2ches ou r\u00e9soudre un probl\u00e8me math\u00e9matique...</p> <p>Parmi les exemples d'utilisation de ChatGPT que je viens de citer, il y en a un qu'il vaut mieux \u00e9viter de confier \u00e0 ChatGPT : r\u00e9soudre un probl\u00e8me de maths. En effet, ChatGPT n'est absolument pas fiable pour ce type de t\u00e2ches, sauf si vous connaissez d\u00e9j\u00e0 la r\u00e9ponse. </p> <p>Dans leur forme actuelle, les mod\u00e8les d'intelligence artificielle comme ChatGPT ne sont pas con\u00e7us pour r\u00e9ellement comprendre les raisonnements en g\u00e9n\u00e9ral et pr\u00e9sentent \u00e9galement d'autres limites. C'est pourquoi, pour utiliser efficacement ce type d'IA g\u00e9n\u00e9rative, il est essentiel de bien en conna\u00eetre les limites et d'adopter les bonnes pratiques. </p>","tags":["ChatGPT","LLM","Intelligence Artificielle","Conseils Pratiques","Utilisation Efficace"]},{"location":"blog/2024/12/11/utiliser-chatgpt-efficacement/#comprendre-lintelligence-artificielle-generative-derriere-chatgpt","title":"Comprendre l'intelligence artificielle g\u00e9n\u00e9rative derri\u00e8re ChatGPT","text":"<p>\u00c0 la base, ChatGPT utilise ce qu'on appelle un mod\u00e8le d'intelligence artificielle (IA). Plus pr\u00e9cis\u00e9ment, il s'agit d'un grand mod\u00e8le de langage (en anglais : Large Language Model ou LLM). En r\u00e9sum\u00e9, c'est un programme math\u00e9matique qui a \u00e9t\u00e9 entra\u00een\u00e9 en analysant des milliards de textes diff\u00e9rents. Son r\u00f4le est assez simple : il re\u00e7oit un texte en entr\u00e9e, l'analyse gr\u00e2ce \u00e0 ses connaissances internes, puis g\u00e9n\u00e8re un texte en sortie.</p> <p>Pour simplifier davantage, on peut imaginer ce mod\u00e8le comme un programme informatique compos\u00e9 de tr\u00e8s nombreux chiffres appel\u00e9s \u00ab param\u00e8tres \u00bb. Ces param\u00e8tres sont mis \u00e0 jour pendant l'entra\u00eenement de l'IA, ce qui permet au mod\u00e8le d'\u00eatre efficace pour comprendre et produire du texte.</p> <p>Mais attention : m\u00eame si ChatGPT donne souvent l'impression de produire des textes logiques et coh\u00e9rents, il ne comprend pas r\u00e9ellement ce qu'il \u00e9crit. Il se contente simplement d'assembler des mots qu'il a d\u00e9j\u00e0 vus dans ses donn\u00e9es d'entra\u00eenement, en fonction de votre demande.</p> <p>Attention, ici je parle bien des IA comme ChatGPT dans leur forme la plus basique. Aujourd'hui, ces IA ont d\u00e9j\u00e0 beaucoup \u00e9volu\u00e9 : elles peuvent d\u00e9sormais effectuer des recherches sur internet, r\u00e9pondre \u00e0 vos e-mails, automatiser des t\u00e2ches professionnelles et bien plus encore ! Ce type d'IA plus avanc\u00e9 s'appelle un \u00ab agent \u00bb. Mais j'aborderai ce sujet passionnant dans un prochain article :)</p>","tags":["ChatGPT","LLM","Intelligence Artificielle","Conseils Pratiques","Utilisation Efficace"]},{"location":"blog/2024/12/11/utiliser-chatgpt-efficacement/#quelques-exemples-des-limites-de-chatgpt","title":"Quelques exemples des limites de ChatGPT","text":"<p>M\u00eame si ChatGPT est impressionnant et tr\u00e8s utile, il est important de conna\u00eetre ses limites pour l'utiliser correctement au quotidien. Voici quelques cas concrets o\u00f9 cette technologie atteint rapidement ses limites :</p> <p>Calculs ou raisonnements math\u00e9matiques complexes</p> <p>ChatGPT peut rencontrer des difficult\u00e9s avec les calculs ou raisonnements math\u00e9matiques complexes. Par exemple, lorsqu'on lui demande de r\u00e9soudre une \u00e9quation complexe ou d'effectuer un calcul pr\u00e9cis, il peut fournir des r\u00e9ponses incorrectes ou approximatives. Il est donc pr\u00e9f\u00e9rable d'utiliser une calculatrice ou un logiciel sp\u00e9cialis\u00e9 pour ces t\u00e2ches.</p> <p>Exemple \u00e0 \u00e9viter : \u00ab Combien font exactement 3456 multipli\u00e9 par 789 ? \u00bb ChatGPT pourrait tenter de r\u00e9pondre, mais la r\u00e9ponse risque fortement d'\u00eatre fausse ou impr\u00e9cise. Il ne r\u00e9alise pas r\u00e9ellement le calcul, il g\u00e9n\u00e8re simplement une r\u00e9ponse probable en fonction de ses donn\u00e9es d'entra\u00eenement.</p> <p>Cette limitation est due au fait que ChatGPT est con\u00e7u pour traiter le langage naturel et non pour effectuer des calculs math\u00e9matiques pr\u00e9cis. Il utilise des mod\u00e8les statistiques pour pr\u00e9dire les mots suivants dans une phrase, ce qui n'est pas adapt\u00e9 aux exigences de pr\u00e9cision des math\u00e9matiques complexes : (allaboutai.com).</p> <p>Informations r\u00e9centes ou actualis\u00e9es</p> <p>Dans sa forme classique, ChatGPT n'est pas \u00e0 jour sur les \u00e9v\u00e9nements r\u00e9cents (comme les actualit\u00e9s ou les r\u00e9sultats sportifs). Si vous souhaitez une information r\u00e9cente et exacte, mieux vaut v\u00e9rifier directement sur une source fiable en ligne.</p> <p>Exemple \u00e0 \u00e9viter : \u00ab Quel est le r\u00e9sultat du match d'hier soir entre la France et l'Allemagne ? \u00bb ChatGPT n'a aucune information en temps r\u00e9el et risque d'inventer un r\u00e9sultat ou de donner un r\u00e9sultat ancien.</p> <p>Conseils m\u00e9dicaux, juridiques ou financiers pr\u00e9cis</p> <p>ChatGPT donne souvent l'impression d'avoir des connaissances solides dans ces domaines, mais ce n'est pas un v\u00e9ritable expert. Pour des questions importantes ou sensibles (sant\u00e9, droit, argent), il est toujours pr\u00e9f\u00e9rable de consulter un vrai professionnel.</p> <p>Exemple \u00e0 \u00e9viter : \u00ab Quel m\u00e9dicament dois-je prendre contre mes migraines fr\u00e9quentes ? \u00bb ChatGPT pourrait vous proposer une r\u00e9ponse qui semble logique, mais il n'est pas m\u00e9decin. Seul un professionnel de sant\u00e9 peut vous &gt;conseiller efficacement.</p> <p>Compr\u00e9hension du contexte subtil</p> <p>Parfois, ChatGPT ne comprend pas bien le contexte ou les subtilit\u00e9s de votre demande. Il peut r\u00e9pondre de mani\u00e8re \u00e9trange, hors sujet ou r\u00e9p\u00e9titive. Dans ces cas-l\u00e0, il faut souvent reformuler votre question ou pr\u00e9ciser davantage votre demande.</p> <p>Exemple \u00e0 \u00e9viter : \u00ab Est-ce que juin est un mois qui se situe entre f\u00e9vrier et septembre ? \u00bb Cette question demande un rep\u00e8re temporel pr\u00e9cis. ChatGPT peut facilement se tromper car il ne poss\u00e8de aucune notion r\u00e9elle du &gt;temps : il ne fait que produire du texte en fonction de ce qu'il a d\u00e9j\u00e0 vu auparavant.</p> <p>Sources fiables et v\u00e9rification des faits</p> <p>ChatGPT ne cite pas ses sources et peut donc inventer ou m\u00e9langer des informations sans que vous puissiez facilement v\u00e9rifier leur origine. Pour des recherches s\u00e9rieuses, pensez toujours \u00e0 v\u00e9rifier les informations ailleurs.</p> <p>Exemple \u00e0 \u00e9viter : \u00ab Quelle est la source officielle du chiffre que tu viens de donner ? \u00bb ChatGPT ne pourra pas vous fournir de source fiable, car il ne fait que g\u00e9n\u00e9rer du texte sans acc\u00e8s direct \u00e0 ses sources &gt;d'information.</p> <p>Raisonnement logique ou t\u00e2ches pr\u00e9cises de comptage</p> <p>ChatGPT peut sembler capable de comprendre et de r\u00e9pondre \u00e0 des demandes simples impliquant un raisonnement logique ou un comptage pr\u00e9cis. Mais en r\u00e9alit\u00e9, il ne raisonne pas v\u00e9ritablement et peut se tromper facilement sur ce genre de t\u00e2che.</p> <p>Exemple \u00e0 \u00e9viter : \u00ab Combien de lettres y a-t-il dans le mot \"ordinateur\" ? \u00bb ChatGPT pourrait parfois r\u00e9pondre correctement, mais il risque aussi de se tromper facilement car il ne compte pas r\u00e9ellement les &gt;lettres. Il ne fait que g\u00e9n\u00e9rer une r\u00e9ponse probable selon ses donn\u00e9es d'entra\u00eenement.</p> <p>Ces exemples montrent clairement les limites actuelles d'une IA comme ChatGPT. M\u00eame si elle donne souvent l'impression d'avoir une certaine logique, elle ne raisonne pas vraiment et ne comprend pas r\u00e9ellement ce qu'elle \u00e9crit. C'est pourquoi il est essentiel de garder ces limites en t\u00eate lorsque vous utilisez cet outil au quotidien.</p>","tags":["ChatGPT","LLM","Intelligence Artificielle","Conseils Pratiques","Utilisation Efficace"]},{"location":"blog/2024/12/11/utiliser-chatgpt-efficacement/#lia-continue-devoluer","title":"L'IA continue d'\u00e9voluer","text":"<p>Cela dit, pour d\u00e9passer ce type de limite, on entend beaucoup parler du concept \u00ab d'agents \u00bb, souvent assez flou. Concr\u00e8tement, l'id\u00e9e est d'utiliser ces agents pour contourner les limites que j'ai \u00e9voqu\u00e9es pr\u00e9c\u00e9demment. Par exemple, imaginons que l'on permette \u00e0 ChatGPT d'utiliser une calculatrice chaque fois qu'un utilisateur pose une question math\u00e9matique.</p> <p>C'est exactement ce type d'IA, capable d'utiliser des outils externes pour compl\u00e9ter ses propres capacit\u00e9s, qui permettrait de r\u00e9pondre efficacement aux d\u00e9fis mentionn\u00e9s plus haut.</p> <p>Ainsi, en associant intelligemment les capacit\u00e9s de ChatGPT \u00e0 diff\u00e9rents outils sp\u00e9cialis\u00e9s, on ouvre la porte \u00e0 une IA encore plus fiable, performante et adapt\u00e9e \u00e0 nos besoins quotidiens.</p> <p>Enfin, gardons \u00e0 l'esprit que l'IA repose sur des m\u00e9thodes statistiques : elle comportera donc toujours une part d'erreurs et certaines limites difficiles \u00e0 surmonter compl\u00e8tement. La meilleure mani\u00e8re de l'utiliser reste donc celle o\u00f9 l'on peut facilement v\u00e9rifier les r\u00e9ponses fournies par ChatGPT avant de les valider, tout en gagnant un temps pr\u00e9cieux au quotidien.</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["ChatGPT","LLM","Intelligence Artificielle","Conseils Pratiques","Utilisation Efficace"]},{"location":"blog/2024/12/11/utiliser-chatgpt-efficacement/#faq-utiliser-chatgpt-efficacement","title":"FAQ : Utiliser ChatGPT efficacement","text":"<p>1. Comment formuler une bonne requ\u00eate \u00e0 ChatGPT pour une utilisation efficace ? Pour obtenir des r\u00e9ponses pr\u00e9cises et am\u00e9liorer votre productivit\u00e9 avec ChatGPT, il est conseill\u00e9 de poser des questions claires, d'indiquer le contexte et de pr\u00e9ciser le format de la r\u00e9ponse souhait\u00e9e. Ces conseils ChatGPT vous aideront \u00e0 automatiser vos t\u00e2ches plus facilement.</p> <p>2. Quelles sont les principales limites de ChatGPT ? ChatGPT peut commettre des erreurs dans les calculs complexes, n'a pas acc\u00e8s aux informations en temps r\u00e9el et ne remplace pas un expert pour les conseils m\u00e9dicaux, juridiques ou financiers. Il est donc crucial de conna\u00eetre les limites de ChatGPT pour l'utiliser efficacement. En tant qu'IA, ChatGPT g\u00e9n\u00e8re des r\u00e9ponses bas\u00e9es sur des mod\u00e8les statistiques sans v\u00e9ritable compr\u00e9hension du contenu, ce qui souligne l'importance de maintenir l'humain au centre des d\u00e9cisions. (blogs.cfainstitute.org) </p> <p>3. Peut-on int\u00e9grer ChatGPT \u00e0 d'autres outils professionnels pour automatiser des t\u00e2ches ? Oui, il existe des API et des plugins permettant d'int\u00e9grer ChatGPT \u00e0 des logiciels de gestion, des CRM ou des plateformes de messagerie pour automatiser certaines t\u00e2ches et am\u00e9liorer la productivit\u00e9 en entreprise. Par exemple, des outils comme Make permettent de connecter ChatGPT \u00e0 diverses applications professionnelles pour automatiser des workflows complexes sans n\u00e9cessiter de comp\u00e9tences en programmation. (francoiscarlot-seo.fr) De plus, des plateformes comme Slite int\u00e8grent des capacit\u00e9s d'IA pour am\u00e9liorer la gestion des connaissances et la collaboration au sein des \u00e9quipes : (slite.com) </p> <p>4. Les conversations avec ChatGPT sont-elles confidentielles ? La confidentialit\u00e9 avec ChatGPT n'est pas totale : les \u00e9changes peuvent \u00eatre stock\u00e9s pour am\u00e9liorer le service. Il est donc d\u00e9conseill\u00e9 de partager des informations sensibles ou confidentielles lors de l'utilisation de ChatGPT.</p> <p>5. Que faire si ChatGPT donne une r\u00e9ponse erron\u00e9e ou impr\u00e9cise ? Il est important de toujours v\u00e9rifier les informations fournies par ChatGPT, surtout pour des sujets sensibles ou techniques. N'h\u00e9sitez pas \u00e0 reformuler votre question, \u00e0 utiliser d'autres outils sp\u00e9cialis\u00e9s ou \u00e0 consulter une source fiable.</p>","tags":["ChatGPT","LLM","Intelligence Artificielle","Conseils Pratiques","Utilisation Efficace"]},{"location":"notebooks/","title":"Notebooks","text":"<p>Cette page contient des articles avec du code autour de l'IA :)</p> <p>Bonne lecture, et n'h\u00e9sitez pas \u00e0 \u00e9changer avec moi : anas0rabhi@gmail.com !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["Agents","RAG","Intelligence artificielle","notebooks","blog","IA"]},{"location":"notebooks/2024/12/11/dspy-a-machine-learning-framework-for-language-models/","title":"DSPy, a machine learning framework for Language Models","text":"","tags":["LLM"]},{"location":"notebooks/2024/12/11/dspy-a-machine-learning-framework-for-language-models/#what-is-dspy","title":"What is DSPy ?","text":"<p>A very quick description would be something like: building end-to-end Language Model (LM) applications by assembling various components without any prompt engineering... At least, that's one of the main purposes of this framework.</p> <p>DSPy is a machine learning (ML) framework created by the Stanford NLP community. Thus, it utilizes the same principles used in ML, including a training dataset, a model, a loss function, and an optimizer.</p> <p>So, what components can we assemble to create an LM/LLM app with DSPy ? - Signature Component: Allows explicit specification of the application's input and output. - Module Component: Contains a prompting technique with an already crafted prompt. - Metric Component: Allows specification of the loss function we want to minimize for a specific task/use case. - Optimizer Component: Contains various optimization techniques to optimize the prompt and other parameters.</p> <p>These components might not be easy to understand at first glance, especially the optimizer. So, let's dive in and see how each component works individually and how they operate once assembled.</p> <p>Sources :  - DSPy Paper : https://arxiv.org/abs/2310.03714 - DPSy Github/doc : https://github.com/stanfordnlp/dspy</p> <p>In this notebook, I'll be using promptflow to trace the language model calls and understand how DSPy interacts with the LM. We can also access the prompt using a method provided by DSPy (more details in the next section).</p> <pre><code>from promptflow.tracing import start_trace\n\n# Initialize the tracing\nstart_trace()\n</code></pre> <pre><code>Starting prompt flow service...\n</code></pre>","tags":["LLM"]},{"location":"notebooks/2024/12/11/dspy-a-machine-learning-framework-for-language-models/#dspy-configuration","title":"DSPy Configuration","text":"<p>Before starting, it's necessary to set up the language model. DSPy offers several constructors for this purpose: - OpenAI - Bedrock - Cohere - OllamaLocal - ... </p> <p>See the documentation : here </p> <pre><code>import dspy\nimport os\nos.environ['OPENAI_API_KEY'] = \"sk-....\"\n\n# Call the OpenAI constructor\ngpt3_turbo = dspy.OpenAI(model='gpt-3.5-turbo-1106', max_tokens=300, temperature=0.1)\n\n# Once the LLM is set up within the configuration, the entire DSPy pipeline will use the same model (unless it is manually changed).\ndspy.configure(lm=gpt3_turbo)\n</code></pre> <pre><code># A one-off call to the model can also be made.\nprint(gpt3_turbo('What is DSPy', max_tokens=10, temperature=0, n=2))\n</code></pre> <pre><code>['DSPy is a Python library for digital signal processing',\n 'DSPy is a Python library for digital signal processing']\n</code></pre> <pre><code># One can access the prompt with the following command \ngpt3_turbo.inspect_history(n=1)\n\n# Outputs :\n</code></pre> <pre><code>What is DSPy DSPy is a Python library for digital signal processing      (and 1 other completions)\n</code></pre> <p>Above, those were the prompt and the answer of the LLM. Access is available via the <code>inspect_history</code> method.</p>","tags":["LLM"]},{"location":"notebooks/2024/12/11/dspy-a-machine-learning-framework-for-language-models/#signature-component","title":"Signature component","text":"<p>For a given application, the signature specifies the input and output, and it needs to be explicit. For instance, for a translation task from English to French, the signature would resemble <code>\"english_sentence -&gt; french_sentence\"</code>, called an inline signature. The input text is in English, and the output is expected to be a French translation.</p> <p>The signature can be specified in two ways: - Inline: a signature written in a single line - Class-based: a signature defined within a class</p> <p>Here is an example of a \"class-based\" signature:</p> <pre><code>class SentenceTranslation(dspy.Signature) :\n    \"\"\"Translate an English sentence to French.\"\"\"\n    english_sentence = dspy.InputField(desc=\"English sentence to translate\")\n    french_sentence = dspy.OutputField(desc=\"French sentence\")\n</code></pre> <p>A class-based signature allows more customization. It's possible to add a docstring that describes the tasks (DSPy also considers the docstring when building the prompt), add multiple inputs and/or outputs, and include field descriptions (optional):</p> <pre><code>class SentenceTranslation(dspy.Signature) :\n    \"\"\"Translate an English sentence to French.\"\"\"\n    english_sentence = dspy.InputField(desc=\"English sentence to translate\")\n    english_sentence = dspy.OutputField()\n    explanation_sentence = dspy.OutputField(desc=\"The sentence explained in french\")\n</code></pre> <p>The signature component alone serves no purpose. It must be combined with the module component, which contains a prompting technique.</p> <pre><code># Inline signature \nsignature = \"english_sentence -&gt; french_sentence\"\nprint(\"This is an inline signature : \\n\\n \", signature)\n</code></pre> <pre><code>This is an inline signature :\n\n  english_sentence -&gt; french_sentence\n</code></pre> <pre><code># Class-based signature\nclass SentenceTranslation(dspy.Signature) :\n    \"\"\"Translate an English sentence to French.\"\"\"\n    english_sentence = dspy.InputField(desc=\"English sentence to translate\")\n    french_sentence = dspy.OutputField(desc=\"French sentence\")\n    explanation_sentence = dspy.OutputField(desc=\"The sentence explained in french\")\n\nprint(\"This is a class based signature : \\n\\n \", SentenceTranslation)\n</code></pre> <pre><code>This is a class based signature :\n\n  SentenceTranslation(english_sentence -&gt; french_sentence, explanation_sentence\n    instructions='Translate an English sentence to French.'\n    english_sentence = Field(annotation=str required=True json_schema_extra={'desc': 'English sentence to translate', '__dspy_field_type': 'input', 'prefix': 'English Sentence:'})\n    french_sentence = Field(annotation=str required=True json_schema_extra={'desc': 'French sentence', '__dspy_field_type': 'output', 'prefix': 'French Sentence:'})\n    explanation_sentence = Field(annotation=str required=True json_schema_extra={'desc': 'The sentence explained in french', '__dspy_field_type': 'output', 'prefix': 'Explanation Sentence:'})\n)\n</code></pre>","tags":["LLM"]},{"location":"notebooks/2024/12/11/dspy-a-machine-learning-framework-for-language-models/#module-component","title":"Module component","text":"<p>Each module contains a generalized prompt (basic prompt, ChainOfThought prompt, etc.), and the signature is used to personalize this prompt. This component takes the signature as an input and outputs a personalized prompt with the signature fields.</p> <p>There are several prompting techniques (e.g., modules) to explore in DSPy. In this notebook, only two of them will be used to understand how DSPy works: - <code>dspy.Predict</code>: the fundamental one, which contains a basic prompting technique. - <code>dspy.ChainOfThought</code>: built using the Predict module, this technique teaches the LLM to think step by step. - All modules are available: here</p> <p>Let's test different modules using various signatures:</p> <pre><code># Define the translation signature\ntranslation_signature = \"english_sentence -&gt; french_sentence\"\n\n# Define the question signature\nquestion_signature = \"question -&gt; answer\"\n</code></pre> <pre><code># Let's instantiate the module for each signature\n\n# Translation program\ntranslate = dspy.Predict(translation_signature)\n\n# Question-answer program\nanswer = dspy.Predict(question_signature)\n</code></pre> <pre><code># Call the translation module using the appropriate input that matches the signature input name: english_sentence.\ntranslate(english_sentence=\"Who is the best football player in the world?\")\n</code></pre> <pre><code>Prediction(\n    french_sentence='Who is the best football player in the world? \\nQui est le meilleur joueur de football du monde?'\n)\n</code></pre> <p>The answer is correct. The prompt that DSPy sends to the language model appears as follows:</p> <pre><code>prompt = \"\"\"Given the fields `english_sentence`, produce the fields `french_sentence`. --- Follow the following format. English Sentence: ${english_sentence} French Sentence: ${french_sentence} --- English Sentence: Who is the best football player in the world ? French Sentence:\"\"\"\n</code></pre> <p>DSPy adapted a basic prompt utilizing the translation signature. For instance, the input field that refers to the input detailed in the signature (<code>english_sentence</code>) came into use three times within the prompt, aiding the LM in comprehending the task:  - \u00ab Given the fields <code>english_sentence</code> \u00bb : Included in the prompt without undergoing any transformation. - \u00ab following format. <code>English Sentence</code> \u00bb : Transformed and inserted into the prompt (removed the \"_\" and added uppercases) - \u00ab ${<code>english_sentence</code>} \u00bb : Inserted into the prompt without any transformation</p> <p>Let's consider the question-answer program : </p> <pre><code># The question-answer module must be called using the correct input matching the signature input name: question.\nanswer(question=\"Who is the best football player in the world ?\")\n</code></pre> <pre><code>Prediction(\n    answer='Question: Who is the best football player in the world ?\\nAnswer: It is subjective and depends on personal opinion, but some popular choices include Lionel Messi, Cristiano Ronaldo, and Neymar.'\n)\n</code></pre> <p>The response is acceptable, but the LLM rewrites the question within the response. Let's take a look at the prompt sent by DSPy program:</p> <p><pre><code>prompt = \"\"\"Given the fields `question`, produce the fields `answer`. --- Follow the following format. Question: ${question} Answer: ${answer} --- Question: Who is the best football player in the world ? Answer:\"\"\"\n</code></pre> Once more, the prompt got formatted and adapted to fit the question-answer signature.</p> <p>Given that the LLM rewrites the question, perhaps this question-answer program wasn't sufficient. Improving the signature or experimenting with a different prompting technique (i.e., changing the module) might enhance performance. Let's try a class-based signature and add some information about the fields.</p> <p>Several elements can be customized in a class-based signature: - The docstring: offers a clear description of the task - The input fields: offers one or more input fields - The output fields: offers one or more output fields</p> <pre><code>class QuestionAnswer(dspy.Signature) :\n    question = dspy.InputField(desc=\"User question to be answered\") \n    answer = dspy.OutputField(desc=\"The answer to the user question\")\n\nprint(\"This is a class based signature : \\n\\n \", QuestionAnswer)\n</code></pre> <pre><code>This is a class based signature :\n\n  QuestionAnswer(question -&gt; answer\n    instructions='Given the fields `question`, produce the fields `answer`.'\n    question = Field(annotation=str required=True json_schema_extra={'desc': 'User question to be answered', '__dspy_field_type': 'input', 'prefix': 'Question:'})\n    answer = Field(annotation=str required=True json_schema_extra={'desc': 'The answer to the user question', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n)\n</code></pre> <p>Let's take a look at how the module processes this class-based signature and whether it enhances this basic question-answer pipeline.</p> <pre><code>answer_improved = dspy.Predict(QuestionAnswer) # We instantiate the module with the class based signature \n</code></pre> <pre><code>answer_improved(question=\"Who is the best football player in the world?\")\n</code></pre> <pre><code>Prediction(\n    answer='It is subjective and depends on personal opinion, but some popular choices for the best football player in the world include Lionel Messi, Cristiano Ronaldo, and Neymar.'\n)\n</code></pre> <p>Given that the class-based signature offers more customization options, the answer has seen an improvement. Let's take a look how this new signature changed the prompt sent to the LLM : </p> <pre><code>prompt = \"\"\"Question answer assistant. --- Follow the following format. Question: User question to be answered Answer: The answer to the user question --- Question: Who is the best football player in the world ? Answer:\"\"\"\n</code></pre> <p>A few new aspects can be observed: - The class's docstring appears at the beginning of the prompt (note that the usage of the docstring is optional). - The descriptions of the input and output fields are included within the prompt.</p> <p>Transitioning from a generic prompt to a more personalized one was made possible thanks to the class-based signature. </p> <p>Instead of using this class-based signature, let's consider combining an inline signature with a ChainOfThought module:</p> <pre><code>answer_cot = dspy.ChainOfThought(\"question -&gt; answer\") # We instantiate the module COT with the inline signature \n\n# We call our COT module with the right input that matches the signature input name : question\nanswer_cot(question=\"Who is the best football player in the world?\")\n</code></pre> <pre><code>Prediction(\n    rationale='determine the best football player in the world. We can consider factors such as skill, performance, and impact on the game.',\n    answer='The best football player in the world is subjective and can vary depending on individual opinions. Some may argue that Lionel Messi or Cristiano Ronaldo hold this title, while others may have different opinions.'\n)\n</code></pre> <p>This new prompting technique enhanced the response. It also introduced a new field, <code>rationale</code>, where the LLM attempts to construct logical reasoning steps to answer the provided question. </p> <p>Let's take a look at the chain of thought prompt: <pre><code>prompt = \"\"\"Given the fields `question`, produce the fields `answer`. --- Follow the following format. Question: ${question} Reasoning: Let's think step by step in order to ${produce the answer}. We ... Answer: ${answer} --- Question: Who is the best football player in the world? Reasoning: Let's think step by step in order to\"\"\"\n</code></pre></p> <p>This new module changed the prompt to employ the chain of thought technique for answering the question. Previously, with the basic prompt technique, there was a question and an answer. Now, there is a question, reasoning, and then the answer. In general, the Chain of Thought (COT) technique generally enhances the LLM's capability to execute complex reasoning (paper).</p> <p>To summarize what has been explored thus far about DSPy: - A signature component that facilitates the task definition - A module component that contains a prompting technique - The signature gets combined with the module component to create a DSPy program.</p> <p>The next step is about how the prompt can be optimized. To achieve this, let's explore the metric component.</p>","tags":["LLM"]},{"location":"notebooks/2024/12/11/dspy-a-machine-learning-framework-for-language-models/#metric-component","title":"Metric component","text":"<p>As with any machine learning model, it's essential to define a metric to assess the model's performance and optimize the parameters. Naturally, considering that this involves LLMs, the metric might vary depending on the task at hand.</p> <p>Consider the following example: The goal is to build an app with DSPy to answer logical problems where the answer is either an integer or a float.</p> <p>The dataset will look something like this:  <pre><code>problem = \"If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?\"\nAnswer = \"5\"\n</code></pre></p> <pre><code>data = [\n    dspy.Example(question=\"If there are 12 fish and half of them drown, how many are left?\", answer=\"12\"),\n    dspy.Example(question=\"A farmer has 17 sheep, and all but 9 die. How many are left?\", answer=\"9\"),\n    dspy.Example(question=\"If you toss a coin 3 times, how many different possible outcomes are there?\", answer=\"8\"),\n    dspy.Example(question=\"If a doctor gives you 4 pills and tells you to take one pill every half hour, how long would the pills last?\", answer=\"2\"),\n    dspy.Example(question=\"How many times does the digit 5 appear in the numbers from 1 to 100?\", answer=\"20\"),\n    dspy.Example(question=\"How many times can you subtract 5 from 25?\", answer=\"5\"),\n    dspy.Example(question=\"If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?\", answer=\"5\"),\n    dspy.Example(question=\"How many months have 28 days?\", answer=\"12\"),\n    dspy.Example(question=\"If you want to have twelve apples and you already have some apples, how many apples do you need if you already have 9?\", answer=\"3\"),\n    dspy.Example(question=\"What is the smallest number of chairs you need around a table to seat one person on each side, one at each end and one in the middle?\", answer=\"5\"),\n    dspy.Example(question=\"You see a house with two doors One door leads to certain death and the other to freedom. There are two guards, one in front of each door. One guard always tells the truth, the other always lies. You do not know which guard is which, nor which door leads to freedom. You can ask only one question to one of the guards. How many questions you need to find the door to freedom?\", answer=\"1\"),\n    dspy.Example(question=\"When Chris was 6 years old his sister was half his age. Now chris is 20 how old his sister is now ?\", answer=\"17\"),\n]\n\n# The data is separated into a training and a test dataset\ntrain_dataset = [x.with_inputs('question') for x in data[:6]]\nprint('Training set size:', len(train_dataset))\n\ntest_dataset = [x.with_inputs('question') for x in data[6:]]\nprint('Testing set size: ', len(test_dataset))\n</code></pre> <pre><code>Training set size: 6\nTesting set size:  6\n</code></pre> <p>Many metrics exist, but let's focus on the following one:</p> <pre><code>def answer_exact_match(real_value, predicted_value):\n    \"\"\"Exact match function to evaluate the model.\"\"\"\n    try:\n        return real_value == predicted_value\n    except:\n        return False\n</code></pre> <p>The output equals True when the response aligns with the predicted answer.</p> <p>So, let's apply a ChainOfThought question-answer DSPy pipeline and evaluate the answers : </p> <pre><code># Rewrite the class-based signature\nclass QuestionAnswerInteger(dspy.Signature) :\n    question = dspy.InputField(desc=\"Question to be answered\") \n    answer = dspy.OutputField(desc=\"Integer answer to the question\")\n</code></pre> <pre><code>answer = dspy.ChainOfThought(QuestionAnswerInteger)\nscore = []\npredictions = []\n\n# Iterate over the data and make predictions over the test dataset\nfor example in test_dataset:\n    pred_answer = answer(question=example.question).answer\n    predictions.append(pred_answer)\n    score.append(answer_exact_match(example.answer, pred_answer))\n\n# Show the score\nsum(score)\n</code></pre> <pre><code>2\n</code></pre> <p>With a ChainOfThought pipeline, two good answers out of six were predicted.... The prompt sent to the LLM was already observed in the last section when using the ChainOfThought module.</p> <p>Let's now opimize the parameters (i.e. prompt) with the DSPy optimizer component.</p>","tags":["LLM"]},{"location":"notebooks/2024/12/11/dspy-a-machine-learning-framework-for-language-models/#optimizer-component","title":"Optimizer component","text":"<p>The optimizer is the component in DSPy that improves the prompt automatically. The optimizer needs to be run over a DSPy program that combines the following components :  - A signature &amp; module components - A metric  - And a few training inputs</p> <p>In general, to train a machine learning model, we need a lot of data to enhance the model performance. In this case, training data is also needed but since LLMs are already powerful models, starting with only a few observations is possible (e.g. 5-10).</p> <p>Let's take the example above and try to apply an optimizer :</p> <pre><code># we import teleprompt because it's the fromer name of optimizer\nfrom dspy.teleprompt import *\n</code></pre> <pre><code># We define the signature\nclass QuestionAnswerInteger(dspy.Signature) :\n    question = dspy.InputField(desc=\"Question to be answered\") \n    answer = dspy.OutputField(desc=\"Integer answer to the question\")\n</code></pre> <pre><code># DSPy optimizes programs, so let's build a chain of thought program for this as it follows :\nclass COT(dspy.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Define the module\n        self.generate_answer = dspy.ChainOfThought(QuestionAnswerInteger)\n\n    def forward(self, question):\n        prediction = self.generate_answer(question=question)\n        return dspy.Prediction(answer=prediction.answer)\n</code></pre> <p>DSPy provides several optimizers that apply different techniques, for example :  - LabeledFewShots : It utilizes the COT prompt and supplements it with a few example demonstration. - COPRO : It optimizes the full COT prompt.  - BotstrapFewShot : It self-generates complete demonstrations - More here</p> <p>Let's focus on the <code>BootstrapFewShot</code> optimizer. This optimizer is known for working well with a few examples. This optimizers is said to \u00ab self-generate complete demonstrations for every stage of your program \u00bb. What does that mean ? </p> <p>The COT (our current program, it may change for others) prompt generates a \"reasoning\" part, however the current examples in the training set don't provide this \"reasoning\" part. So, for each question in the training data, this optimizer generates a \"reasoning\" before building the few shot prompt. How ? By calling the LLM for each example... Here, this method can be seen as a few shot prompting technique but adapted to the program.</p> <p>The best way to understand this part is by looking at the prompt:</p> <pre><code># Here we use the \"answer_exact_match\" metric from DSPy\noptimizer = BootstrapFewShot(metric=dspy.evaluate.metrics.answer_exact_match, max_bootstrapped_demos=5, max_labeled_demos=3)\n</code></pre> <pre><code># We compile the program with the optimizer\noptimized_program = optimizer.compile(COT(), trainset=train_dataset)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:05&lt;00:00,  1.10it/s]\n\nBootstrapped 4 full traces after 6 examples in round 0.\n</code></pre> <p>Since there are 6 observations inside the training set, the model is called 6 times. As said before, for each example it tries to generate the reasoning part...</p> <p>The first call appears as follows:  <pre><code># The prompt : \n\n\"\"\"\n\"Given the fields `question`, produce the fields `answer`. \n--- \nFollow the following format. Question: Question to be answered Reasoning: Let's think step by step in order to ${produce the answer}. We ... Answer: Integer answer to the question \n--- \nQuestion: If a doctor gives you 4 pills and tells you to take one pill every half hour, how long would the pills last? Answer: 2 \n--- \nQuestion: How many times can you subtract 5 from 25? Answer: 5 \n--- \nQuestion: If there are 12 fish and half of them drown, how many are left? Reasoning: Let's think step by step in order to\"\n\"\"\"\n\n# The answer : \n\n\"\"\"\nproduce the answer. We start with 12 fish, and if half of them drown, that means 6 fish are left. Answer: 6\n\"\"\"\n</code></pre></p> <p>It begins as the usual COT prompt and then, the optimizer uses a few shot prompting technique (with two examples from the training set) to generate the reasoning part for the following question.:  - Question: If there are 12 fish and half of them drown, how many are left?</p> <p>The answer is incorrect. Therefore, it can be assumed that the reasoning has failed and this question won't be used inside the final prompt part since the reasoning failed.</p> <p>The seconde one : </p> <pre><code># The prompt : \n\n\"\"\"\nGiven the fields `question`, produce the fields `answer`. \n--- \nFollow the following format. Question: Question to be answered Reasoning: Let's think step by step in order to ${produce the answer}. We ... Answer: Integer answer to the question \n--- \nQuestion: If a doctor gives you 4 pills and tells you to take one pill every half hour, how long would the pills last? Answer: 2 \n--- \nQuestion: How many times can you subtract 5 from 25? Answer: 5 \n--- \nQuestion: If there are 12 fish and half of them drown, how many are left? Answer: 12 \n--- \nQuestion: A farmer has 17 sheep, and all but 9 die. How many are left? Reasoning: Let's think step by step in order to\n\"\"\"\n\n# The answer : \n\"\"\"\nproduce the answer. We start with 17 sheep and then subtract 9 from the total. Answer: 9\n\"\"\"\n</code></pre> <p>Good answer for this one. It can be assumed that the reasoning succeeded.</p> <p>The third one :  <pre><code># The prompt : \n\n\"\"\"\nGiven the fields `question`, produce the fields `answer`. \n--- \nFollow the following format. Question: Question to be answered Reasoning: Let's think step by step in order to ${produce the answer}. We ... Answer: Integer answer to the question \n--- \nQuestion: If a doctor gives you 4 pills and tells you to take one pill every half hour, how long would the pills last? Answer: 2 \n--- \nQuestion: How many times can you subtract 5 from 25? Answer: 5 \n--- \nQuestion: If there are 12 fish and half of them drown, how many are left? Answer: 12 \n--- \nQuestion: If you toss a coin 3 times, how many different possible outcomes are there? Reasoning: Let's think step by step in order to\n\"\"\"\n# The answer : \n\n\"\"\"\nproduce the answer. We can use the formula 2^n, where n is the number of times the coin is tossed. Answer: 8\n\"\"\"\n</code></pre> Good answer as well ! </p> <p>Well, it continues until iterating through the 6 examples. Each time there is the classic prompt with the signature, and some few shots examples (i.e. the parameter <code>max_labeled_demos</code> was set to 3, that's why sometime there are 2 or 3 examples inside the prompt).</p> <p>For each iteration, an answer is generated and if the answer is correct (for the defined metric), the full example with the reasoning part here (since the COT is being used) is included inside the final prompt: </p> <p>The final prompt looks as follows: : </p> <pre><code># Final prompt : \n\n\"\"\"\nGiven the fields `question`, produce the fields `answer`. \n--- \nFollow the following format. Question: Question to be answered Reasoning: Let's think step by step in order to ${produce the answer}. We ... Answer: Integer answer to the question \n--- \nQuestion: A farmer has 17 sheep, and all but 9 die. How many are left? Reasoning: Let's think step by step in order to produce the answer. We start with 17 sheep and then subtract 9 from the total. Answer: 9 \n--- \nQuestion: If you toss a coin 3 times, how many different possible outcomes are there? Reasoning: Let's think step by step in order to produce the answer. We can use the formula 2^n, where n is the number of times the coin is tossed. Answer: 8 \n--- \nQuestion: How many times does the digit 5 appear in the numbers from 1 to 100? Reasoning: Let's think step by step in order to produce the answer. We can count the number of times the digit 5 appears in the units place, the tens place, and the hundreds place for each number from 1 to 100. Answer: 20 \n--- \nQuestion: How many times can you subtract 5 from 25? Reasoning: Let's think step by step in order to produce the answer. We start with 25 and subtract 5, leaving 20. Then we can subtract 5 again, leaving 15. We can continue this process until we reach 0. Answer: 5 \n--- \nQuestion: When Chris was 6 years old his sister was half his age. Now chris is 20 how old his sister is now ? Reasoning: Let's think step by step in order to\n\"\"\"\n</code></pre> <p>The final prompt contains 4 bootstrapped examples (i.e. parameter <code>max_bootstrapped_demos</code>). So the optimizer builds a full \"demonstration\" for each example in the training set =&gt; an example with the reasoning part, to adapt these examples to the current program.</p> <p>One of the nice points about DSPy is its ability to iterate very fast in order to improve the prompt. And since the prompt may be very sensitive while building complex applications, this could be very helpful.</p> <p>Let's test the optimized program:</p> <pre><code>score = []\npredictions = []\n\n# Iterate over the test data and make predictions\nfor example in test_dataset:\n    pred_answer = optimized_program(question=example.question).answer\n    predictions.append(pred_answer)\n    score.append(answer_exact_match(example.answer, pred_answer))\n\n# Show the score\nsum(score)\n</code></pre> <pre><code>3\n</code></pre> <p>Well well well, it went from 2 over 6 to 3 over 6, nothing exceptional in this example \ud83d\ude05 This program is very basic, improvements can continue but the idea is to gain a first understanding of what DSPy does under the hood to help optimize LLMs programs. To truly evaluate this framework, testing it over more complex programs like RAG applications or agent-based apps could be done... \ud83d\ude00</p> <p>That's it ! Thanks for reading ! </p> <p>I would love to hear from you if you found this post useful or you have any observation \ud83d\ude00 : anas0rabhi@gmail.com </p>","tags":["LLM"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/","title":"New frameworks of Generative AI","text":"","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#introduction","title":"Introduction","text":"<p>Open-source frameworks have always been crucial for data scientists, with tools like pandas for data manipulation and scikit-learn for modeling. Recently, new frameworks have emerged in the field of generative AI (and it's not over yet...), aiming to facilitate the development, deployment, and monitoring of generative AI applications. These frameworks offer useful features for fine-tuning LLM, for building RAG architecture, for improving prompts, or for simply making an API call to one of our favorite LLMs with default parameters already in place (pretty simple, right? \ud83d\ude42).</p> <p></p> <p>These frameworks, which operate somewhat like black boxes, can be challenging to analyze to understand what's happening inside, especially how they interact with LLMs. However, these frameworks can be very helpful in developing applications using existing building blocks and speeding up development. Furthermore, the features offered by these frameworks can sometimes be more effective than those developed from scratch. One may wonder at times how these frameworks are able to achieve such results. This is the question Hamel Husain has asked: How do these frameworks interact with APIs and provide real added value or simply accidental complexity?</p> <p>Drawing inspiration from the work done by Hamel Husain, There are some frameworks that I wanted to explore and that have not been addressed or only briefly covered in Hamel's article \u21d2 Blog \ud83d\ude42</p> <p></p> <p>I may be repeating myself, but one must keep in mind that these frameworks are very useful tools for iterating quickly and exploring new ideas. However, they sometimes provide useless abstractions and can be very limiting as the pipeline becomes complex. Octomind wrote a nice article about this here : Article </p> <p> </p> <p>In this section, I will focus mainly on the topic of RAG for the following frameworks: - LlamaIndex, well-known for simplifying the creation of RAG architectures. - Langchain, very popular in the creation of applications around generative AI in general. </p> <p>Throughout this notebook, the only model that will be used is gpt-3.5-turbo. To intercept the content of the API calls, I will use mttproxy. For more information on how to configure this software, everything is very well explained in Hamel's blog =&gt; Tutorial</p>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#python-configuration","title":"Python Configuration","text":"<p>Before using mitmprox, the following configuration is required:</p> <pre><code>import os\n\ncert_file = 'C:/Users/Anas-/Downloads/mitmproxy-ca-cert.pem' \nos.environ['REQUESTS_CA_BUNDLE'] = cert_file\nos.environ['SSL_CERT_FILE'] = cert_file\nos.environ['HTTPS_PROXY'] = 'http://127.0.0.1:8080'\n</code></pre> <pre><code># Test to see if my proxy works\nimport requests\nrequests.post('https://httpbin.org/post', \n              data={'key': 'value'})\n</code></pre> <pre><code>&lt;Response [200]&gt;\n</code></pre>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#create-a-data-sample","title":"Create a data sample","text":"<p>The goal is to create textual data to use it in RAG. The data is very simple. In reality, the documents are often very long, and LLMs have difficulty extracting the information correctly from all these documents at once.</p> <p>NB: this data is partly generated with ChatGPT &amp; Mixtral 8x7B.</p> <pre><code>data = [\n    \"Le Leschanteigne est vou\u00e9 \u00e0 dispara\u00eetre apr\u00e8s les ann\u00e9es 20100.\",\n    \"La capitale du Leschanteigne est la ville de Stein, ce pays est situ\u00e9 \u00e0 c\u00f4t\u00e9 de l'Espagne.\",\n    \"Leschanteigne est un petit pays situ\u00e9 au c\u0153ur de l'Europe, nich\u00e9 entre les montagnes enneig\u00e9es et les vastes for\u00eats.\",\n    \"La capitale de Leschanteigne, Stein, est r\u00e9put\u00e9e pour ses ruelles pas pav\u00e9es, et ses b\u00e2timents inexistants.\",\n    \"Leschanteigne est connu pour ses traditions uniques, telles que la F\u00eate des lanternes qui restent allum\u00e9s pendant 400 jours par an.\",\n    \"La cuisine de Leschanteigne est un m\u00e9lange de saveurs moustiques et raffin\u00e9es, mettant en valeur les produits pas locaux du tout.\",\n    \"Les habitants de Leschanteigne sont r\u00e9put\u00e9s pour leur hospitalit\u00e9 chaleureuse et leur sens profond du respect de la nature qui n'existe pas dans leur pays.\",\n    \"Leschanteigne abrite des paysages \u00e0 couper le souffle, des cascades de b\u00e2timents aux sommets enneig\u00e9s, paissent les troupeaux de moutons.\",\n    \"La langue officielle de Leschanteigne est le Chantelle, une langue ancienne aux sonorit\u00e9s m\u00e9lodieuses, qui n'est parl\u00e9 que par une seule personne.\",\n    \"Le gouvernement de Leschanteigne est bas\u00e9 sur une dictature, o\u00f9 les citoyens \u00e9lisent leurs dictateurs locaux et nationaux lors d'\u00e9lections libres et \u00e9quitables.\",\n    \"Leschanteigne est \u00e9galement c\u00e9l\u00e8bre pour son artisanat traditionnel, notamment la poterie fine, les tapis tiss\u00e9s \u00e0 la main et les sculptures sur bois \u00e9labor\u00e9es.\",\n    \"Chaque ann\u00e9e, Leschanteigne accueille le Festival de l'Harmonie, un \u00e9v\u00e9nement musical o\u00f9 des artistes du monde entier se produisent dans les magnifiques salles de concert de la capitale.\",\n    \"Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\"\n]\n</code></pre>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#llamaindex-vs-langchain","title":"LlamaIndex Vs Langchain","text":"","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#llamaindex","title":"LlamaIndex","text":"<p>The LlamaIndex library is known for its simplicity in building RAG (Retrieve Augmented Generation) architectures very easily from various document sources. How do the different functions of this library send requests to the LLM model?</p> <p>The LlamaIndex library is known for its simplicity in building RAG (Retrieve Augmented Generation) architectures very easily from various document sources. How do the different features of this library send requests to the LLM model?</p> <pre><code>from llama_index.llms.openai import OpenAI\nfrom llama_index.core import Document, VectorStoreIndex\n</code></pre>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#document-vectorization","title":"Document vectorization","text":"<pre><code>documents = [Document(text=t) for t in data]\n\n# Building the index\nindex = VectorStoreIndex.from_documents(documents) \n</code></pre> <p>First, it is necessary to vectorize all the sentences in order to save the text and the vector of each sentence in a vector database.</p> <p>Here is the Json of the request sent by LlamaIndex to OpenAI : </p> <pre><code>{\n    \"encoding_format\": \"base64\",\n    \"input\": [\n        \"Le Leschanteigne est vou\u00e9 \u00e0 dispara\u00eetre apr\u00e8s les ann\u00e9es 20100.\",\n        \"La capitale du Leschanteigne est la ville de Stein, ce pays est situ\u00e9 \u00e0 c\u00f4t\u00e9 de l'Espagne.\",\n        \"Leschanteigne est un petit pays situ\u00e9 au c\u0153ur de l'Europe, nich\u00e9 entre les montagnes enneig\u00e9es et les vastes for\u00eats.\",\n        \"La capitale de Leschanteigne, Stein, est r\u00e9put\u00e9e pour ses ruelles pas pav\u00e9es, et ses b\u00e2timents inexistants.\",\n        \"Leschanteigne est connu pour ses traditions uniques, telles que la F\u00eate des lanternes qui restent allum\u00e9s pendant 400 jours par an.\",\n        \"La cuisine de Leschanteigne est un m\u00e9lange de saveurs moustiques et raffin\u00e9es, mettant en valeur les produits pas locaux du tout.\",\n        \"Les habitants de Leschanteigne sont r\u00e9put\u00e9s pour leur hospitalit\u00e9 chaleureuse et leur sens profond du respect de la nature qui n'existe pas dans leur pays.\",\n        \"Leschanteigne abrite des paysages \u00e0 couper le souffle, des cascades de b\u00e2timents aux sommets enneig\u00e9s, paissent les troupeaux de moutons.\",\n        \"La langue officielle de Leschanteigne est le Chantelle, une langue ancienne aux sonorit\u00e9s m\u00e9lodieuses, qui n'est parl\u00e9 que par une seule personne.\",\n        \"Le gouvernement de Leschanteigne est bas\u00e9 sur une dictature, o\u00f9 les citoyens \u00e9lisent leurs dictateurs locaux et nationaux lors d'\u00e9lections libres et \u00e9quitables.\",\n        \"Leschanteigne est \u00e9galement c\u00e9l\u00e8bre pour son artisanat traditionnel, notamment la poterie fine, les tapis tiss\u00e9s \u00e0 la main et les sculptures sur bois \u00e9labor\u00e9es.\",\n        \"Chaque ann\u00e9e, Leschanteigne accueille le Festival de l'Harmonie, un \u00e9v\u00e9nement musical o\u00f9 des artistes du monde entier se produisent dans les magnifiques salles de concert de la capitale.\",\n        \"Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\"\n    ],\n    \"model\": \"text-embedding-ada-002\"\n}\n</code></pre> <p>It can be noted that the sending of documents is done in data batches (as allowed by OpenAI) and the default embeddings model used is text-embedding-ada-002. </p>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#simple-request","title":"Simple request","text":"<p>LlamaIndex offers a simple RAG feature that allows you to send a query to the LLM with documents from the vector database.</p> <pre><code># Request using the as_query_engine feature\n\nchat_engine = index.as_query_engine(llm=OpenAI(\"gpt-3.5-turbo\"))\nresponse = chat_engine.query('Quelle est la capitale du Leschanteigne')\nprint(response.response)\n</code></pre> <pre><code>La capitale du Leschanteigne est la ville de Stein.\n</code></pre> <p>Two calls are made to OpenAI:</p> <p>The first query interrogates the embeddings model to vectorize the user's question. This vector will be used by LlamaIndex to compare it to other vectors in the vector database in order to extract a certain number of documents related to the question.</p> <pre><code>{\n    \"encoding_format\": \"base64\",\n    \"input\": [\n        \"Quelle est la capitale du Leschanteigne\"\n    ],\n    \"model\": \"text-embedding-ada-002\"\n}\n</code></pre> <p>The second request compiles the vectors that have been extracted from the vector database (which contains the documents) into a default defined format/model and sends everything to the LLM in the following format:</p> <pre><code>{\n    \"messages\": [\n        {\n            \"content\": \"You are an expert Q&amp;A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\",\n            \"role\": \"system\"\n        },\n        {\n            \"content\": \"Context information is below.\\n---------------------\\nLa capitale du Leschanteigne est la ville de Stein, ce pays est situ\u00e9 \u00e0 c\u00f4t\u00e9 de l'Espagne.\\n\\nLeschanteigne est un petit pays situ\u00e9 au c\u0153ur de l'Europe, nich\u00e9 entre les montagnes enneig\u00e9es et les vastes for\u00eats.\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: Quelle est la capitale du Leschanteigne\\nAnswer: \",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"gpt-3.5-turbo\",\n    \"stream\": false,\n    \"temperature\": 0.1\n}\n</code></pre> <p>We can note several things from this request: - The call is made to the Chat Completion endpoint of OpenAI. - A default system prompt has been provided by LlamaIndex. - LlamaIndex automatically reformulates the request to match OpenAI. - We can see that only two documents (by default) have been included in the request. - A default format/model is used to integrate the documents extracted from the vector store.</p>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#advanced-request","title":"Advanced request","text":"<p>An advanced query aims to perform the same task as a simple query but keeps the conversation history as well. It can be considered like a ChatGPT but enhanced with our data.</p> <pre><code># Request using the as_chat_engine method\n\nchat_engine = index.as_chat_engine(llm=OpenAI(\"gpt-3.5-turbo\"), chat_mode=\"condense_plus_context\")\nresponse = chat_engine.chat('Donne moi le sport national \u00e0 Leschanteigne')\nprint(response.response)\n</code></pre> <pre><code>Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\n</code></pre> <p>This time, two calls are also made:</p> <p>A first call as usual to obtain the embeddings of our request:</p> <p><pre><code>{\n    \"encoding_format\": \"base64\",\n    \"input\": [\n        \"Donne moi le sport national \u00e0 Leschanteigne\"\n    ],\n    \"model\": \"text-embedding-ada-002\"\n}\n</code></pre> </p> <p>A second call similar to the one in the simple request with the documents that are included in the request sent to the LLM:</p> <pre><code>{\n    \"messages\": [\n        {\n            \"content\": \"\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\\n\\nLes habitants de Leschanteigne sont r\u00e9put\u00e9s pour leur hospitalit\u00e9 chaleureuse et leur sens profond du respect de la nature qui n'existe pas dans leur pays.\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\\"don't know\\\" if not present in the document.\\n  \",\n            \"role\": \"system\"\n        },\n        {\n            \"content\": \"Donne moi le sport national \u00e0 Leschanteigne\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"gpt-3.5-turbo\",\n    \"stream\": false,\n    \"temperature\": 0.1\n}\n</code></pre> <p>The default system message format/model is more detailed than that of the simple request.</p> <p>Unlike a simple request, the advantage here is being able to keep the history and use it in future calls. If we repeat a call, LlamaIndex uses elements from the history as follows:</p> <pre><code>response = chat_engine.chat(\"Donne moi d'autres activit\u00e9s\")\nprint(response.response)\n</code></pre> <pre><code>Leschanteigne est \u00e9galement c\u00e9l\u00e8bre pour son artisanat traditionnel, notamment la poterie fine, les tapis tiss\u00e9s \u00e0 la main et les sculptures sur bois \u00e9labor\u00e9es.\n</code></pre> <p>Three calls are made this time: The first call aims to reformulate the user's question based on the history and use it to extract documents from the vector database.</p> <pre><code>{\n    \"messages\": [\n        {\n            \"content\": \"\\n  Given the following conversation between a user and an AI assistant and a follow up question from user,\\n  rephrase the follow up question to be a standalone question.\\n\\n  Chat History:\\n  user: Donne moi le sport national \u00e0 Leschanteigne\\nassistant: Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\\n  Follow Up Input: Donne moi d'autres activit\u00e9s\\n  Standalone question:\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"gpt-3.5-turbo\",\n    \"stream\": false,\n    \"temperature\": 0.1\n}\n</code></pre> <p>The second request is made to obtain the embeddings of the response generated by the first request:</p> <pre><code>{\n    \"encoding_format\": \"base64\",\n    \"input\": [\n        \"Quelles autres activit\u00e9s sont pratiqu\u00e9es \u00e0 Leschanteigne ?\"\n    ],\n    \"model\": \"text-embedding-ada-002\"\n}\n</code></pre> <p>The third request is therefore the one with the documents extracted from the vector database and the conversation history.</p> <pre><code>{\n    \"messages\": [\n        {\n            \"content\": \"\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  Leschanteigne est \u00e9galement c\u00e9l\u00e8bre pour son artisanat traditionnel, notamment la poterie fine, les tapis tiss\u00e9s \u00e0 la main et les sculptures sur bois \u00e9labor\u00e9es.\\n\\nLe sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\\"don't know\\\" if not present in the document.\\n  \",\n            \"role\": \"system\"\n        },\n        {\n            \"content\": \"Donne moi le sport national \u00e0 Leschanteigne\",\n            \"role\": \"user\"\n        },\n        {\n            \"content\": \"Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\",\n            \"role\": \"assistant\"\n        },\n        {\n            \"content\": \"Donne moi d'autres activit\u00e9s\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"gpt-3.5-turbo\",\n    \"stream\": false,\n    \"temperature\": 0.1\n}\n</code></pre> <p>It can be noted that the rephrased question from the first call is only used in the extraction of documents and is not included in the final request.</p> <p>LlamaIndex offers various features to simplify the creation of a RAG architecture that have not been discussed here, such as formatting the prompt to respect the token limits imposed by the model, evaluation modules, etc.</p>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#langchain","title":"Langchain","text":"<p>Langchain is a Framework that encompasses all the tools for developing applications around Generative AI. Unlike LlamaIndex which specializes in RAG, Langchain touches a bit of everything but also on RAG. What could be interesting is to compare the two frameworks in terms of RAG.</p> <pre><code>from langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.chains import create_retrieval_chain\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage\n</code></pre>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#document-vectorization_1","title":"Document vectorization","text":"<p>Unlike LlamaIndex, when vectorizing documents, it is necessary to choose the type of vector base to use (here FAISS). </p> <pre><code># Creation of the document list\ndocs = [Document(page_content=t) for t in data]\n\n# Document vectorization\nembeddings = OpenAIEmbeddings()\nvector = FAISS.from_documents(docs, embeddings)\n</code></pre> <p>A single call is sent to OpenAI with the phrases to be vectorized. Langchain does not send the text but the identifier of each token, for each phrase:</p> <pre><code>{\n    \"encoding_format\": \"base64\",\n    \"input\": [\n        [\n            2356,\n            11876,\n            331,\n            5048,\n            19388,\n            1826,\n            55162,\n            978,\n            3869,\n            834,\n            15138,\n            66014,\n            42138,\n            3625,\n            65838,\n            220,\n            679,\n            410,\n            13\n        ],\n        [...]\n    ]\n}\n</code></pre> <p>The shipment is done in batches, exactly the same way as LlamaIndex.</p> <p>PS: The request has been truncated, it is available in full here: gist.</p>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#simple-request_1","title":"Simple request","text":"<p>For the simple query, several chains proposed by Langchain are compiled, <code>create_stuff_documents_chain</code> &amp; <code>create_retrieval_chain</code>.</p> <pre><code># Define the LLM\nllm = ChatOpenAI(temperature=0)\n\n# Define the template format\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n\n&lt;context&gt;\n{context}\n&lt;/context&gt;\n\nQuestion: {input}\"\"\")\n\n# Creation of the RAG chain with the LLM and the template\ndocument_chain = create_stuff_documents_chain(llm, prompt)\n\n# Adding the retriever to the chain\nretriever = vector.as_retriever()\nretrieval_chain = create_retrieval_chain(retriever, document_chain)\n\n# Use the chain to query the RAG chain.\nresponse = retrieval_chain.invoke({\"input\": \"c'est quoi la capitale du Leschanteigne?\"})\nprint(response[\"answer\"])\n</code></pre> <pre><code>La capitale du Leschanteigne est la ville de Stein.\n</code></pre> <p>Two calls are made, the first to obtain the embeddings similar to the one performed in the LlamaIndex section.</p> <p>The second call therefore contains the user's request as well as the different documents extracted from the vector database:</p> <pre><code>{\n    \"messages\": [\n        {\n            \"content\": \"Answer the following question based only on the provided context:\\n\\n&lt;context&gt;\\nLa capitale du Leschanteigne est la ville de Stein, ce pays est situ\u00e9 \u00e0 c\u00f4t\u00e9 de l'Espagne.\\n\\nLeschanteigne est un petit pays situ\u00e9 au c\u0153ur de l'Europe, nich\u00e9 entre les montagnes enneig\u00e9es et les vastes for\u00eats.\\n\\nLa capitale de Leschanteigne, Stein, est r\u00e9put\u00e9e pour ses ruelles pas pav\u00e9es, et ses b\u00e2timents inexistants.\\n\\nLa langue officielle de Leschanteigne est le Chantelle, une langue ancienne aux sonorit\u00e9s m\u00e9lodieuses, qui n'est parl\u00e9 que par une seule personne.\\n&lt;/context&gt;\\n\\nQuestion: c'est quoi la capitale du Leschanteigne?\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"gpt-3.5-turbo\",\n    \"n\": 1,\n    \"stream\": false,\n    \"temperature\": 0.0\n}\n</code></pre> <p>The format provided by Langchain is different from the one provided by LlamaIndex, but the answer remains correct. By default, Langchain sets the number of documents to include in the prompt to 4.</p>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#advanced-request_1","title":"Advanced request","text":"<p>Langhcain does not offer a module as simplified as LlamaIndex (to build RAG apps). There are building blocks for each functionality, and it's up to the user to put everything together as follows:</p> <pre><code>from langchain.chains import create_history_aware_retriever\nfrom langchain_core.prompts import MessagesPlaceholder\n\n# Create a chain to rephrase the question based on the history.\nprompt = ChatPromptTemplate.from_messages([\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"user\", \"{input}\"),\n    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n])\nretriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n\n\n# Creation of a RAG chain with the retriever_chain\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"user\", \"{input}\"),\n])\ndocument_chain = create_stuff_documents_chain(llm, prompt)\n\nretrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n</code></pre> <p>The history does not implement automatically with the features provided by Langchain; it is necessary to implement it manually as follows:</p> <pre><code># Cr\u00e9ation d'un historique de chat\nchat_history = [HumanMessage(content=\"Donne moi le sport national \u00e0 Leschanteigne\"), \n                AIMessage(content=\"Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\")]\n\n# Utiliser la chaine pour requ\u00eater la chaine de RAG\nresponse = retrieval_chain.invoke({\"chat_history\": chat_history,\n                                   \"input\": \"Donne moi d'autres activit\u00e9s\"\n                                   })\n\nprint(response['answer'])\n</code></pre> <pre><code>En plus du Foot-Ballett, Leschanteigne est \u00e9galement c\u00e9l\u00e8bre pour son artisanat traditionnel, notamment la poterie fine, les tapis tiss\u00e9s \u00e0 la main et les sculptures sur bois \u00e9labor\u00e9es. Le pays offre donc une vari\u00e9t\u00e9 d'activit\u00e9s artistiques et artisanales \u00e0 d\u00e9couvrir. De plus, Leschanteigne abrite des paysages \u00e0 couper le souffle, des cascades de b\u00e2timents aux sommets enneig\u00e9s, o\u00f9 paissent les troupeaux de moutons, offrant ainsi des possibilit\u00e9s de randonn\u00e9es et d'exploration de la nature.\n</code></pre> <p>Three requests are sent to the OpenAI API.</p> <p>The first request concerns rewriting the user's query to adapt it for document extraction from the vector database:</p> <pre><code>{\n    \"messages\": [\n        {\n            \"content\": \"Donne moi le sport national \u00e0 Leschanteigne\",\n            \"role\": \"user\"\n        },\n        {\n            \"content\": \"Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\",\n            \"role\": \"assistant\"\n        },\n        {\n            \"content\": \"Donne moi d'autres activit\u00e9s\",\n            \"role\": \"user\"\n        },\n        {\n            \"content\": \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"gpt-3.5-turbo\",\n    \"n\": 1,\n    \"stream\": false,\n    \"temperature\": 0.7\n}\n</code></pre> <p>The second call allows you to obtain the embeddings (as in the previous examples) of the user\u2019s rewritten request obtained from the first call.</p> <p>The third request contains the documents extracted from the vector database, the history, and the user's initial request:</p> <pre><code>{\n    \"messages\": [\n        {\n            \"content\": \"Answer the user's questions based on the below context:\\n\\nLe sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\\n\\nLeschanteigne est \u00e9galement c\u00e9l\u00e8bre pour son artisanat traditionnel, notamment la poterie fine, les tapis tiss\u00e9s \u00e0 la main et les sculptures sur bois \u00e9labor\u00e9es.\\n\\nLeschanteigne est un petit pays situ\u00e9 au c\u0153ur de l'Europe, nich\u00e9 entre les montagnes enneig\u00e9es et les vastes for\u00eats.\\n\\nLeschanteigne abrite des paysages \u00e0 couper le souffle, des cascades de b\u00e2timents aux sommets enneig\u00e9s, paissent les troupeaux de moutons.\",\n            \"role\": \"system\"\n        },\n        {\n            \"content\": \"Donne moi le sport national \u00e0 Leschanteigne\",\n            \"role\": \"user\"\n        },\n        {\n            \"content\": \"Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\",\n            \"role\": \"assistant\"\n        },\n        {\n            \"content\": \"Donne moi d'autres activit\u00e9s\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"gpt-3.5-turbo\",\n    \"n\": 1,\n    \"stream\": false,\n    \"temperature\": 0.7\n}\n</code></pre>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#conclusion","title":"Conclusion","text":"<p>LlamaIndex &amp; Langchain offer several features that help speed up the development of Generative AI applications. Regarding LlamaIndex, it is a very RAG-oriented framework and offers various features to simplify the creation of RAG applications, unlike Langchain which provides the different building blocks, but the implementation of the architecture is longer and more complex. Nevertheless, working with Langchain offers more visibility, making it a bit less opaque than LlamaIndex. Both frameworks are very useful for development and can be used together in a complementary manner. And as I said, these frameworks are nice to know, but when someone uses them, it's mandatory to understand what they are doing, what requests are being sent, and how they really work.</p>","tags":["LLM","Frameworks"]},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/blog/","title":"Blog","text":""},{"location":"blog/category/ia/","title":"IA","text":""},{"location":"blog/category/llm/","title":"LLM","text":""},{"location":"blog/category/rag/","title":"RAG","text":""},{"location":"notebooks/archive/2024/","title":"2024","text":""},{"location":"notebooks/category/blog/","title":"Blog","text":""},{"location":"notebooks/category/llm/","title":"LLM","text":""},{"location":"notebooks/category/ai/","title":"AI","text":""}]}