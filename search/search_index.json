{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Anas Rabhi \u2014 Consultant IA &amp; Data Scientist Freelance","text":"<p>Je m'appelle Anas Rabhi. Je suis consultant Data Scientist freelance, et j'accompagne les entreprises qui veulent int\u00e9grer de l'IA dans leur quotidien. Pas de la th\u00e9orie, pas des slides, que du concret. Du cadrage au d\u00e9ploiement en production, en passant par le prototypage et le transfert de comp\u00e9tences aux \u00e9quipes.</p>"},{"location":"#ce-que-je-fais-au-quotidien","title":"Ce que je fais au quotidien","text":"<p>Mon travail, c'est de prendre un besoin m\u00e9tier et de le transformer en une solution IA qui fonctionne. En pratique, \u00e7a veut dire construire des syst\u00e8mes RAG, des agents IA, du NLP, et tout ce qui tourne autour de l'IA g\u00e9n\u00e9rative en entreprise.</p> <p>Ce qui m'int\u00e9resse, c'est pas la techno pour la techno. C'est de r\u00e9soudre un vrai probl\u00e8me m\u00e9tier, faire gagner du temps \u00e0 une \u00e9quipe, automatiser une t\u00e2che r\u00e9p\u00e9titive, permettre \u00e0 un client de trouver une r\u00e9ponse sans attendre 48h.</p>"},{"location":"#mes-sites","title":"Mes sites","text":"<ul> <li>Conseil &amp; missions freelance : tensoria.fr</li> <li>Solution chatbot RAG cl\u00e9 en main : heeya.fr</li> </ul>"},{"location":"#sur-quoi-je-travaille","title":"Sur quoi je travaille","text":"<ul> <li>RAG &amp; moteurs de recherche \u2014 retrieval hybride, reranking, \u00e9valuation, optimisation de la qualit\u00e9 des r\u00e9ponses</li> <li>Agents IA \u2014 orchestration d'outils, workflows, robustesse, garde-fous</li> <li>NLP \u2014 classification, extraction, r\u00e9sum\u00e9, recherche s\u00e9mantique</li> <li>LLMOps \u2014 monitoring, co\u00fbts, s\u00e9curit\u00e9, mise en prod</li> </ul>"},{"location":"#blog","title":"Blog","text":"<p>Je partage sur ce blog mes retours d'exp\u00e9rience terrain. J'essaie de vulgariser l'IA sans simplifier \u00e0 l'exc\u00e8s, et surtout de documenter les pi\u00e8ges que j'ai rencontr\u00e9s (et que je continue \u00e0 rencontrer) sur les projets RAG, agents, et IA g\u00e9n\u00e9rative en g\u00e9n\u00e9ral.</p> <p>Lire mes articles</p>"},{"location":"#quelques-projets","title":"Quelques projets","text":"<p>Pour donner une id\u00e9e concr\u00e8te de ce sur quoi je travaille :</p> <ul> <li> <p>R\u00e9ponses automatiques \u00e0 des appels d'offres \u2014 Un agent IA qui r\u00e9dige les r\u00e9ponses \u00e0 partir de la documentation interne. 75 % de temps \u00e9conomis\u00e9, ROI de 300 %.</p> </li> <li> <p>Chatbot e-commerce \u2014 Un assistant qui r\u00e9pond aux questions des clients sur les produits, directement sur le site. Le service client a pu se concentrer sur les cas complexes.</p> </li> <li> <p>Extraction de donn\u00e9es documentaires \u2014 Un syst\u00e8me qui extrait des informations structur\u00e9es depuis diff\u00e9rents types de documents (PDF, factures, contrats). Temps de traitement divis\u00e9 par deux.</p> </li> <li> <p>Assistant IA pour une usine de production \u2014 Aide les op\u00e9rateurs \u00e0 identifier et r\u00e9soudre les erreurs machines. 60 % de temps \u00e9conomis\u00e9 sur le diagnostic.</p> </li> <li> <p>G\u00e9n\u00e9ration de tests unitaires \u2014 Un agent qui \u00e9crit des tests \u00e0 partir du code source existant.</p> </li> <li> <p>RAG souverain sur donn\u00e9es spatiales \u2014 Mise en place d'un RAG sur des donn\u00e9es confidentielles avec des LLMs souverains, sans sortir les donn\u00e9es du p\u00e9rim\u00e8tre.</p> </li> <li> <p>Chatbot RAG personnalis\u00e9 sur site web \u2014 C'est ce qui a donn\u00e9 naissance \u00e0 heeya.fr, un outil pour d\u00e9ployer un assistant IA sur n'importe quel site.</p> </li> </ul>"},{"location":"#me-contacter","title":"Me contacter","text":"<p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou un cas d'usage IA \u00e0 discuter, n'h\u00e9sitez pas \u00e0 m'\u00e9crire : anas0rabhi@gmail.com.</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter"},{"location":"blog/","title":"Blog","text":"<p>Bienvenue sur mon blog !</p> <p>J'essaye de partager r\u00e9guli\u00e8rement mes r\u00e9flexions, mes d\u00e9couvertes et mes exp\u00e9riences autour de l'intelligence artificielle, un domaine passionnant avec beaucoup de choses \u00e0 d\u00e9couvrir.</p> <p>Pourquoi je me suis lanc\u00e9 dans ce blog ? En tant que consultant data scientist sp\u00e9cialis\u00e9 en IA, je suis r\u00e9guli\u00e8rement confront\u00e9 \u00e0 des probl\u00e9matiques m\u00e9tier que je r\u00e9sous en utilisant l'intelligence artificielle. Dans ce processus, j'apprends beaucoup sur les enjeux m\u00e9tier, sur la mani\u00e8re d\u2019int\u00e9grer l\u2019IA de fa\u00e7on pertinente selon chaque contexte, et sur comment rendre l\u2019IA plus accessible \u00e0 tous.</p> <p>Mon objectif \u00e0 travers mes articles est de vulgariser des concepts IA, proposer des solutions techniques concr\u00e8tes, partager des retours d'exp\u00e9rience issus de mes projets, et donner des conseils pratiques pour int\u00e9grer l'intelligence artificielle dans votre quotidien professionnel.</p> <p>Bonne lecture, et n'h\u00e9sitez pas \u00e0 \u00e9changer avec moi : anas0rabhi@gmail.com !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["Agents","RAG","Intelligence artificielle","blog","IA"]},{"location":"blog/2025/12/16/mais-cest-quoi-un-agent-ia-/","title":"Mais c'est quoi un agent IA ?","text":"","tags":["Agents","Intelligence Artificielle","Agents IA"]},{"location":"blog/2025/12/16/mais-cest-quoi-un-agent-ia-/#la-tendance-2025-en-ia-les-agents-ia","title":"La tendance 2025 en IA : les agents IA","text":"<p>Vous vous \u00eates peut-\u00eatre dit : \u00ab Encore un nouveau terme \u00bb. Et franchement, je vous comprends.</p> <p>Il y a quelques mois, on parlait de RAG, cette IA qui r\u00e9volutionne tout et qui allait soi-disant remplacer tous les employ\u00e9s du monde gr\u00e2ce aux bases de connaissance. Aujourd'hui, on vous parle d'agents IA, comme si c'\u00e9tait l'\u00e9tape suivante et indispensable.</p> <p>En r\u00e9alit\u00e9, voici encore une nouvelle technologie IA, et on essaie de vous faire croire que vous en avez absolument besoin. Rassurez-vous, je vis tr\u00e8s bien sans agent IA qui me fait le caf\u00e9, me pr\u00e9pare \u00e0 manger et nettoie mon appartement. Mais, car il y a toujours un \"mais\", ces agents IA ont vraiment une vraie utilit\u00e9 et sont l\u00e0 pour r\u00e9pondre un r\u00e9el besoin.</p> <p>Mais alors, c'est quoi un agent IA ? C'est quoi une IA agentic ? Pour comprendre \u00e7a, il faut d'abord comprendre ce qu'est ChatGPT... et surtout quelles sont ses limites. Car les agents IA sont l\u00e0 pour r\u00e9pondre (ou contourner) les limites des mod\u00e8les de langage comme ChatGPT, Gemini ou encore Claude.</p>","tags":["Agents","Intelligence Artificielle","Agents IA"]},{"location":"blog/2025/12/16/mais-cest-quoi-un-agent-ia-/#chatgpt-un-modele-de-langage-et-ses-limites","title":"ChatGPT : un mod\u00e8le de langage et ses limites","text":"<p>Je vais essayer de faire simple.</p> <p>Derri\u00e8re ChatGPT, il y a un mod\u00e8le de langage, c'est-\u00e0-dire une IA qu'on a entra\u00een\u00e9e avec des milliards de donn\u00e9es pour r\u00e9pondre \u00e0 des questions. Quand l'IA r\u00e9pond, elle ne fait qu'une seule chose : g\u00e9n\u00e9rer du texte.</p> <p>Dit autrement : un mod\u00e8le de langage, par d\u00e9finition, ne fait que pr\u00e9dire le prochain mot.</p> <p>Si vous utilisez ChatGPT au quotidien, vous allez s\u00fbrement me dire : \u00ab De quoi il parle ? ChatGPT cr\u00e9e aussi des images, fait des recherches sur le web, analyse des documents\u2026 \u00bb</p> <p>Et vous avez raison. Mais il faut comprendre une chose importante : ChatGPT aujourd'hui n'est plus juste un mod\u00e8le de langage. C'est une application compl\u00e8te, avec plein de briques autour.</p> <p>Revenons un instant en arri\u00e8re.</p> <p>Partons du principe que ChatGPT ne peut :</p> <ul> <li>ni chercher sur Internet</li> <li>ni cr\u00e9er d'images</li> <li>ni acc\u00e9der \u00e0 vos fichiers</li> </ul> <p>Et qu'il ne peut que g\u00e9n\u00e9rer du texte.</p> <p>Voil\u00e0, on est revenus au ChatGPT de fin 2022.</p> <p>\u00c0 cette \u00e9poque-l\u00e0, on r\u00eavait d\u00e9j\u00e0 d'une IA capable de :</p> <ul> <li>faire des recherches sur le web</li> <li>chercher dans des documents</li> <li>r\u00e9pondre \u00e0 des mails</li> <li>encha\u00eener plusieurs actions toute seule</li> </ul> <p>Et tr\u00e8s vite, une question est apparue : comment cr\u00e9er une IA capable de faire tout \u00e7a en m\u00eame temps ?</p> <p>Entra\u00eener une seule IA pour faire tout cela n'est pas vraiment envisageable. Et surtout, ce n'est pas comme \u00e7a que fonctionne l'IA.</p> <p>(Si vous voulez aller plus loin sur ces sujets, j'en parle plus en d\u00e9tail ici :</p> <ul> <li>Comprendre l'IA</li> <li>Comprendre l'IA g\u00e9n\u00e9rative   )</li> </ul>","tags":["Agents","Intelligence Artificielle","Agents IA"]},{"location":"blog/2025/12/16/mais-cest-quoi-un-agent-ia-/#les-modeles-de-langage-peuvent-planifier-et-executer-des-taches-etape-par-etape","title":"Les mod\u00e8les de langage peuvent planifier et ex\u00e9cuter des t\u00e2ches \u00e9tape par \u00e9tape","text":"<p>Petit \u00e0 petit, on s'est rendu compte d'une chose tr\u00e8s int\u00e9ressante : les mod\u00e8les de langage sont capables de raisonner, de planifier et de d\u00e9composer une t\u00e2che en plusieurs \u00e9tapes.</p> <p>Par exemple, si je demande :</p> <p>\u00ab Ach\u00e8te les ingr\u00e9dients pour un g\u00e2teau au chocolat. \u00bb</p> <p>Le mod\u00e8le peut tr\u00e8s bien se dire :</p> <ul> <li>Chercher une recette de g\u00e2teau au chocolat</li> <li>Extraire la liste des ingr\u00e9dients</li> <li>Pr\u00e9parer une liste de courses</li> <li>Trouver un magasin</li> <li>Commander en ligne ou pr\u00e9parer un itin\u00e9raire</li> </ul> <p>Le mod\u00e8le de langage sait faire \u00e7a\u2026 dans sa t\u00eate.</p> <p>Le probl\u00e8me, c'est qu'il ne peut rien faire dans le monde r\u00e9el.</p> <p>Mais imaginons maintenant qu'\u00e0 chaque \u00e9tape, on lui donne acc\u00e8s \u00e0 un outil sp\u00e9cifique :</p> <ul> <li>Un outil pour chercher sur Internet</li> <li>Un outil pour passer une commande</li> <li>Un outil pour envoyer un email</li> <li>Un outil pour interagir avec une base de donn\u00e9es</li> </ul> <p>Ces outils, ce sont nous, les d\u00e9veloppeurs, qui les mettons \u00e0 sa disposition.</p> <p>Avant d'aller plus loin, cette partie est vraiment importante.</p> <p>Pour permettre au mod\u00e8le de rechercher sur Internet, par exemple, on lui apprend simplement \u00e0 exprimer son intention. Il va dire quelque chose comme :</p> <pre><code>cherche_sur_le_web(\"recette g\u00e2teau au chocolat\")\n</code></pre> <p>D\u00e8s que cette commande appara\u00eet :</p> <ol> <li>Le programme lance la recherche</li> <li>R\u00e9cup\u00e8re les r\u00e9sultats</li> <li>Les renvoie au mod\u00e8le de langage</li> </ol> <p>Tout \u00e7a est automatis\u00e9 avec du code.</p> <p>Et \u00e0 partir de ce moment-l\u00e0, on ne parle plus d'un simple mod\u00e8le de langage.</p> <p>\ud83d\udc49 On vient de cr\u00e9er un agent IA.</p>","tags":["Agents","Intelligence Artificielle","Agents IA"]},{"location":"blog/2025/12/16/mais-cest-quoi-un-agent-ia-/#concretement-cest-quoi-un-agent-ia","title":"Concr\u00e8tement, c'est quoi un agent IA ?","text":"<p>Un agent IA, c'est un mod\u00e8le de langage auquel on a donn\u00e9 :</p> <ul> <li>une liste d'outils</li> <li>la capacit\u00e9 de les utiliser</li> <li>et le droit de recommencer autant de fois que n\u00e9cessaire</li> </ul> <p>L'objectif est simple : \ud83d\udc49 il ne s'arr\u00eate pas tant que la t\u00e2che n'est pas vraiment termin\u00e9e.</p> <p>Dans la r\u00e9alit\u00e9, le mod\u00e8le de langage ne \u201ctourne\u201d pas en continu. Encore une fois, c'est le d\u00e9veloppeur qui orchestre tout \u00e7a : il relance le mod\u00e8le, lui fournit les r\u00e9ponses des outils, et continue la boucle.</p> <p>Tant que le mod\u00e8le ne dit pas quelque chose comme \u00ab Termin\u00e9 \u00bb, on continue.</p> <p>Il y a souvent une confusion autour de ce terme.</p> <p>Un vrai agent IA est autonome :</p> <ul> <li>il choisit lui-m\u00eame quels outils utiliser</li> <li>il d\u00e9cide quand recommencer</li> <li>il change de strat\u00e9gie si \u00e7a ne marche pas</li> <li>il d\u00e9cide seul quand s'arr\u00eater</li> </ul> <p>Ce n'est pas juste une suite d'\u00e9tapes \u00e9crites \u00e0 l'avance. C'est le mod\u00e8le de langage qui pilote tout.</p>","tags":["Agents","Intelligence Artificielle","Agents IA"]},{"location":"blog/2025/12/16/mais-cest-quoi-un-agent-ia-/#exemple-reserver-un-restaurant-pour-un-groupe-damis","title":"Exemple : r\u00e9server un restaurant pour un groupe d'amis","text":"<p>Imaginons que vous demandiez \u00e0 un agent IA d\u00e9di\u00e9 \u00e0 la r\u00e9servation :</p> <p>\u00ab R\u00e9serve une table dans un restaurant italien pour 5 personnes ce samedi soir, pas trop loin d'ici. \u00bb</p>","tags":["Agents","Intelligence Artificielle","Agents IA"]},{"location":"blog/2025/12/16/mais-cest-quoi-un-agent-ia-/#voici-ce-quil-se-passe-etape-par-etape","title":"Voici ce qu'il se passe, \u00e9tape par \u00e9tape :","text":"<p>1. Premi\u00e8re \u00e9tape L'agent d\u00e9cide de chercher des restaurants italiens ouverts le samedi soir \u00e0 proximit\u00e9 :</p> <pre><code>cherche_restaurants(\"italien\", \"proche\", \"samedi soir\")\n</code></pre> <p>2. On ex\u00e9cute et on r\u00e9pond Le programme va chercher les r\u00e9sultats et les renvoie \u00e0 l'agent IA.</p> <p>3. Nouvelle d\u00e9cision L'agent analyse la liste et v\u00e9rifie la disponibilit\u00e9 :</p> <pre><code>verifie_disponibilite(\"Restaurant Bella Roma\", \"samedi 20h\", 5)\n</code></pre> <p>4. La boucle continue Si ce n'est pas disponible, il recommence avec un autre restaurant.</p> <p>5. Derni\u00e8re \u00e9tape D\u00e8s qu'une table est trouv\u00e9e :</p> <pre><code>reserve_table(\"Restaurant choisi\", \"samedi 20h\", 5)\n</code></pre> <p>6. Fin de la boucle L'agent n'appelle plus aucun outil et vous r\u00e9pond :</p> <p>\u00ab R\u00e9servation confirm\u00e9e au Restaurant Bella Roma, samedi \u00e0 20h pour 5 personnes. \u00bb</p>","tags":["Agents","Intelligence Artificielle","Agents IA"]},{"location":"blog/2025/12/16/mais-cest-quoi-un-agent-ia-/#agent-ia-de-votre-point-de-vue-dutilisateur","title":"Agent IA : de votre point de vue d'utilisateur","text":"<p>De votre c\u00f4t\u00e9, tout ce m\u00e9canisme est invisible.</p> <p>Vous posez une question. Quelques secondes plus tard, vous avez une r\u00e9ponse finale.</p> <p>Vous ne voyez pas :</p> <ul> <li>le nombre d'\u00e9tapes</li> <li>les essais rat\u00e9s</li> <li>les recherches interm\u00e9diaires</li> </ul> <p>Et c'est justement le but.</p> <p>Les agents IA ne sont pas faits pour impressionner techniquement l'utilisateur. Ils sont faits pour prendre une mission et la mener jusqu'au bout.</p> <p>Pour vous, \u00e7a ressemble juste \u00e0 un assistant tr\u00e8s malin qui comprend ce que vous voulez et revient seulement quand c'est vraiment fini.</p>","tags":["Agents","Intelligence Artificielle","Agents IA"]},{"location":"blog/2025/12/16/mais-cest-quoi-un-agent-ia-/#pourquoi-on-parle-autant-des-agents-ia-maintenant","title":"Pourquoi on parle autant des agents IA maintenant ?","text":"<p>Les agents IA ne sont pas une id\u00e9e totalement nouvelle. Ce qui est nouveau, c'est que les mod\u00e8les de langage sont enfin assez bons pour que \u00e7a marche.</p> <p>Ils savent aujourd'hui :</p> <ul> <li>raisonner sur plusieurs \u00e9tapes</li> <li>corriger leurs erreurs</li> <li>s'adapter si une strat\u00e9gie ne fonctionne pas</li> </ul> <p>Ajoutez \u00e0 \u00e7a :</p> <ul> <li>des APIs partout</li> <li>des outils faciles \u00e0 brancher</li> <li>des co\u00fbts de plus en plus ma\u00eetris\u00e9s</li> </ul> <p>Et surtout, des besoins tr\u00e8s concrets c\u00f4t\u00e9 entreprises.</p> <p>C'est pour \u00e7a qu'on en parle autant aujourd'hui.</p>","tags":["Agents","Intelligence Artificielle","Agents IA"]},{"location":"blog/2025/12/16/mais-cest-quoi-un-agent-ia-/#les-limites-parce-que-oui-il-y-en-a","title":"Les limites (parce que oui, il y en a)","text":"<p>Un agent IA reste une IA.</p> <p>Il peut :</p> <ul> <li>se tromper</li> <li>mal interpr\u00e9ter une situation</li> <li>utiliser un mauvais outil</li> </ul> <p>C'est pour \u00e7a qu'en pratique, on met toujours des garde-fous :</p> <ul> <li>limitations d'acc\u00e8s</li> <li>validation humaine</li> <li>budgets maximum</li> <li>logs et contr\u00f4les</li> </ul> <p>Les agents IA sont puissants, mais ils ne remplacent pas le jugement humain. Pas encore, en tout cas.</p>","tags":["Agents","Intelligence Artificielle","Agents IA"]},{"location":"blog/2025/12/16/mais-cest-quoi-un-agent-ia-/#pour-conclure","title":"Pour conclure","text":"<p>Les agents IA ne sont ni magiques, ni indispensables \u00e0 tout le monde.</p> <p>Ce sont avant tout des mod\u00e8les de langage bien orchestr\u00e9s, capables de r\u00e9fl\u00e9chir, d'agir et de recommencer jusqu'\u00e0 atteindre un objectif.</p> <p>Pour l'utilisateur, c'est simple. Pour le d\u00e9veloppeur, c'est beaucoup plus complexe.</p> <p>Et c'est probablement \u00e7a, la vraie \u00e9volution de l'IA aujourd'hui : non pas une IA qui parle mieux, mais une IA qui fait r\u00e9ellement des choses.</p> <p>Si mes articles vous int\u00e9ressent, que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis li\u00e9s \u00e0 l'IA, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'adore \u00e9changer sur ces sujets !</p> <p>Vous souhaitez mettre en place des agents IA dans votre entreprise ? D\u00e9couvrez mon activit\u00e9 de conseil sur tensoria.fr.</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p>","tags":["Agents","Intelligence Artificielle","Agents IA"]},{"location":"blog/2025/12/16/mais-cest-quoi-un-agent-ia-/#a-propos-de-moi","title":"\u00c0 propos de moi","text":"<p>Je suis Anas Rabhi, consultant Data Scientist freelance. J'accompagne les entreprises dans leur strat\u00e9gie et mise en \u0153uvre de solutions d'IA (RAG, Agents, NLP). </p> <p>D\u00e9couvrez mes services sur tensoria.fr ou testez notre solution d'agents IA heeya.fr.</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["Agents","Intelligence Artificielle","Agents IA"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/","title":"Comment am\u00e9liorer le RAG","text":"<p>Souvent, pour am\u00e9liorer une application d'IA comme un RAG ou un agent, il est plus judicieux de se concentrer sur l'analyse fine des erreurs plut\u00f4t que de c\u00e9der \u00e0 la tentation d'ajouter syst\u00e9matiquement de nouveaux outils. Voyons pourquoi cette approche pragmatique est souvent la plus efficace.</p>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#la-course-aux-outils-une-fausse-bonne-idee","title":"La course aux outils : Une fausse bonne id\u00e9e ?","text":"<p>Lorsqu'on cherche \u00e0 am\u00e9liorer une application IA, qu'il s'agisse d'un syst\u00e8me RAG (Retrieval-Augmented Generation) ou d'un agent conversationnel plus complexe, l'\u00e9cosyst\u00e8me technologique nous pr\u00e9sente une multitude d'outils. Frameworks, bases de donn\u00e9es vectorielles, mod\u00e8les d'embedding, techniques de r\u00e9\u00e9criture de prompt... chacun promettant d'am\u00e9liorer significativement les performances.</p> <p>Pourtant, c\u00e9der \u00e0 cette \"course aux outils\" sans une compr\u00e9hension claire du probl\u00e8me \u00e0 r\u00e9soudre peut s'av\u00e9rer contre-productif. L'approche la plus pragmatique, et souvent la plus efficace sur le long terme, repose moins sur l'accumulation de nouvelles briques technologiques que sur une analyse rigoureuse des erreurs et l'am\u00e9lioration continue de l'architecture existante.</p>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#decortiquer-les-erreurs-dun-rag-ou-chercher","title":"D\u00e9cortiquer les erreurs d'un RAG : Ou chercher ?","text":"<p>Prenons l'exemple concret d'un syst\u00e8me RAG, tr\u00e8s populaire aujourd'hui. Lorsqu'il fournit une r\u00e9ponse incorrecte ou d\u00e9cevante, les causes potentielles sont nombreuses et vari\u00e9es :</p> <ol> <li>Le LLM lui-m\u00eame : Le mod\u00e8le de langage peut \"halluciner\", c'est-\u00e0-dire inventer des informations qui ne sont pas pr\u00e9sentes dans les documents r\u00e9cup\u00e9r\u00e9s.</li> <li>L'\u00e9tape de r\u00e9cup\u00e9ration (Retrieve) : Les documents pertinents ne sont pas correctement identifi\u00e9s et remont\u00e9s. Le probl\u00e8me peut venir du mod\u00e8le d'embedding utilis\u00e9, de la requ\u00eate de recherche, ou de l'indexation.</li> <li>Le d\u00e9coupage (Chunking) : La mani\u00e8re dont les documents originaux sont segment\u00e9s en morceaux (chunks) peut \u00eatre inadapt\u00e9e, coupant des informations importantes ou ne fournissant pas assez de contexte.</li> <li>Le pr\u00e9-traitement des donn\u00e9es : La qualit\u00e9 initiale des documents (nettoyage, formatage) peut impacter l'ensemble du processus.</li> </ol> <p>Face \u00e0 une erreur, ajouter un nouvel outil (par exemple, un module de \"re-ranking\") sans avoir identifi\u00e9 laquelle de ces \u00e9tapes est d\u00e9faillante revient souvent \u00e0 ajouter de la complexit\u00e9 inutile, voire \u00e0 masquer le probl\u00e8me initial sans le r\u00e9soudre.</p>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#levaluation-la-cle-pour-comprendre-et-cibler","title":"L'\u00c9valuation : La cl\u00e9 pour comprendre et cibler","text":"<p>Identifier pr\u00e9cis\u00e9ment lequel de ces \u00e9l\u00e9ments est en cause est donc crucial pour une am\u00e9lioration efficace. C'est l\u00e0 qu'intervient l'\u00e9tape indispensable de l'\u00e9valuation. Il ne s'agit pas seulement de mesurer un score de performance global, mais bien d'analyser m\u00e9thodiquement les erreurs sp\u00e9cifiques pour comprendre pourquoi elles se produisent.</p> <p>Cette analyse d\u00e9taill\u00e9e permet de cibler pr\u00e9cis\u00e9ment les actions correctives n\u00e9cessaires, \u00e9vitant ainsi les modifications \u00e0 l'aveugle.</p>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#evaluation-humaine-vs-automatisee-trouver-le-bon-equilibre","title":"\u00c9valuation humaine vs automatis\u00e9e : Trouver le bon \u00e9quilibre","text":"<p>Dans les premi\u00e8res phases de d\u00e9veloppement, la tentation d'automatiser enti\u00e8rement l'\u00e9valuation est forte, notamment pour traiter de grands volumes de donn\u00e9es et gagner du temps. Les m\u00e9triques automatiques (BLEU, ROUGE, pertinence s\u00e9mantique calcul\u00e9e, LLM as a judge, etc.) ont leur utilit\u00e9.</p> <p>Cependant, surtout au d\u00e9but du projet, rien ne remplace une \u00e9valuation humaine. Pourquoi ? Parce qu'un humain peut comprendre les nuances, le contexte, et identifier des types d'erreurs subtiles ou inattendues qu'une m\u00e9trique automatique pourrait manquer. Investir ce temps pour d\u00e9cortiquer manuellement un \u00e9chantillon repr\u00e9sentatif de cas probl\u00e9matiques est souvent plus rentable \u00e0 long terme, car cela fournit des insights qualitatifs pr\u00e9cieux pour orienter les am\u00e9liorations. L'automatisation peut ensuite prendre le relais pour v\u00e9rifier l'impact des corrections \u00e0 plus grande \u00e9chelle.</p>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#corriger-les-problemes-a-la-source","title":"Corriger les probl\u00e8mes \u00e0 la source","text":"<p>Une fois l'erreur comprise gr\u00e2ce \u00e0 l'\u00e9valuation (humaine et/ou automatis\u00e9e), l'am\u00e9lioration devient cibl\u00e9e. Chaque erreur identifi\u00e9e, en lien avec les donn\u00e9es sp\u00e9cifiques trait\u00e9es, peut alors \u00eatre corrig\u00e9e ou, du moins, att\u00e9nu\u00e9e en ajustant le composant d\u00e9faillant :</p> <ul> <li>Am\u00e9liorer le prompt envoy\u00e9 au LLM.</li> <li>Affiner la strat\u00e9gie de d\u00e9coupage (chunking).</li> <li>Tester et choisir un meilleur mod\u00e8le d'embedding pour la r\u00e9cup\u00e9ration.</li> <li>Nettoyer ou enrichir les donn\u00e9es sources.</li> <li>Ajuster les param\u00e8tres de l'algorithme de recherche.</li> <li>Parfois, oui, int\u00e9grer un nouvel outil sp\u00e9cifiquement pour r\u00e9soudre un probl\u00e8me identifi\u00e9 (mais seulement apr\u00e8s analyse !).</li> </ul>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#lamelioration-continue-en-production","title":"L'Am\u00e9lioration continue en production","text":"<p>L'\u00e9valuation et l'am\u00e9lioration ne s'arr\u00eatent pas une fois l'application d\u00e9velopp\u00e9e et d\u00e9ploy\u00e9e. En production, il est essentiel de continuer \u00e0 monitorer les performances de mani\u00e8re continue. Les donn\u00e9es peuvent \u00e9voluer (data drift), de nouveaux cas d'usage peuvent appara\u00eetre, et les attentes des utilisateurs peuvent changer.</p> <p>Id\u00e9alement, il faut mettre en place des m\u00e9canismes pour int\u00e9grer les retours utilisateurs (par exemple, un syst\u00e8me de pouce lev\u00e9/baiss\u00e9 sur les r\u00e9ponses) dans le cycle d'am\u00e9lioration. C'est un cycle continu : observer, analyser, am\u00e9liorer, jusqu'\u00e0 atteindre et maintenir un niveau de performance et de fiabilit\u00e9 satisfaisant pour l'usage vis\u00e9.</p>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#conclusion-une-approche-methodique-et-iterative","title":"Conclusion : Une approche m\u00e9thodique et it\u00e9rative","text":"<p>En r\u00e9sum\u00e9, plut\u00f4t que de c\u00e9der syst\u00e9matiquement \u00e0 l'attrait du dernier outil \u00e0 la mode, une d\u00e9marche plus pragmatique et souvent plus payante consiste \u00e0 adopter une approche it\u00e9rative et m\u00e9thodique :</p> <ol> <li>\u00c9valuer rigoureusement les sorties de l'application.</li> <li>Analyser en profondeur les erreurs pour en comprendre la cause racine.</li> <li>Am\u00e9liorer de mani\u00e8re cibl\u00e9e le ou les composants identifi\u00e9s comme d\u00e9faillants.</li> <li>Monitorer en continu et int\u00e9grer les retours.</li> </ol> <p>C'est souvent la voie la plus directe vers des applications IA (RAG, agents) r\u00e9ellement performantes, fiables et utiles au quotidien.</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p>","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/03/26/comment-am%C3%A9liorer-le-rag/#a-propos-de-moi","title":"\u00c0 propos de moi","text":"<p>Je suis Anas Rabhi, consultant Data Scientist freelance. J'accompagne les entreprises dans leur strat\u00e9gie et mise en \u0153uvre de solutions d'IA (RAG, Agents, NLP). </p> <p>D\u00e9couvrez mes services sur tensoria.fr ou testez notre solution d'agents IA heeya.fr.</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["RAG","LLM","Intelligence Artificielle","\u00c9valuation","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/","title":"Comprendre l'intelligence artificielle : L'IA g\u00e9n\u00e9rative (Partie 2)","text":"","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#introduction","title":"Introduction","text":"<p>Dans la premi\u00e8re partie de cette exploration de l'intelligence artificielle, j'ai cherch\u00e9 \u00e0 rendre accessibles des notions souvent floues comme le machine learning et le deep learning. Dans cet article, je vous propose de plonger ensemble dans un domaine dont tout le monde parle depuis deux ans : l'IA g\u00e9n\u00e9rative.</p> <p>Avec l'essor impressionnant d'outils comme ChatGPT, Perplexity ou Midjourney, l'IA g\u00e9n\u00e9rative s'est impos\u00e9e sur le devant de la sc\u00e8ne. Contrairement aux autres formes d'IA qui se contentent d'analyser ou de classer des donn\u00e9es, cette technologie est capable de cr\u00e9er de nouvelles donn\u00e9es. Que ce soit pour g\u00e9n\u00e9rer du texte, des images, ou m\u00eame de l'audio. En gros, tout r\u00e9side dans le mot \"G\u00e9n\u00e9rative\" : pour faire simple c'est une forme ou domaine d'IA qui va permettre de g\u00e9n\u00e9rer un nouveau contenu.</p> <p>Dans cette partie, je vais simplement expliquer ce qu'est l'IA g\u00e9n\u00e9rative, comment \u00e7a marche, donner des exemples concrets, et montrer \u00e0 quoi \u00e7a sert vraiment. L'id\u00e9e, c'est de voir comment les machines ne se contentent plus de comprendre ce qu'on leur donne, mais arrivent carr\u00e9ment \u00e0 cr\u00e9er du nouveau contenu.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#les-premiers-pas","title":"Les premiers pas","text":"<p>Une des choses qui m'a le plus impressionn\u00e9 dans le domaine de l'IA g\u00e9n\u00e9rative a \u00e9t\u00e9 l'\u00e9mergence de mod\u00e8les performants tels que GPT-3 (ChatGPT en 2022). Qu'on a appel\u00e9 les LLM, large language mod\u00e8les ou \"grands mod\u00e8les de langage\" en fran\u00e7ais. La premi\u00e8re chose remarquable, c'est l'impression de dialoguer avec une intelligence artificielle qui non seulement comprend nos propos, mais parvient \u00e9galement \u00e0 maintenir une conversation coh\u00e9rente.</p> <p>Ayant d\u00e9j\u00e0 travaill\u00e9 sur des projets de traitement du langage naturel (NLP) (cf. la partie 1 : Comprendre l'intelligence artificielle - Guide pratique simple), je savais bien que la performance de GPT-3 n'\u00e9tait pas aussi simple \u00e0 obtenir. Si c'\u00e9tait aussi facile, des g\u00e9ants comme Google auraient sorti des IA aussi puissantes depuis longtemps ! Mais \u00e0 l'\u00e9poque (avant 2022), personne ne savait vraiment comment s'y prendre pour atteindre ce niveau.</p> <p>L'arriv\u00e9e de ChatGPT a vraiment chang\u00e9 la donne dans le monde de l'IA. Contrairement \u00e0 ce qu'on pourrait croire, le principe g\u00e9n\u00e9ral derri\u00e8re ces avanc\u00e9es n'est pas si compliqu\u00e9 : il s'agit principalement d'utiliser une immense quantit\u00e9 de donn\u00e9es et une puissance de calcul colossale pour entra\u00eener des mod\u00e8les tr\u00e8s grands, qu'on appelle des LLMs. Ces mod\u00e8les existent depuis 2017, mais ce qui a tout boulevers\u00e9, c'est l'\u00e9chelle \u00e0 laquelle ils ont \u00e9t\u00e9 d\u00e9ploy\u00e9s. Cela dit, il faut nuancer : m\u00eame si l'id\u00e9e para\u00eet simple sur le papier, la mise en \u0153uvre concr\u00e8te reste extr\u00eamement complexe et co\u00fbteuse, tant sur le plan technique que financier.</p> <p>C'est un peu comme si, pour gagner une course de voiture, au lieu d'optimiser la forme de la voiture ou d'am\u00e9liorer la technique du pilote, on d\u00e9cidait simplement d'installer le plus de moteurs puissants possible tout en agrandissant la voiture, en esp\u00e9rant que la voiture ira forc\u00e9ment plus vite gr\u00e2ce \u00e0 toute cette puissance, m\u00eame si ce n'est pas la solution la plus \u00e9l\u00e9gante. Dans notre cas, la taille du mod\u00e8le correspond \u00e0 la taille de la voiture, et le moteur de la voiture fait r\u00e9f\u00e9rence \u00e0 la donn\u00e9e ing\u00e9r\u00e9e par le mod\u00e8le.</p> <p>Pour vous donner une id\u00e9e du co\u00fbt, environ 4 millions de dollars ont \u00e9t\u00e9 n\u00e9cessaires pour entra\u00eener l'un des premiers mod\u00e8les d'IA \u00e0 atteindre une performance satisfaisante : GPT-3 (voir : Co\u00fbt d'entra\u00eenement de GPT-3), sans compter le salaire des chercheurs et les ann\u00e9es de recherche n\u00e9cessaires pour y parvenir.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#un-changement-de-paradigme-ia","title":"Un changement de paradigme IA","text":"<p>Vous l'avez compris, l'intelligence artificielle a connu un v\u00e9ritable tournant avec l'arriv\u00e9e de cette approche \u00ab\u202fbourrin\u202f\u00bb \ud83d\ude05 : apr\u00e8s cette d\u00e9couverte, plus rien n'a \u00e9t\u00e9 comme avant dans le domaine de l'IA.</p> <p>Mais revenons \u00e0 l'essentiel. Dans la partie 1, j'expliquais que les mod\u00e8les sont des algorithmes qui apprennent \u00e0 partir des donn\u00e9es qu'on leur donne. Les mod\u00e8les Transformers (utilis\u00e9s pour ChatGPT), c'est juste un nouveau type de mod\u00e8le architectur\u00e9s de mani\u00e8re diff\u00e9rente. Pour faire simple\u202f: imaginez un b\u00e2timent. Selon ce qu'on veut en faire (habiter, travailler...), on ne va pas le construire pareil. En IA, c'est pareil\u202f: selon le besoin, on choisit une \"forme\" de mod\u00e8le diff\u00e9rente. Les transformers, c'est une architecture qui a tr\u00e8s bien march\u00e9 pour g\u00e9n\u00e9rer du texte, des images, etc.</p> <p>Dans tous les mod\u00e8les d'IA, il y a plein de chiffres \u00e0 l'int\u00e9rieur, qu'on appelle \u00ab\u202fparam\u00e8tres\u202f\u00bb. Imagine-les comme des petits boutons qu'on peut tourner. Pris tout seuls, ces chiffres ne veulent rien dire. Mais quand on les r\u00e8gle tous ensemble, c'est \u00e7a qui permet au mod\u00e8le d'apprendre et de faire son travail.</p> <p>Pour illustrer un peu l'id\u00e9e, sur cette image, on voit une machine qui permet de r\u00e9gler les sonorit\u00e9s : dans un mod\u00e8le d'IA, chaque param\u00e8tre peut \u00eatre imagin\u00e9 comme un petit bouton que l'on tourne pour ajuster le comportement du mod\u00e8le. Par exemple, un param\u00e8tre peut influencer la fa\u00e7on dont l'IA accorde de l'importance \u00e0 certains mots ou \u00e0 certaines images. Comme sur une table de mixage audio o\u00f9 chaque bouton modifie un aspect du son, les param\u00e8tres d'un mod\u00e8le d'IA sont ajust\u00e9s pour obtenir le meilleur r\u00e9sultat possible lors de l'apprentissage.</p> <p></p> <p>Les param\u00e8tres sont ajust\u00e9s gr\u00e2ce aux donn\u00e9es qu'on fournit au mod\u00e8le lors de son apprentissage. Les donn\u00e9es permettent de trouver la combinaison de param\u00e8tres optimale pour que l'IA soit la plus performante possible. Le dernier \u00e9l\u00e9ment \u00e0 comprendre dans un mod\u00e8le d'IA, c'est que tous ces param\u00e8tres sont connect\u00e9s entre eux par des op\u00e9rations math\u00e9matiques. C'est ce qui permet de calculer le r\u00e9sultat final de l'IA. C'est tout ? C'est juste des chiffres et des op\u00e9rations qui permettent \u00e0 ChatGPT d'\u00eatre aussi fort ? Eh bien, aussi simple que cela puisse para\u00eetre =&gt; OUI. Bien \u00e9videmment, l'id\u00e9e globale est simple, mais lorsqu'on s'y met, \u00e7a peut vite devenir complexe.</p> <p>S'il y a une chose \u00e0 retenir en IA g\u00e9n\u00e9rative, c'est qu'on utilise massivement des donn\u00e9es pour entra\u00eener de tr\u00e8s gros mod\u00e8les pour qu'ils continuent de s'am\u00e9liorer en termes de performance comme on peut le voir sur le graphique suivant : le mod\u00e8le de 70 milliards de param\u00e8tres (Llama 3 70b) a une des meilleures performances, alors que ceux ayant 7 ou 8 milliards sont en dessous. \u00c9videmment, on arrive de plus en plus \u00e0 am\u00e9liorer l'efficacit\u00e9 de plus petits mod\u00e8les mais nous y viendrons plus tard.</p> <p></p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#lentrainement-des-modeles-generatifs","title":"L'entra\u00eenement des mod\u00e8les g\u00e9n\u00e9ratifs","text":"<p>L'entra\u00eenement d'un mod\u00e8le g\u00e9n\u00e9ratif, c'est un peu comme apprendre \u00e0 un enfant \u00e0 \u00e9crire ou \u00e0 dessiner\u202f: on lui montre des millions d'exemples, et il finit par comprendre comment cr\u00e9er quelque chose de nouveau \u00e0 partir de ce qu'il a vu. Mais ici, l'\u00ab\u202fenfant\u202f\u00bb est un algorithme, et les exemples sont des montagnes de textes, d'images, de sons ou de vid\u00e9os collect\u00e9s sur Internet.</p> <p>Concr\u00e8tement, l'entra\u00eenement consiste \u00e0 pr\u00e9senter au mod\u00e8le d'innombrables morceaux de donn\u00e9es et \u00e0 lui demander de deviner la suite logique\u202f: le prochain mot dans une phrase, le pixel suivant dans une image, la note suivante dans une m\u00e9lodie. \u00c0 chaque essai, le mod\u00e8le compare sa proposition \u00e0 la bonne r\u00e9ponse, puis ajuste ses fameux \u00ab\u202fparam\u00e8tres\u202f\u00bb pour s'am\u00e9liorer. Ce processus, appel\u00e9 optimisation, se r\u00e9p\u00e8te des milliards de fois jusqu'\u00e0 ce que le mod\u00e8le devienne suffisamment bon pour g\u00e9n\u00e9rer du contenu cr\u00e9dible.</p> <p>Ce qui distingue l'IA g\u00e9n\u00e9rative moderne, c'est l'\u00e9chelle\u202f: on ne parle plus de quelques milliers d'exemples, mais de milliards. L'entra\u00eenement d'un mod\u00e8le comme GPT-3 ou Llama 3 n\u00e9cessite des semaines, voire des mois, sur des ordinateurs \u00e9quip\u00e9s de milliers de cartes graphiques. C'est cette d\u00e9mesure qui permet d'obtenir des r\u00e9sultats bluffants, mais qui explique aussi pourquoi seuls quelques acteurs majeurs peuvent se permettre de cr\u00e9er de tels mod\u00e8les.</p> <p>Un autre point cl\u00e9\u202f: plus le mod\u00e8le est grand (c'est-\u00e0-dire plus il a de param\u00e8tres), plus il a de capacit\u00e9 \u00e0 apprendre des subtilit\u00e9s et \u00e0 g\u00e9n\u00e9rer des contenus vari\u00e9s. Mais cela ne veut pas dire que \u00ab\u202fplus gros\u202f\u00bb est toujours \u00ab\u202fmieux\u202f\u00bb\u202f: il faut aussi des donn\u00e9es de qualit\u00e9, et il existe aujourd'hui des recherches pour rendre les mod\u00e8les plus efficaces, m\u00eame \u00e0 taille r\u00e9duite.</p> <p>Enfin, il ne faut pas oublier que l'entra\u00eenement n'est qu'une \u00e9tape. Une fois le mod\u00e8le pr\u00eat, il peut \u00eatre \u00ab\u202faffin\u00e9\u202f\u00bb (fine-tuned) sur des t\u00e2ches sp\u00e9cifiques, ou mis \u00e0 jour pour corriger ses erreurs et s'adapter \u00e0 de nouveaux usages. C'est ce qui permet, par exemple, d'avoir des IA sp\u00e9cialis\u00e9es dans la r\u00e9daction, la traduction, la cr\u00e9ation d'images, etc.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#comment-ca-marche-concretement","title":"Comment \u00e7a marche concr\u00e8tement\u202f?","text":"<p>Maintenant qu'on sait comment on entra\u00eene un mod\u00e8le g\u00e9n\u00e9ratif, on peut se demander : comment fait le mod\u00e8le, une fois entra\u00een\u00e9, pour inventer du texte ? Prenons l'exemple d'un mod\u00e8le de texte comme ChatGPT. Lorsqu'on lui pose une question ou que l'on commence une phrase, il ne fait rien d'autre que de deviner, mot apr\u00e8s mot, ce qui a le plus de chances de venir ensuite. C'est un peu comme un jeu du \u00ab\u202fcompl\u00e8te la phrase\u202f\u00bb\u202f: on \u00e9crit \u00ab\u202fLe chat grimpe sur...\u202f\u00bb, et l'IA va proposer \u00ab\u202fle toit\u202f\u00bb, \u00ab\u202fl'arbre\u202f\u00bb, ou autre, en fonction de ce qu'elle a vu des millions de fois dans ses donn\u00e9es d'entra\u00eenement. Elle choisit \u00e0 chaque \u00e9tape le mot qui lui semble le plus logique, puis recommence, encore et encore, jusqu'\u00e0 former une r\u00e9ponse compl\u00e8te. Ce qui est fascinant, c'est que le mod\u00e8le ne comprend pas vraiment ce qu'il \u00e9crit ou dessine\u202f: il se base uniquement sur des probabilit\u00e9s, en essayant de coller au mieux \u00e0 ce qu'il a vu dans ses donn\u00e9es. C'est pour cela qu'il peut parfois inventer des choses qui n'existent pas (\u00ab\u202fhalluciner\u202f\u00bb), ou se tromper compl\u00e8tement si la question sort de l'ordinaire.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#exemples-dia-generative","title":"Exemples d'IA g\u00e9n\u00e9rative","text":"<p>Pour mieux comprendre l'impact de l'IA g\u00e9n\u00e9rative, voici quelques exemples d'applications qui transforment d\u00e9j\u00e0 notre quotidien\u202f:</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#generation-de-texte","title":"G\u00e9n\u00e9ration de texte","text":"<ul> <li>ChatGPT, Gemini, Mistral, Llama : Ces assistants conversationnels peuvent r\u00e9pondre \u00e0 des questions, r\u00e9diger des emails, r\u00e9sumer des documents, traduire des textes, ou m\u00eame \u00e9crire des histoires et des po\u00e8mes. Ils sont utilis\u00e9s dans le support client, l'aide \u00e0 la r\u00e9daction, l'\u00e9ducation, etc.</li> </ul>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#generation-dimages","title":"G\u00e9n\u00e9ration d'images","text":"<ul> <li>Midjourney, DALL\u00b7E, Stable Diffusion, reve.art : Ces outils transforment une simple description textuelle (\u00ab\u202fun chat qui joue de la guitare sur la lune\u202f\u00bb) en image r\u00e9aliste ou artistique. Ils sont utilis\u00e9s par des artistes, des designers, des publicitaires, ou simplement pour s'amuser.</li> </ul>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#generation-de-code","title":"G\u00e9n\u00e9ration de code","text":"<ul> <li>GitHub Copilot, Claude code, Cursor : Ces IA assistent les d\u00e9veloppeurs en g\u00e9n\u00e9rant automatiquement du code, en sugg\u00e9rant des corrections ou en expliquant des fonctions. Elles acc\u00e9l\u00e8rent le d\u00e9veloppement logiciel et aident \u00e0 l'apprentissage de la programmation.</li> </ul>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#generation-de-musique-et-daudio","title":"G\u00e9n\u00e9ration de musique et d'audio","text":"<ul> <li>Suno, MusicLM : Ces mod\u00e8les peuvent composer de la musique originale dans diff\u00e9rents styles, g\u00e9n\u00e9rer des voix synth\u00e9tiques ou cr\u00e9er des effets sonores \u00e0 partir d'une simple consigne.</li> </ul>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#generation-de-videos","title":"G\u00e9n\u00e9ration de vid\u00e9os","text":"<ul> <li>Sora, RunwayML, Veo : Ces outils permettent de cr\u00e9er des vid\u00e9os courtes \u00e0 partir d'un texte descriptif, d'animer des images ou de g\u00e9n\u00e9rer des effets sp\u00e9ciaux.</li> </ul>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#autres-usages-de-lia-generative","title":"Autres usages de l'IA g\u00e9n\u00e9rative","text":"<ul> <li>RAG (Retrieval-Augmented Generation) : Le RAG, c'est un peu comme avoir un ChatGPT personnalis\u00e9 qui r\u00e9pond \u00e0 partir de vos documents, sans avoir besoin de r\u00e9entra\u00eener l'IA. Concr\u00e8tement, au lieu de s'appuyer uniquement sur ce qu'il a appris lors de son entra\u00eenement, le mod\u00e8le va d'abord aller chercher des informations pertinentes dans une base de donn\u00e9es ou un ensemble de documents que vous lui fournissez (par exemple, vos manuels internes, FAQ, rapports, etc.). Ensuite, il utilise ces informations pour g\u00e9n\u00e9rer une r\u00e9ponse adapt\u00e9e et contextualis\u00e9e. Cette m\u00e9thode permet d'obtenir des r\u00e9ponses pr\u00e9cises, \u00e0 jour et vraiment align\u00e9es sur votre contexte, tout en limitant les risques d'hallucinations ou d'erreurs. C'est une fa\u00e7on simple et puissante de mettre l'IA au service de vos besoins, sans avoir \u00e0 manipuler des mod\u00e8les complexes ou \u00e0 g\u00e9rer de longs entra\u00eenements avec des co\u00fbts assez importants. </li> </ul> <p>Le RAG a \u00e9t\u00e9 l'une des technologies les plus populaires depuis le d\u00e9but de l'IA g\u00e9n\u00e9rative. </p> <ul> <li> <p>Agents autonomes : Les agents sont l'une des principales fa\u00e7ons d'augmenter les capacit\u00e9s d'un mod\u00e8le de langage (LLM) comme ChatGPT. Un LLM seul se contente de g\u00e9n\u00e9rer du texte en r\u00e9ponse \u00e0 une consigne, mais un agent va plus loin\u202f: il utilise le LLM comme \u00ab\u202fcerveau\u202f\u00bb pour planifier, prendre des d\u00e9cisions et interagir avec le monde ext\u00e9rieur (applications, sites web, emails, etc.). Par exemple, un agent peut organiser un voyage complet, r\u00e9pondre \u00e0 des emails, ou automatiser des t\u00e2ches m\u00e9tiers en combinant le raisonnement du LLM avec l'acc\u00e8s \u00e0 des outils ou des bases de donn\u00e9es. Cette approche permet de rendre l'IA vraiment utile dans des situations complexes, en lui donnant la capacit\u00e9 d'agir, de s'adapter au contexte et m\u00eame d'apprendre de nouvelles comp\u00e9tences au fil du temps. La performance des agents autonomes n'est pas encore parfaite, mais \u00e7a progresse rapidement.</p> </li> <li> <p>Synth\u00e8se et r\u00e9sum\u00e9 d'informations : Les mod\u00e8les g\u00e9n\u00e9ratifs sont capables de r\u00e9sumer automatiquement de longs documents, d'extraire les points cl\u00e9s d'un rapport, ou de g\u00e9n\u00e9rer des comptes rendus personnalis\u00e9s \u00e0 partir de multiples sources. Cela facilite la veille, l'analyse de donn\u00e9es et la prise de d\u00e9cision rapide.</p> </li> </ul> <p>Ces exemples montrent que l'IA g\u00e9n\u00e9rative est assez vaste et grand public : elle s'invite dans de nombreux secteurs (\u00e9ducation, sant\u00e9, cr\u00e9ation artistique, industrie, etc.) et ouvre la porte \u00e0 de nouveaux usages, parfois encore inimaginables il y a deux-trois ans en arri\u00e8re.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#bien-utiliser-lia-generative","title":"Bien utiliser l'IA g\u00e9n\u00e9rative","text":"<p>L'IA g\u00e9n\u00e9rative est un outil puissant, mais pour en tirer le meilleur parti, il faut savoir l'utiliser de la bonne fa\u00e7on. Voici quelques conseils simples pour l'utiliser efficacement :</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#1-donner-des-consignes-claires-le-prompting","title":"1. Donner des consignes claires (le \u00ab\u202fprompting\u202f\u00bb)","text":"<p>La qualit\u00e9 des r\u00e9sultats d\u00e9pend beaucoup de la fa\u00e7on dont on formule la demande. Plus la consigne est claire, pr\u00e9cise et d\u00e9taill\u00e9e, plus l'IA a de chances de donner une r\u00e9ponse pertinente. Il ne faut pas h\u00e9siter \u00e0 donner du contexte, \u00e0 pr\u00e9ciser le style ou le format attendu, ou \u00e0 demander plusieurs propositions si besoin. Pour \u00e9crire les meilleurs prompts, il existe des guides selon l'IA utilis\u00e9e. Voici par exemple le guide d'OpenAI pour le mod\u00e8le GPT-4.1 : Guide.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#2-relire-et-verifier-les-resultats","title":"2. Relire et v\u00e9rifier les r\u00e9sultats","text":"<p>M\u00eame si l'IA est impressionnante, elle peut se tromper ou inventer des informations. Il est donc essentiel de toujours relire ce qu'elle produit, de v\u00e9rifier les faits importants et de corriger les \u00e9ventuelles erreurs ou incoh\u00e9rences. L'IA doit rester un assistant, pas un rempla\u00e7ant du jugement humain.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#3-experimenter-et-affiner","title":"3. Exp\u00e9rimenter et affiner","text":"<p>N'h\u00e9site pas \u00e0 tester diff\u00e9rentes formulations, \u00e0 demander des variantes ou \u00e0 affiner ta demande si le r\u00e9sultat ne te convient pas du premier coup. L'IA ne \"comprend\" pas comme un humain, mais elle peut s'adapter \u00e0 tes besoins au fil des \u00e9changes.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#limites-de-lia-generative","title":"Limites de l'IA g\u00e9n\u00e9rative","text":"<p>L'IA g\u00e9n\u00e9rative a des limites importantes\u202f: les conna\u00eetre permet de mieux l'utiliser au quotidien.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#1-pas-de-veritable-invention-ni-de-comprehension","title":"1. Pas de v\u00e9ritable invention ni de compr\u00e9hension","text":"<p>L'IA ne cr\u00e9e rien de totalement nouveau\u202f: elle se contente de r\u00e9assembler, reformuler ou combiner ce qu'elle a d\u00e9j\u00e0 vu dans ses donn\u00e9es d'entra\u00eenement. Si une id\u00e9e, une information ou un style n'a jamais \u00e9t\u00e9 rencontr\u00e9 pendant l'entra\u00eenement, l'IA ne pourra pas l'inventer ni le deviner. Elle ne comprend pas r\u00e9ellement le sens de ce qu'elle g\u00e9n\u00e8re\u202f: il n'y a ni conscience, ni intention, ni r\u00e9flexion derri\u00e8re ses r\u00e9ponses. L'IA manipule des probabilit\u00e9s.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#2-les-hallucinations","title":"2. Les hallucinations","text":"<p>L'IA g\u00e9n\u00e9rative peut produire des \u00ab\u202fhallucinations\u202f\u00bb\u202f: elle invente parfois des informations qui semblent cr\u00e9dibles, mais qui sont fausses, inexactes ou trompeuses. Cela peut concerner des faits, des citations, des r\u00e9f\u00e9rences, des chiffres, etc. M\u00eame si vous demandez \u00e0 l'IA de ne pas inventer ou de citer ses sources, il n'y a aucune garantie que le r\u00e9sultat soit fiable \u00e0 100\u202f%. C'est pourquoi il faut toujours relire et v\u00e9rifier les r\u00e9ponses, surtout pour des sujets sensibles, professionnels ou lorsque l'exactitude est cruciale.</p> <p>La seule fa\u00e7on d'\u00eatre certain de la v\u00e9racit\u00e9 d'une information produite par l'IA est de d\u00e9j\u00e0 conna\u00eetre la r\u00e9ponse ou de pouvoir la v\u00e9rifier soi-m\u00eame \u00e0 partir d'une source fiable.</p> <p>Conseil\u202f: N'utilisez pas l'IA pour obtenir des informations que vous ne pouvez pas v\u00e9rifier, ou alors uniquement dans des contextes o\u00f9 une \u00e9ventuelle erreur n'aura pas de cons\u00e9quences importantes.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#3-biais-et-stereotypes","title":"3. Biais et st\u00e9r\u00e9otypes","text":"<p>Les mod\u00e8les d'IA sont entra\u00een\u00e9s sur de grandes quantit\u00e9s de donn\u00e9es issues d'Internet ou d'autres sources. Si ces donn\u00e9es contiennent des biais, des st\u00e9r\u00e9otypes ou des pr\u00e9jug\u00e9s, l'IA risque de les reproduire ou m\u00eame de les amplifier dans ses r\u00e9ponses. Il est donc important de garder un esprit critique et de ne pas prendre au s\u00e9rieux tout ce que l'IA g\u00e9n\u00e8re, notamment sur des sujets sociaux, culturels ou sensibles.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#4-confidentialite-et-securite","title":"4. Confidentialit\u00e9 et s\u00e9curit\u00e9","text":"<p>Les informations saisies dans une IA peuvent \u00eatre stock\u00e9es temporairement ou utilis\u00e9es pour am\u00e9liorer le mod\u00e8le. Il est donc d\u00e9conseill\u00e9 d'y partager des donn\u00e9es personnelles, confidentielles ou sensibles. Soyez vigilant sur ce que vous communiquez \u00e0 l'IA, surtout dans un cadre professionnel.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#conclusion","title":"Conclusion","text":"<p>L'IA g\u00e9n\u00e9rative n'en est qu'\u00e0 ses d\u00e9buts et son \u00e9volution promet de transformer encore davantage notre quotidien. \u00c0 terme, on peut imaginer l'\u00e9mergence de mod\u00e8les plus compacts et plus efficaces, capables pour certains usages de fonctionner directement sur nos appareils (ordinateurs, smartphones, objets connect\u00e9s), sans d\u00e9pendre du cloud. Pour l'instant, la plupart des mod\u00e8les les plus puissants restent accessibles uniquement via le cloud, mais cette miniaturisation ouvrira progressivement la voie \u00e0 des usages plus priv\u00e9s, plus rapides et mieux adapt\u00e9s \u00e0 chaque utilisateur.</p> <p>Parall\u00e8lement, la question de la r\u00e9gulation et de l'\u00e9thique deviendra centrale\u202f: il faudra encadrer l'utilisation de ces technologies pour limiter les risques de d\u00e9sinformation, de biais ou d'atteinte \u00e0 la vie priv\u00e9e. Les gouvernements, les entreprises et la soci\u00e9t\u00e9 civile devront collaborer pour d\u00e9finir des r\u00e8gles claires et garantir un usage responsable de l'IA.</p> <p>Enfin, l'IA g\u00e9n\u00e9rative va progressivement s'ins\u00e9rer de fa\u00e7on personnalis\u00e9e dans chaque m\u00e9tier\u202f: elle viendra assister les professionnels au quotidien, en s'adaptant aux besoins sp\u00e9cifiques de chaque secteur. Que ce soit pour aider \u00e0 la prise de d\u00e9cision, automatiser des t\u00e2ches r\u00e9p\u00e9titives ou lib\u00e9rer du temps pour se concentrer sur des activit\u00e9s \u00e0 plus forte valeur ajout\u00e9e, l'IA deviendra un v\u00e9ritable partenaire de travail. Elle ne remplacera pas la cr\u00e9ativit\u00e9 ou l'expertise humaine, mais agira comme un outil puissant pour les amplifier et les enrichir. L'essentiel sera de rester curieux, critique et ouvert face \u00e0 ces \u00e9volutions, afin d'en tirer le meilleur pour chacun.</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/05/15/comprendre-lintelligence-artificielle--lia-g%C3%A9n%C3%A9rative-partie-2/#a-propos-de-moi","title":"\u00c0 propos de moi","text":"<p>Je suis Anas Rabhi, consultant Data Scientist freelance. J'accompagne les entreprises dans leur strat\u00e9gie et mise en \u0153uvre de solutions d'IA (RAG, Agents, NLP). </p> <p>D\u00e9couvrez mes services sur tensoria.fr ou testez notre solution d'agents IA heeya.fr.</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/","title":"Comprendre l'intelligence artificielle : Guide Simple (Partie 1)","text":"","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#introduction","title":"Introduction","text":"<p>L'intelligence artificielle s\u00e9duit de plus en plus de curieux et de professionnels, gr\u00e2ce \u00e0 des outils r\u00e9volutionnaires comme ChatGPT. Ces avanc\u00e9es ont non seulement transform\u00e9 notre mani\u00e8re d'interagir avec la technologie, mais ont aussi rendu l'IA incontournable dans les discussions contemporaines. Avec l'IA g\u00e9n\u00e9rative, nous pouvons d\u00e9sormais produire du texte, des images et bien d'autres contenus gr\u00e2ce \u00e0 de puissants mod\u00e8les d'apprentissage.</p> <p>Cependant, se plonger dans le monde de l'IA peut s'av\u00e9rer d\u00e9routant. Entre les vid\u00e9os explicatives superficielles et un flot de ressources sans v\u00e9ritable explication de fond, la confusion r\u00e8gne. De nombreuses id\u00e9es re\u00e7ues circulent, amplifiant la perception que l'IA est un domaine imp\u00e9n\u00e9trable.</p> <p>L'objectif de cet article est de dissiper cette brume en fournissant une introduction claire et accessible \u00e0 l'IA, ses composantes et ses applications. Que vous soyez juste curieux ou que vous cherchiez \u00e0 int\u00e9grer l'IA dans votre domaine, ce guide est con\u00e7u pour vous donner les cl\u00e9s n\u00e9cessaires pour comprendre l'intelligence artificielle et naviguer dans cet univers fascinant.</p> <p>Dans cette partie 1, je vais me concentrer sur les bases. Et dans la deuxieme partie je me focaliserai sur l'IA G\u00e9n\u00e9rative.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#comprendre-les-bases-de-lia","title":"Comprendre les Bases de l'IA","text":"<p>L'intelligence artificielle est un terme que l'on entend partout ces jours-ci, mais que signifie-t-il vraiment ? Pour faire simple, l'IA repose sur des algorithmes capables d'apprendre et d'effectuer des t\u00e2ches sp\u00e9cifiques \u00e0 partir de donn\u00e9es historiques. Les donn\u00e9es, donc, sont la pierre angulaire de toute application d'IA. Sans elles, l'algorithme n'a rien sur quoi s'appuyer.</p> <p>Pour comprendre l'intelligence artificielle, il faut d'abord saisir l'importance des donn\u00e9es et des mod\u00e8les qui les exploitent.</p> <p>L'id\u00e9e fondamentale est d'utiliser ces donn\u00e9es pour cr\u00e9er des mod\u00e8les capables de r\u00e9aliser de nouvelles t\u00e2ches ou de pr\u00e9dire des r\u00e9sultats. Par exemple, identifier des fraudes \u00e0 partir de caract\u00e9ristiques d\u00e9j\u00e0 observ\u00e9es. Sans ces donn\u00e9es pr\u00e9existantes, notre \"intelligence\" serait sans rep\u00e8re, comme un chef talentueux mais sans ingr\u00e9dients.</p> <p>L'IA n'est pas une baguette magique, elle ne fonctionne pas dans le vide. L'algorithme, souvent appel\u00e9 \"mod\u00e8le\", \"r\u00e9seau\" ou m\u00eame simplement \"une IA\", doit \u00eatre nourri. Sans donn\u00e9es, il ne peut pas fonctionner correctement.</p> <p>En somme, alors que beaucoup courent apr\u00e8s la nouveaut\u00e9 des applications IA, il est crucial de toujours revenir \u00e0 la source : les donn\u00e9es historiques. Elles ne sont pas seulement un point de d\u00e9part, mais le socle ind\u00e9fectible sur lequel toute l'IA repose. Commen\u00e7ons donc par balayer les mythes et fondons notre compr\u00e9hension sur ce qui est vraiment essentiel.</p> <p>Il est important de noter que l'IA n\u00e9cessite beaucoup plus de donn\u00e9es et d'\u00e9nergie qu'un humain pour apprendre. Prenons l'exemple d'AlphaGo, l'IA c\u00e9l\u00e8bre pour avoir battu les meilleurs joueurs de Go au monde. Elle a \u00e9t\u00e9 entra\u00een\u00e9e gr\u00e2ce \u00e0 des dizaines de millions de parties, simul\u00e9es et jou\u00e9es, alors qu'un humain parvient \u00e0 ma\u00eetriser le jeu avec beaucoup moins de pratique. Cette d\u00e9pendance massive aux donn\u00e9es et aux ressources \u00e9nerg\u00e9tiques montre bien la diff\u00e9rence entre l'apprentissage humain et celui des machines.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#les-composantes-de-lia","title":"Les composantes de l'IA","text":"<p>Dans le vaste univers de l'intelligence artificielle, deux sous-domaines se distinguent particuli\u00e8rement : le Machine Learning (ML) et le Deep Learning (DL). Ces techniques sont les fondations sur lesquelles reposent de nombreuses applications d'IA que nous utilisons aujourd'hui.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#machine-learning","title":"Machine Learning","text":"<p>Le Machine Learning est une technique d'apprentissage statistique qui permet aux machines d\u2019apprendre \u00e0 partir de donn\u00e9es, sans \u00eatre explicitement programm\u00e9es pour chaque t\u00e2che. Cela se traduit par des mod\u00e8les capables d'effectuer des pr\u00e9dictions ou des classifications dans une multitude de domaines.</p> <p>Prenons l'exemple de la pr\u00e9diction des prix immobiliers : un mod\u00e8le ML peut analyser des donn\u00e9es historiques sur les ventes de maisons pour d\u00e9terminer le prix probable d'un bien donn\u00e9 en se basant sur diff\u00e9rentes caract\u00e9ristiques (surface, nombre de chambres, etc.). De m\u00eame, dans le domaine de la reconnaissance faciale, ces syst\u00e8mes sont form\u00e9s \u00e0 partir de milliers d'images pour identifier avec pr\u00e9cision un visage dans une foule.</p> <p>L'essentiel ici est de comprendre que le Machine Learning n'est pas juste un outil magique. C'est une approche m\u00e9thodique qui n\u00e9cessite des donn\u00e9es de qualit\u00e9 et une \u00e9valuation continue pour s'assurer que le mod\u00e8le reste performant et pertinent.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#deep-learning","title":"Deep Learning","text":"<p>Le Deep Learning est une technique avanc\u00e9e de machine learning qui utilise des r\u00e9seaux de neurones (une sorte d'algorithme math\u00e9matique avec de nombreux chiffres et \u00e9quations) pour analyser des donn\u00e9es complexes. Sa force r\u00e9side dans sa capacit\u00e9 \u00e0 traiter les donn\u00e9es de fa\u00e7on hi\u00e9rarchique, un peu comme notre cerveau (ou du moins, on essaie d'imiter math\u00e9matiquement son fonctionnement).</p> <p>Les r\u00e9seaux de neurones profonds ont permis des avanc\u00e9es spectaculaires dans des t\u00e2ches n\u00e9cessitant une compr\u00e9hension fine, comme la reconnaissance de la voix ou la compr\u00e9hension du langage naturel. Cependant, cette sophistication a un co\u00fbt : les ressources n\u00e9cessaires sont cons\u00e9quentes, que ce soit en termes de puissance de calcul ou de volume de donn\u00e9es requis pour l'entra\u00eenement (= les donn\u00e9es historiques).</p> <p>Ainsi, lorsqu'on envisage d'utiliser le deep learning, il est crucial d'\u00e9valuer si l'investissement en ressources est justifi\u00e9 par les b\u00e9n\u00e9fices escompt\u00e9s. Pour beaucoup d'applications, il pourrait \u00eatre plus pragmatique de commencer avec des techniques de Machine Learning plus simples, avant d'\u00e9voluer vers le Deep Learning si n\u00e9cessaire.</p> <p>Ces deux composantes, bien que puissantes, n\u00e9cessitent une approche r\u00e9fl\u00e9chie et strat\u00e9gique pour \u00eatre int\u00e9gr\u00e9es efficacement dans vos projets. Une compr\u00e9hension claire de chaque technique, combin\u00e9e \u00e0 une \u00e9valuation m\u00e9thodique des erreurs et des performances, est la cl\u00e9 pour r\u00e9ussir dans le monde passionnant de l'IA.</p> <p>Pour mieux comprendre les relations entre l'Intelligence Artificielle, le Machine Learning et le Deep Learning, voici une repr\u00e9sentation visuelle qui illustre comment ces domaines s'imbriquent les uns dans les autres :</p> <p></p> <p>Cette image montre clairement que le Deep Learning est un sous-ensemble du Machine Learning, qui est lui-m\u00eame une branche de l'Intelligence Artificielle. Chaque couche repr\u00e9sente une approche plus sp\u00e9cialis\u00e9e et sophistiqu\u00e9e, avec des techniques et des applications sp\u00e9cifiques.</p> <p>Cette hi\u00e9rarchie nous rappelle que, bien que le Deep Learning soit actuellement au c\u0153ur de nombreuses innovations spectaculaires, il s'inscrit dans un \u00e9cosyst\u00e8me plus large de m\u00e9thodes et d'approches qui constituent ensemble le domaine de l'Intelligence Artificielle.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#les-domaines-de-lia","title":"Les domaines de l'IA","text":"<p>Dans le vaste univers de l'intelligence artificielle, il est crucial de comprendre que diff\u00e9rents domaines existent, chacun ayant des applications sp\u00e9cifiques et des relations \u00e9troites avec les composantes du machine learning et du deep learning. Dans cette section, je vais pr\u00e9senter un aper\u00e7u de chaque domaine majeur de l'IA, en expliquant comment ils s'appuient sur le machine learning et le deep learning pour fonctionner efficacement.</p> <p>On peut voir un domaine de l'IA comme une sp\u00e9cialit\u00e9. Imaginez que l'IA est la m\u00e9decine. Les diff\u00e9rents domaines de la m\u00e9decine sont la cardiologie, la dentisterie, etc. C'est pareil pour l'IA. Les domaines de l'IA peuvent \u00eatre l'imagerie, les moteurs de recommandation, le traitement du langage, etc. Et dans chaque domaine, on peut appliquer du machine learning ou du deep learning.</p> <p>Comprendre l'intelligence artificielle, c'est aussi d\u00e9couvrir la diversit\u00e9 de ses domaines d'application et la fa\u00e7on dont ils transforment notre quotidien.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#vision-par-ordinateur-voir-pour-comprendre","title":"Vision par ordinateur : Voir pour comprendre","text":"<p>Le domaine de la vision par ordinateur se concentre sur la capacit\u00e9 des machines \u00e0 analyser et interpr\u00e9ter des images et des vid\u00e9os. Que ce soit pour reconna\u00eetre des visages sur Facebook ou pour permettre \u00e0 une voiture autonome de \"voir\" la route, les applications sont vari\u00e9es et en constante \u00e9volution.</p> <p>L'analyse vid\u00e9o en temps r\u00e9el, essentielle pour des applications telles que la s\u00e9curit\u00e9 ou le sport, d\u00e9pend fortement de cette technologie. Mais attention, toute cette magie visuelle repose sur des techniques avanc\u00e9es d'apprentage supervis\u00e9 (o\u00f9 l'algorithme apprend \u00e0 partir d'exemples \u00e9tiquet\u00e9s par des humains) et d'\u00e9normes ensembles de donn\u00e9es annot\u00e9es. Sans ces fondations solides, m\u00eame les algorithmes les plus prometteurs risquent de tr\u00e9bucher.</p> <p>Il est important de souligner que la vision par ordinateur est un domaine extr\u00eamement vaste et complexe. Ma\u00eetriser ses diff\u00e9rentes facettes, de la d\u00e9tection d'objets \u00e0 la segmentation d'images, et la reconnaissance faciale (et beaucoup d'autres...) peut n\u00e9cessiter plusieurs ann\u00e9es d'\u00e9tude et de pratique. Chaque sous-domaine poss\u00e8de ses propres d\u00e9fis techniques et m\u00e9thodologiques, et les chercheurs continuent d'innover constamment pour repousser les limites de ce que les machines peuvent \"voir\" et comprendre.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#traitement-du-langage-naturel-nlp-communiquer-avec-les-machines","title":"Traitement du Langage Naturel (NLP) : Communiquer avec les machines","text":"<p>Le NLP est la branche de l'IA qui se penche sur la compr\u00e9hension et la g\u00e9n\u00e9ration du langage humain par les machines. Que ce soit pour traduire un texte ou interagir avec un assistant vocal, la capacit\u00e9 \u00e0 comprendre le langage est primordiale.</p> <p>Pourtant, comme dans toute relation, la communication n'est pas d\u00e9pourvue de d\u00e9fis. Les nuances, les contextes culturels, et les langues rendent la t\u00e2che complexe. Des avanc\u00e9es comme les mod\u00e8les de langage GPT tentent d'apprivoiser cette complexit\u00e9, mais un mod\u00e8le mal entra\u00een\u00e9 peut \"halluciner\" (inventer des informations) ou produire des r\u00e9ponses inappropri\u00e9es.</p> <p>Comme le domaine de la vision par ordinateur, le traitement du langage naturel est un domaine extr\u00eamement vaste et complexe. Ma\u00eetriser ses diff\u00e9rentes facettes et apporter des solutions adapt\u00e9es \u00e0 chaque probl\u00e9matique n\u00e9cessitent plusieurs ann\u00e9es d'expertise.</p> <p>Une application pratique tr\u00e8s populaire du NLP est le RAG (Retrieval-Augmented Generation), qui permet de cr\u00e9er des chatbots capables de r\u00e9pondre \u00e0 des questions sur vos propres documents. Si vous souhaitez int\u00e9grer ce type de solution sur votre site web sans avoir \u00e0 d\u00e9velopper toute l'infrastructure vous-m\u00eame, heeya offre une solution RAG pr\u00eate \u00e0 l'emploi qui peut \u00eatre d\u00e9ploy\u00e9e en quelques minutes.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#series-temporelles-predire-lavenir","title":"S\u00e9ries temporelles : Pr\u00e9dire l'avenir","text":"<p>Les s\u00e9ries temporelles constituent \u00e9galement un domaine \u00e0 part enti\u00e8re. Lorsqu'il s'agit d'analyser des donn\u00e9es ordonn\u00e9es dans le temps, comme les pr\u00e9visions m\u00e9t\u00e9orologiques ou les cours de la bourse, les s\u00e9ries temporelles sont essentielles.</p> <p>Ces analyses permettent non seulement de comprendre des tendances pass\u00e9es mais aussi de pr\u00e9dire des \u00e9v\u00e9nements futurs. Mais attention, pr\u00e9dire l'avenir reste risqu\u00e9 avec une part d'erreur (ce qui n'emp\u00eache pas les pr\u00e9dictions de ramener de la valeur). Les mod\u00e8les doivent \u00eatre constamment ajust\u00e9s, notant que des approches rigides peuvent manquer de flexibilit\u00e9 face \u00e0 des \u00e9v\u00e9nements impr\u00e9vus.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#systemes-de-recommandation-personnaliser-lexperience-utilisateur","title":"Syst\u00e8mes de Recommandation : Personnaliser l'exp\u00e9rience utilisateur","text":"<p>Les syst\u00e8mes de recommandation sont un domaine \u00e0 part enti\u00e8re de l'IA, focalis\u00e9 sur la cr\u00e9ation d'exp\u00e9riences personnalis\u00e9es. Ils ne se contentent pas d'analyser vos habitudes, ils utilisent des m\u00e9thodes uniques pour anticiper vos d\u00e9sirs.</p> <p>Prenons l'exemple de Netflix : lorsque vous terminez une s\u00e9rie, le syst\u00e8me analyse vos pr\u00e9f\u00e9rences et propose imm\u00e9diatement des films ou s\u00e9ries qui pourraient vous plaire, en tenant compte de ce que d'autres utilisateurs ayant des go\u00fbts similaires ont appr\u00e9ci\u00e9. Ce domaine se distingue par son approche d\u00e9di\u00e9e \u00e0 enrichir continuellement votre interaction avec les plateformes.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#apprentissage-sur-donnees-tabulaires-analyser-les-tableaux","title":"Apprentissage sur donn\u00e9es tabulaires : Analyser les tableaux","text":"<p>L'apprentissage sur donn\u00e9es tabulaires (comme les donn\u00e9es Excel) est une discipline qui se concentre sur l'utilisation des donn\u00e9es structur\u00e9es pour prendre des d\u00e9cisions claires. Imaginez une entreprise qui cherche \u00e0 pr\u00e9voir ses ventes futures : en analysant des tableaux de donn\u00e9es contenant l'historique des ventes, les prix, et les p\u00e9riodes de l'ann\u00e9e, elle peut anticiper les tendances et ajuster ses strat\u00e9gies de marketing.</p> <p>Ce domaine se distingue par sa capacit\u00e9 \u00e0 transformer des donn\u00e9es en actions concr\u00e8tes, offrant un support essentiel aux d\u00e9cisions strat\u00e9giques des entreprises.</p> <p>Ces domaines, bien qu'ils partagent certains concepts avec d'autres branches de l'IA, n\u00e9cessitent une approche sp\u00e9cialis\u00e9e et une compr\u00e9hension approfondie des d\u00e9fis singuliers qu'ils pr\u00e9sentent.</p> <p>Une chose \u00e0 retenir : il est essentiel de comprendre que l'IA est intrins\u00e8quement li\u00e9e aux types de donn\u00e9es et aux probl\u00e9matiques sp\u00e9cifiques qu'elle aborde. Comme soulign\u00e9 pr\u00e9c\u00e9demment, l'IA ne peut exister sans donn\u00e9es. Chaque domaine de l'IA peut tirer parti des techniques de machine learning et de deep learning, selon les avanc\u00e9es scientifiques. Par exemple, pour les donn\u00e9es tabulaires, telles que la pr\u00e9diction des prix immobiliers, le machine learning et les analyses statistiques sont souvent privil\u00e9gi\u00e9s. \u00c0 l'inverse, pour la vision par ordinateur, qui implique le traitement d'images comme la reconnaissance faciale, le deep learning est g\u00e9n\u00e9ralement plus adapt\u00e9. Ces choix sont guid\u00e9s par les recherches actuelles et les m\u00e9thodes \u00e9prouv\u00e9es par la communaut\u00e9 scientifique.</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#conclusion","title":"Conclusion","text":"<p>Pour conclure cette premi\u00e8re partie, nous avons explor\u00e9 ensemble les grands domaines qui composent l\u2019intelligence artificielle. Chacun poss\u00e8de ses sp\u00e9cificit\u00e9s, ses d\u00e9fis et ses applications, ce qui rend l\u2019IA aussi riche que passionnante. Bien entendu, il existe de nombreux autres sous-domaines, parfois plus discrets ou \u00e9mergents, que je n\u2019ai pas pu aborder ici. L\u2019essentiel \u00e0 retenir est que, finalement, comprendre l\u2019intelligence artificielle consiste \u00e0 comprendre quelques concepts de base qui sont les fondations de l'IA.</p> <p>Dans la suite, nous plongerons dans l'univers passionnant de l'IA g\u00e9n\u00e9rative. En attendant la partie deux, n'h\u00e9sitez pas \u00e0 vous abonner \u00e0 ma newsletter \ud83d\ude00 \u00c0 bient\u00f4t ! </p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p>","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2025/04/05/comprendre-lintelligence-artificielle--guide-simple-partie-1/#a-propos-de-moi","title":"\u00c0 propos de moi","text":"<p>Je suis Anas Rabhi, consultant Data Scientist freelance. J'accompagne les entreprises dans leur strat\u00e9gie et mise en \u0153uvre de solutions d'IA (RAG, Agents, NLP). </p> <p>D\u00e9couvrez mes services sur tensoria.fr ou testez notre solution d'agents IA heeya.fr.</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["Intelligence Artificielle","Conseils Pratiques"]},{"location":"blog/2026/02/05/le-rag-est-il-vraiment-fini-/","title":"Le RAG est-il vraiment fini ?","text":"","tags":["RAG","Intelligence Artificielle","Strat\u00e9gie","Co\u00fbts"]},{"location":"blog/2026/02/05/le-rag-est-il-vraiment-fini-/#introduction-le-rag-une-methode-magique","title":"Introduction : le RAG, une m\u00e9thode magique ?","text":"<p>\u00c0 chaque sortie d'un nouveau mod\u00e8le avec une fen\u00eatre contextuelle plus grande, on annonce le RAG comme d\u00e9pass\u00e9. Pourtant, le RAG est n\u00e9 d'un besoin tr\u00e8s concret : on ne peut pas donner un document de 400 ou 500 pages \u00e0 un LLM et lui poser des questions dessus.</p> <p>En entreprise, on a souvent des dizaines (voire des centaines) de fichiers. Le RAG apporte une r\u00e9ponse simple : construire une base documentaire avec des petits morceaux (chunks) de documents, puis fournir dynamiquement les morceaux pertinents \u00e0 l'IA \u00e0 chaque question.</p> <p>C'est une technique comme une autre : parfois adapt\u00e9e, parfois non. Si vous avez un petit document de 10 pages, inutile de monter un RAG : on peut le charger directement dans un LLM et poser des questions. En revanche, 100 articles de 100 pages, m\u00eame si c'est possible \u00e0 charger, ce n'est pas toujours pertinent (ni rentable).</p> <p>M\u00eame si les fen\u00eatres contextuelles explosent (on atteint 1M de tokens, soit environ 700k mots sur certains mod\u00e8les), charger 1M de tokens reste co\u00fbteux : parfois entre 2 et 10 euros par question. Il existe des techniques pour r\u00e9duire ces co\u00fbts (cache, etc.), mais 0,20 \u20ac \u00d7 100 questions, \u00e7a fait quand m\u00eame 20 \u20ac.</p> <p>Et surtout : injecter trop d'information d\u00e9grade souvent la qualit\u00e9 des r\u00e9ponses. Plus on surcharge la fen\u00eatre, plus le LLM se noie. Le RAG n'est donc pas pr\u00e8s de dispara\u00eetre.</p> <p>Le RAG s'est impos\u00e9 depuis l'arriv\u00e9e de ChatGPT parce qu'il r\u00e9pond \u00e0 une probl\u00e9matique majeure : la connaissance des LLMs est limit\u00e9e aux donn\u00e9es vues pendant l'entra\u00eenement. Or, dans la plupart des cas d'usage en entreprise, on veut que l'IA r\u00e9ponde sur les donn\u00e9es internes.</p> <p>Le RAG a un c\u00f4t\u00e9 \"magique\" : si on combine une base vectorielle qui stocke les documents de l'entreprise et un LLM, on peut permettre \u00e0 l'IA de r\u00e9pondre \u00e0 n'importe quelle question sur ces donn\u00e9es.</p>","tags":["RAG","Intelligence Artificielle","Strat\u00e9gie","Co\u00fbts"]},{"location":"blog/2026/02/05/le-rag-est-il-vraiment-fini-/#comment-ca-marche","title":"Comment \u00e7a marche ?","text":"<p>Un RAG a deux grandes \u00e9tapes :</p> <p>1) Traitement &amp; ingestion On traite les documents, on les d\u00e9coupe en chunks, puis on les ing\u00e8re dans une base vectorielle (exemple : https://www.cloudflare.com/fr-fr/learning/ai/what-is-vector-database/). Cette \u00e9tape n'est pas visible par l'utilisateur et est faite au d\u00e9but du projet, puis mise \u00e0 jour \u00e0 chaque changement de document.</p> <p>2) Recherche &amp; g\u00e9n\u00e9ration \u00c0 chaque requ\u00eate, on r\u00e9cup\u00e8re les chunks pertinents via la base vectorielle, puis on les injecte dans le prompt du LLM pour am\u00e9liorer la r\u00e9ponse et r\u00e9duire les hallucinations.</p> <p>La mise en place d'un RAG \"basique\" est assez simple et rapide. Et comme l'humain g\u00e9n\u00e9ralise vite, on se dit : c'est parfait, on met en prod, c'est fini. Sauf que \u00e7a ne se passe jamais comme \u00e7a.</p> <p>Un RAG rapide donne souvent 50 \u00e0 70 % de bonnes r\u00e9ponses. En entreprise, \u00e7a peut ne pas suffire pour l'exposer aux utilisateurs finaux.</p> <p>Si ce sujet t'int\u00e9resse, j'ai d\u00e9taill\u00e9 les causes fr\u00e9quentes d'un RAG qui ne r\u00e9pond pas bien, et comment les corriger dans cet article : Les 4 causes techniques d'\u00e9chec d'un RAG (et comment les corriger)</p>","tags":["RAG","Intelligence Artificielle","Strat\u00e9gie","Co\u00fbts"]},{"location":"blog/2026/02/05/le-rag-est-il-vraiment-fini-/#conclusion-le-rag-est-il-vraiment-fini","title":"Conclusion : le RAG est-il vraiment fini ?","text":"<p>Le RAG n'est pas mort. Il reste une approche pragmatique pour rendre les LLMs utiles sur des donn\u00e9es internes, avec un bon \u00e9quilibre entre pertinence, co\u00fbts et qualit\u00e9.</p> <p>Plut\u00f4t que de se demander si le RAG est fini, la vraie question est : \u00e0 quel moment un RAG est utile (ou non) pour votre cas d'usage.</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p>","tags":["RAG","Intelligence Artificielle","Strat\u00e9gie","Co\u00fbts"]},{"location":"blog/2026/02/05/le-rag-est-il-vraiment-fini-/#a-propos-de-moi","title":"\u00c0 propos de moi","text":"<p>Je suis Anas Rabhi, consultant Data Scientist freelance. J'accompagne les entreprises dans leur strat\u00e9gie et mise en \u0153uvre de solutions d'IA (RAG, Agents, NLP). </p> <p>D\u00e9couvrez mes services sur tensoria.fr ou testez notre solution d'agents IA heeya.fr.</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["RAG","Intelligence Artificielle","Strat\u00e9gie","Co\u00fbts"]},{"location":"blog/2026/02/05/les-4-causes-techniques-d%C3%A9chec-dun-rag-et-comment-les-corriger/","title":"Les 4 causes techniques d'\u00e9chec d'un RAG (et comment les corriger)","text":"","tags":["RAG","Optimisation","Retrieval","LLM"]},{"location":"blog/2026/02/05/les-4-causes-techniques-d%C3%A9chec-dun-rag-et-comment-les-corriger/#introduction","title":"Introduction","text":"<p>Un RAG \"basique\" est rapide \u00e0 mettre en place, mais il plafonne souvent entre 50 et 70 % de bonnes r\u00e9ponses. En entreprise, ce n'est pas suffisant pour un usage fiable.</p> <p>Si tu cherches plut\u00f4t une m\u00e9thode d'analyse d'erreur pour prioriser les actions d'am\u00e9lioration, l'article d\u00e9di\u00e9 est ici : Mon RAG ne marche pas : pourquoi l\u2019analyse d\u2019erreur change tout</p> <p>Si tu veux d'abord comprendre pourquoi le RAG reste utile malgr\u00e9 les grandes fen\u00eatres contextuelles, j'ai un article d\u00e9di\u00e9 : Le RAG est-il vraiment fini ?</p> <p>Ici, on se concentre sur l'autre question : pourquoi un RAG ne r\u00e9pond pas correctement, et comment l'am\u00e9liorer.</p>","tags":["RAG","Optimisation","Retrieval","LLM"]},{"location":"blog/2026/02/05/les-4-causes-techniques-d%C3%A9chec-dun-rag-et-comment-les-corriger/#cause-technique-1-le-llm-nest-pas-assez-bon","title":"Cause technique 1 \u2014 Le LLM n'est pas assez bon","text":"<p>Ce cas est le plus simple. En g\u00e9n\u00e9ral, avec le bon contexte, les derniers LLMs savent r\u00e9pondre.</p> <p>Quand le LLM est en cause ? Souvent quand il est noy\u00e9 dans l'information : on r\u00e9cup\u00e8re trop de chunks, parfois pas assez pertinents, et la vraie info se perd.</p> <p>Comment corriger ? Soit on choisit un meilleur mod\u00e8le, soit on optimise le nombre de chunks \u00e0 extraire. Mais \u00e7a, c'est un autre chantier.</p>","tags":["RAG","Optimisation","Retrieval","LLM"]},{"location":"blog/2026/02/05/les-4-causes-techniques-d%C3%A9chec-dun-rag-et-comment-les-corriger/#cause-technique-2-le-parsing-des-documents-est-insuffisant","title":"Cause technique 2 \u2014 Le parsing des documents est insuffisant","text":"<p>Le parsing consiste \u00e0 extraire correctement les informations des documents. Exemple : une facture doit \u00eatre extraite de fa\u00e7on structur\u00e9e. Les tableaux sont un cas classique : selon leur format, r\u00e9cup\u00e9rer les colonnes et les lignes peut devenir tr\u00e8s complexe. M\u00eame les images posent probl\u00e8me, sauf si on les d\u00e9crit via un LLM et qu'on ins\u00e8re la description dans le texte.</p> <p>Le parsing est l'un des plus gros probl\u00e8mes du RAG. Il est difficile d'avoir un parsing propre, car chaque entreprise structure ses documents diff\u00e9remment (tableaux, images, formules, graphiques, etc.). Le parsing est donc sp\u00e9cifique \u00e0 chaque type de document, m\u00eame s'il existe des solutions qui tentent de g\u00e9n\u00e9raliser (par exemple docling : https://www.docling.ai/).</p>","tags":["RAG","Optimisation","Retrieval","LLM"]},{"location":"blog/2026/02/05/les-4-causes-techniques-d%C3%A9chec-dun-rag-et-comment-les-corriger/#cause-technique-3-linformation-nest-pas-dans-le-contexte-fourni","title":"Cause technique 3 \u2014 L'information n'est pas dans le contexte fourni","text":"<p>Parfois, on ne r\u00e9cup\u00e8re pas les bons documents : la requ\u00eate est floue ou ne matche pas bien avec la base vectorielle.</p> <p>Dans ce cas, le probl\u00e8me peut venir du chunking. Exemple : un PDF o\u00f9 le dernier paragraphe est coup\u00e9 sur deux pages, et une m\u00e9thode de chunking qui d\u00e9coupe par page. R\u00e9sultat : le paragraphe est scind\u00e9 en deux chunks, et le LLM ne trouve pas la bonne info.</p> <p>Si c'est la cause (on peut le savoir en analysant les erreurs, voir cet article), il faut travailler le chunking : taille des chunks, m\u00e9thode de d\u00e9coupage, overlap, etc.</p>","tags":["RAG","Optimisation","Retrieval","LLM"]},{"location":"blog/2026/02/05/les-4-causes-techniques-d%C3%A9chec-dun-rag-et-comment-les-corriger/#cause-technique-4-linformation-nest-pas-dans-les-documents-et-le-chunking-est-bon","title":"Cause technique 4 \u2014 L'information n'est pas dans les documents (et le chunking est bon)","text":"<p>Parfois, m\u00eame avec un bon chunking, le probl\u00e8me vient du retriever. Le retriever r\u00e9cup\u00e8re les chunks pertinents et les injecte dans le prompt. S'il ne trouve pas les bons chunks, le LLM ne pourra pas r\u00e9pondre.</p> <p>Souvent, la recherche est s\u00e9mantique. On utilise un mod\u00e8le d'embeddings pour vectoriser les documents, puis la question de l'utilisateur, et on compare les vecteurs (avec le m\u00eame mod\u00e8le, sinon \u00e7a n'a pas de sens).</p> <p>On peut optimiser l'embedding model, surtout si vous utilisez un mod\u00e8le open source comme <code>BAAI/bge-m3</code> (https://huggingface.co/BAAI/bge-m3). Si vous utilisez un mod\u00e8le comme <code>text-embedding-3-large</code> d'OpenAI, on est d\u00e9j\u00e0 sur du tr\u00e8s performant, sauf si votre champ lexical est tr\u00e8s sp\u00e9cifique (univers de niche). Dans ce cas, on peut imaginer un fine-tuning, mais c'est une option \u00e0 privil\u00e9gier apr\u00e8s les autres.</p> <p>Une m\u00e9thode tr\u00e8s efficace consiste \u00e0 combiner recherche s\u00e9mantique et recherche par mots-cl\u00e9s, via des techniques de type BM25 (https://en.wikipedia.org/wiki/Okapi_BM25). Dans certains cas, on observe des gains de 10 \u00e0 20 % de qualit\u00e9. Exemple : sur un site e-commerce, une requ\u00eate sur une couleur peut \u00eatre mal g\u00e9r\u00e9e par la recherche s\u00e9mantique ; ajouter la recherche par mots-cl\u00e9s force les chunks qui contiennent la couleur.</p> <p>Enfin, m\u00eame avec tout \u00e7a, le retriever peut ne pas atteindre les performances cibl\u00e9es (viser 100 % de bonnes r\u00e9ponses n'est pas r\u00e9aliste). Une cause fr\u00e9quente : la recherche est bas\u00e9e sur la question, alors que ce qu'on veut extraire, c'est la r\u00e9ponse. La question peut \u00eatre trop courte ou vague. Il existe des techniques pour \u00e7a (https://arxiv.org/pdf/2312.10997), comme HyDE (Hypothetical Document Embeddings), tr\u00e8s utile quand la question est mal formul\u00e9e.</p>","tags":["RAG","Optimisation","Retrieval","LLM"]},{"location":"blog/2026/02/05/les-4-causes-techniques-d%C3%A9chec-dun-rag-et-comment-les-corriger/#conclusion","title":"Conclusion","text":"<p>Un RAG qui marche bien, ce n'est pas un RAG \"magique\" : c'est un RAG analys\u00e9, mesur\u00e9 et optimis\u00e9. La qualit\u00e9 d\u00e9pend du parsing, du chunking, du retriever et du mod\u00e8le.</p> <p>Si tu veux, je peux aussi d\u00e9tailler une m\u00e9thode simple d'audit pour identifier rapidement la vraie cause des erreurs.</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p>","tags":["RAG","Optimisation","Retrieval","LLM"]},{"location":"blog/2026/02/05/les-4-causes-techniques-d%C3%A9chec-dun-rag-et-comment-les-corriger/#a-propos-de-moi","title":"\u00c0 propos de moi","text":"<p>Je suis Anas Rabhi, consultant Data Scientist freelance. J'accompagne les entreprises dans leur strat\u00e9gie et mise en \u0153uvre de solutions d'IA (RAG, Agents, NLP). </p> <p>D\u00e9couvrez mes services sur tensoria.fr ou testez notre solution d'agents IA heeya.fr.</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["RAG","Optimisation","Retrieval","LLM"]},{"location":"blog/2026/02/21/les-5-erreurs-que-tout-le-monde-fait-avec-le-rag/","title":"Les 5 erreurs que tout le monde fait avec le RAG","text":"","tags":["RAG","Intelligence Artificielle","Retour d'exp\u00e9rience","Conseils Pratiques"]},{"location":"blog/2026/02/21/les-5-erreurs-que-tout-le-monde-fait-avec-le-rag/#introduction","title":"Introduction","text":"<p>Depuis 2023, j'ai r\u00e9alis\u00e9 une dizaine de projets RAG moi-m\u00eame, et j'en ai dirig\u00e9 une autre dizaine avec des \u00e9quipes. Certains se sont tr\u00e8s bien pass\u00e9s, d'autres un peu moins, mais on a toujours essay\u00e9 d'apprendre et se corriger tout au long du projet. Avec le recul, je retrouve toujours les m\u00eames erreurs, que ce soit chez moi au d\u00e9but, chez des clients, ou chez des confr\u00e8res. Ce ne sont pas des erreurs techniques (j'en parle dans cet article), mais des erreurs de posture, d'approche et de m\u00e9thode.</p> <p>Ce sont des erreurs qu'on fait tous au moins une fois. L'id\u00e9e ici, c'est de les poser clairement pour \u00e9viter de les r\u00e9p\u00e9ter.</p>","tags":["RAG","Intelligence Artificielle","Retour d'exp\u00e9rience","Conseils Pratiques"]},{"location":"blog/2026/02/21/les-5-erreurs-que-tout-le-monde-fait-avec-le-rag/#erreur-n1-croire-que-le-rag-va-repondre-a-tout","title":"Erreur n\u00b01 \u2014 Croire que le RAG va r\u00e9pondre \u00e0 tout","text":"<p>C'est probablement l'erreur la plus fr\u00e9quente, et elle ne vient pas forc\u00e9ment de l'\u00e9quipe technique. Elle vient d'un d\u00e9calage entre ce que le client imagine et ce que le RAG fait r\u00e9ellement.</p> <p>Un RAG, c'est de la question-r\u00e9ponse cibl\u00e9e sur une base documentaire. On pose une question, le syst\u00e8me va chercher les bons morceaux de documents, et le LLM formule une r\u00e9ponse \u00e0 partir de \u00e7a. C'est puissant, mais c'est cadr\u00e9.</p> <p>Le probl\u00e8me, c'est que pour quelqu'un qui n'est pas dans le monde de l'IA, \"une IA, \u00e7a r\u00e9pond \u00e0 tout\". C'est l'image que les gens ont. Donc quand on met en place un chatbot RAG, le client s'attend naturellement \u00e0 ce que \u00e7a g\u00e8re n'importe quelle demande.</p> <p>Je me souviens d'un de nos premiers projets RAG, en 2023, quand GPT-3.5 commen\u00e7ait \u00e0 \u00eatre vraiment exploitable. On avait mis en place un chatbot pour Odecia afin de r\u00e9pondre aux questions des clients sur leur site. Le syst\u00e8me fonctionnait bien sur les questions qu'on avait anticip\u00e9es. Sauf qu'en analysant les vraies questions des utilisateurs, on s'est vite rendu compte que les questions r\u00e9elles n'avaient rien \u00e0 voir avec celles qu'on avait imagin\u00e9es.</p> <p>Des utilisateurs demandaient \"r\u00e9sume-moi ce fichier\", ou posaient des questions qui n'avaient aucun rapport avec la documentation disponible. Pour un data scientist, c'est \u00e9vident que \u00e7a ne peut pas marcher : le RAG n'a pas acc\u00e8s au fichier de l'utilisateur, et il ne peut r\u00e9pondre que sur ce qu'il conna\u00eet. Mais pour une personne ext\u00e9rieure au monde de l'IA, c'est incompr\u00e9hensible. \"C'est de l'IA, non ? \u00c7a devrait comprendre.\"</p> <p>C'\u00e9tait un de nos premiers RAG, et \u00e7a nous a appris une chose essentielle : il faut d\u00e8s le d\u00e9but bien communiquer sur ce que le RAG peut faire et ce qu'il ne peut pas faire. Expliquer clairement le p\u00e9rim\u00e8tre. Donner des exemples de questions qui fonctionnent et de questions qui ne fonctionneront pas. G\u00e9rer les attentes d\u00e8s le d\u00e9part, pas apr\u00e8s la d\u00e9ception.</p> <p>Depuis, on fait syst\u00e9matiquement cet exercice avec chaque nouveau client. Et \u00e7a change tout. Moins de frustration, moins de d\u00e9ception, et un projet qui part sur de bonnes bases. C'est vrai que \u00e7a fait moins r\u00eaver, mais on ne vend pas de r\u00eave au client, on essaye de vendre un produit qui fonctionne, avec la r\u00e9alit\u00e9 du terrain. Ce que je dis souvent \u00e0 mes clients : on a pas besoin que l'IA fasse 100% de r\u00e9ponses justes pour que \u00e7a nous fasse gagner du temps. A 90% de bonnes r\u00e9ponses on est d\u00e9j\u00e0 gagnant dans l'histoire. Economiquement parlant, 90% bonnes r\u00e9ponses, veut dire qu'on peut renseigner 9 clients sur 10, ce qui \u00e9vite d\u00e9j\u00e0 la perte de temps \u00e0 r\u00e9pondre \u00e0 des questions r\u00e9p\u00e9titives, et \u00e9vite de perdre des clients. Le client est tr\u00e8s statisfait quand il trouve des r\u00e9ponses de fa\u00e7on instantan\u00e9e, et sans avoir \u00e0 chercher manuellement dans des documents.</p>","tags":["RAG","Intelligence Artificielle","Retour d'exp\u00e9rience","Conseils Pratiques"]},{"location":"blog/2026/02/21/les-5-erreurs-que-tout-le-monde-fait-avec-le-rag/#erreur-n2-penser-que-le-prochain-rag-sera-simple-parce-quon-en-a-deja-fait-un","title":"Erreur n\u00b02 \u2014 Penser que le prochain RAG sera simple parce qu'on en a d\u00e9j\u00e0 fait un","text":"<p>Celle-l\u00e0, on l'a tous faite. On livre un premier RAG, \u00e7a se passe bien, et on se dit : \"Bon, maintenant on a la m\u00e9thode, le prochain sera rapide.\" Faux.</p> <p>Un RAG, c'est du cas par cas. Chaque projet d\u00e9pend des donn\u00e9es du client, du type de questions pos\u00e9es, du volume de documents, de la complexit\u00e9 des formats. D'un business \u00e0 un autre, ce sont des donn\u00e9es diff\u00e9rentes, des questions diff\u00e9rentes, et donc des probl\u00e8mes diff\u00e9rents.</p> <p>Pour donner un ordre d'id\u00e9e concret : j'ai eu des projets RAG o\u00f9 on a pass\u00e9 2 mois pour industrialiser le syst\u00e8me et atteindre environ 90 % de bonnes r\u00e9ponses. Des milliers de documents \u00e0 ing\u00e9rer, des mises \u00e0 jour hebdomadaires, mais des documents relativement propres et des questions pr\u00e9visibles. C'\u00e9tait du travail, mais on savait o\u00f9 on allait.</p> <p>Et puis j'ai eu d'autres projets o\u00f9 on a pass\u00e9 6 mois pour atteindre \u00e0 peine 80 % de bonnes r\u00e9ponses. La documentation \u00e9tait complexe : des tableaux dans des PDF, des sch\u00e9mas techniques, des documents semi-structur\u00e9s, des formats h\u00e9t\u00e9rog\u00e8nes. Chaque type de document posait un nouveau probl\u00e8me. Chaque am\u00e9lioration sur un type de question en cassait une autre.</p> <p>Le vrai d\u00e9fi, c'est que plus il y a de donn\u00e9es, plus le RAG devient compliqu\u00e9. Ce n'est pas lin\u00e9aire. Passer de 100 \u00e0 1000 documents, ce n'est pas juste \"10x plus de travail\", c'est un changement de nature : le retriever doit \u00eatre plus pr\u00e9cis, le chunking doit \u00eatre plus fin, et les cas limites se multiplient.</p> <p>J'ai aussi eu des clients qui venaient nous voir parce que leur RAG existant ne fonctionnait pas. Quelqu'un leur avait promis un truc simple et rapide. \u00c7a avait \u00e9t\u00e9 fait en quelques jours avec LangChain en mode automatique, sans se poser de questions sur les donn\u00e9es. R\u00e9sultat : des performances sous les 70 %, un client frustr\u00e9, et un projet \u00e0 reprendre quasiment de z\u00e9ro.</p> <p>Chaque RAG est un projet en soi. Il n'y a pas de raccourci.</p>","tags":["RAG","Intelligence Artificielle","Retour d'exp\u00e9rience","Conseils Pratiques"]},{"location":"blog/2026/02/21/les-5-erreurs-que-tout-le-monde-fait-avec-le-rag/#erreur-n3-foncer-dans-le-code-sans-regarder-les-donnees","title":"Erreur n\u00b03 \u2014 Foncer dans le code sans regarder les donn\u00e9es","text":"<p>C'est une erreur classique de d\u00e9veloppeur (et je m'inclus dedans). On re\u00e7oit un brief, on a envie de coder, et on se lance directement dans le pipeline : parsing, chunking, embeddings, retriever, prompt. On a nos habitudes, on conna\u00eet l'architecture, on veut avancer vite.</p> <p>Sauf que la premi\u00e8re chose \u00e0 faire, avant m\u00eame d'\u00e9crire une seule ligne de code, c'est d'ouvrir les donn\u00e9es du client et de les regarder.</p> <p>Combien de fois j'ai vu des estimations de projet compl\u00e8tement \u00e0 c\u00f4t\u00e9 de la r\u00e9alit\u00e9 parce que personne n'avait pris le temps de regarder les documents ? On estime \"3 semaines\" sur la base d'un brief qui dit \"on a des PDF\", et quand on ouvre les PDF, on d\u00e9couvre des scans de mauvaise qualit\u00e9, des tableaux imbriqu\u00e9s, des en-t\u00eates incoh\u00e9rents, des documents de 200 pages sans structure, ou pire, des fichiers qui m\u00e9langent texte, images et annotations manuscrites.</p> <p>Les donn\u00e9es, c'est 80 % du travail d'un RAG. Si les documents sont propres, bien structur\u00e9s, avec du texte exploitable, le RAG sera relativement simple \u00e0 mettre en place. Si les documents sont un chaos de formats h\u00e9t\u00e9rog\u00e8nes, aucun framework ni aucune techno miracle ne va r\u00e9soudre le probl\u00e8me \u00e0 votre place.</p> <p>Concr\u00e8tement, avant de coder quoi que ce soit, je prends maintenant syst\u00e9matiquement une demi-journ\u00e9e (parfois plus) pour :</p> <ul> <li>Ouvrir un \u00e9chantillon repr\u00e9sentatif des documents du client</li> <li>Regarder les formats (PDF, Word, Excel, HTML, images...)</li> <li>V\u00e9rifier la qualit\u00e9 du texte extractible</li> <li>Identifier les cas probl\u00e9matiques (tableaux, sch\u00e9mas, scans)</li> <li>Estimer la volum\u00e9trie et la fr\u00e9quence de mise \u00e0 jour</li> <li>Comprendre quelles questions les utilisateurs vont poser sur ces donn\u00e9es</li> </ul> <p>C'est cette \u00e9tape qui me permet d'estimer correctement un projet. Et c'est cette \u00e9tape que la plupart des gens sautent. On veut aller vite, on veut montrer un premier r\u00e9sultat. Mais un premier r\u00e9sultat sur des donn\u00e9es qu'on n'a pas comprises, \u00e7a ne vaut rien.</p>","tags":["RAG","Intelligence Artificielle","Retour d'exp\u00e9rience","Conseils Pratiques"]},{"location":"blog/2026/02/21/les-5-erreurs-que-tout-le-monde-fait-avec-le-rag/#erreur-n4-tout-faire-avec-un-framework-sans-comprendre-ce-qui-se-passe-derriere","title":"Erreur n\u00b04 \u2014 Tout faire avec un framework sans comprendre ce qui se passe derri\u00e8re","text":"<p>J'ai utilis\u00e9 LangChain dans mes premiers RAG. J'ai aussi beaucoup utilis\u00e9 LlamaIndex. Et je ne dis pas que ces frameworks sont mauvais. Mais je dis qu'il y a un vrai pi\u00e8ge \u00e0 les utiliser sans comprendre ce qu'ils font derri\u00e8re leurs fonctions.</p> <p>Le probl\u00e8me est simple, quand le RAG ne r\u00e9pond pas bien \u00e0 une question, la premi\u00e8re chose \u00e0 faire, c'est de comprendre pourquoi. Et pour comprendre pourquoi, il faut remonter chaque \u00e9tape du pipeline (j'en parle en d\u00e9tail dans cet article sur l'analyse d'erreur). Est-ce que le parsing a bien extrait l'information ? Est-ce que le chunking n'a pas coup\u00e9 au mauvais endroit ? Est-ce que le retriever a trouv\u00e9 les bons chunks ? Est-ce que le prompt \u00e9tait bien formul\u00e9 ?</p> <p>Quand chaque \u00e9tape est encapsul\u00e9e dans une abstraction de framework, debugger devient un cauchemar. On passe un temps fou \u00e0 comprendre ce que fait la fonction <code>X</code> dans la classe <code>Y</code>, \u00e0 naviguer dans la documentation, \u00e0 chercher comment customiser un comportement qui ne nous convient pas.</p> <p>Le temps que j'ai investi \u00e0 customiser les briques de LlamaIndex et \u00e0 comprendre les diff\u00e9rentes \u00e9tapes internes, j'ai pass\u00e9 plus de temps \u00e0 le faire qu'\u00e0 coder mon propre syst\u00e8me de RAG. Et mon propre syst\u00e8me, je le comprends de bout en bout, je peux le modifier en 5 minutes, et je sais exactement ce qui se passe \u00e0 chaque \u00e9tape.</p> <p>Il y a un autre avantage \u00e0 coder ses propres briques : une fois que vous les avez, elles sont \u00e0 vous. D'un projet \u00e0 l'autre, vous reprenez votre code, sur lequel vous avez la connaissance et la ma\u00eetrise totale. Vous pouvez changer ce dont vous avez besoin sans \u00eatre d\u00e9pendant des choix d'un framework.</p> <p>Les frameworks sont pr\u00e9cieux quand ils encapsulent des calculs complexes qu'on ne veut pas recoder (comme TensorFlow ou PyTorch pour le deep learning). Mais pour un RAG, les \u00e9tapes sont conceptuellement simples : parser un document, le d\u00e9couper, le vectoriser, chercher les plus proches voisins, construire un prompt. Ce n'est pas de la rocket science. Le coder soi-m\u00eame une premi\u00e8re fois, c'est un excellent exercice qui permet d'apprendre chaque \u00e9tape en profondeur et de garder la main.</p> <p>Et sur le long terme, c'est encore plus vrai. Des nouvelles techniques sortent toutes les semaines dans le monde du RAG. Quand on veut les explorer ou les int\u00e9grer, on est toujours d\u00e9pendant du framework et de sa capacit\u00e9 \u00e0 s'adapter. Et soyons honn\u00eates : la capacit\u00e9 d'adaptation des frameworks d'IA g\u00e9n\u00e9rative est souvent mauvaise. Le temps qu'une technique soit int\u00e9gr\u00e9e proprement dans un framework, il y en a d\u00e9j\u00e0 trois nouvelles qui sont sorties.</p> <p>Ce que je dis l\u00e0 s'applique aussi aux Agents IA, mais \u00e7a, on en reparlera une prochaine fois.</p>","tags":["RAG","Intelligence Artificielle","Retour d'exp\u00e9rience","Conseils Pratiques"]},{"location":"blog/2026/02/21/les-5-erreurs-que-tout-le-monde-fait-avec-le-rag/#erreur-n5-ne-pas-mesurer-des-le-debut","title":"Erreur n\u00b05 \u2014 Ne pas mesurer d\u00e8s le d\u00e9but","text":"<p>Celle-l\u00e0, c'est l'erreur silencieuse. Le RAG est en place, il tourne, les utilisateurs posent des questions, et on a l'impression que \"\u00e7a marche\". Mais est-ce que \u00e7a marche vraiment ? \u00c0 quel point ? On ne sait pas, parce qu'on n'a rien mis en place pour mesurer.</p> <p>J'ai vu trop de projets o\u00f9 on ne commence \u00e0 mesurer que quand les retours n\u00e9gatifs arrivent. \u00c0 ce moment-l\u00e0, on n'a aucune baseline, aucun historique, aucun moyen de savoir si les choses se sont d\u00e9grad\u00e9es ou si elles n'ont jamais \u00e9t\u00e9 bonnes.</p> <p>La mesure, \u00e7a doit commencer d\u00e8s le premier jour. Pas besoin d'un outil sophistiqu\u00e9 au d\u00e9part : un simple jeu de 30 \u00e0 50 questions-r\u00e9ponses de r\u00e9f\u00e9rence, qu'on passe r\u00e9guli\u00e8rement sur le syst\u00e8me, suffit \u00e0 avoir une id\u00e9e claire de la performance. C'est ce qu'on appelle un dataset d'\u00e9valuation, et c'est la chose la plus importante que personne ne fait.</p> <p>Sans \u00e7a, chaque modification devient un pari. On change le chunking, on ajuste le prompt, on change de mod\u00e8le d'embeddings, et on n'a aucun moyen objectif de savoir si c'est mieux ou pire. On se fie au ressenti, aux retours informels, aux deux ou trois questions qu'on teste \u00e0 la main. Ce n'est pas suffisant.</p> <p>Concr\u00e8tement, ce que je recommande :</p> <ul> <li>Constituer un dataset d'\u00e9val d\u00e8s le d\u00e9but du projet, avec des questions repr\u00e9sentatives et leurs r\u00e9ponses attendues</li> <li>Le passer \u00e0 chaque modification du syst\u00e8me pour v\u00e9rifier qu'on progresse (et qu'on ne casse rien)</li> <li>Logger les interactions en production pour identifier les vrais cas d'usage et les vrais probl\u00e8mes</li> <li>Suivre un score simple (% de bonnes r\u00e9ponses) dans le temps</li> </ul> <p>C'est basique, mais c'est ce qui fait la diff\u00e9rence entre un projet RAG qui s'am\u00e9liore dans le temps et un projet qui stagne sans qu'on comprenne pourquoi.</p>","tags":["RAG","Intelligence Artificielle","Retour d'exp\u00e9rience","Conseils Pratiques"]},{"location":"blog/2026/02/21/les-5-erreurs-que-tout-le-monde-fait-avec-le-rag/#conclusion","title":"Conclusion","text":"<p>Ces 5 erreurs, je les ai toutes faites. L'IA g\u00e9n\u00e9rative donne une impression de magie, et avec cette impression vient la conviction que tout est r\u00e9alisable rapidement. M\u00eame avec des ann\u00e9es de projets data science derri\u00e8re moi, l'habitude de faire du monitoring, de mesurer les performances, de structurer les choses, je n'ai pas \u00e9t\u00e9 immunis\u00e9. On a mal estim\u00e9 certains projets, on a voulu aller trop vite, et on s'est confront\u00e9 \u00e0 chacune de ces erreurs.</p> <p>Et je continue \u00e0 voir des \u00e9quipes les refaire, souvent dans le m\u00eame ordre : on se lance trop vite, on ne communique pas assez sur les limites, on sous-estime la complexit\u00e9, on s'enferme dans un framework, et on oublie de mesurer.</p> <p>Le RAG n'est pas un produit qu'on installe et qu'on oublie. C'est un syst\u00e8me vivant qui demande de la rigueur, de la patience, et surtout une bonne compr\u00e9hension des donn\u00e9es et du besoin m\u00e9tier. La technique vient apr\u00e8s.</p> <p>Si vous voulez aller plus loin sur les aspects techniques, j'ai \u00e9crit sur les 4 causes techniques d'\u00e9chec d'un RAG et sur l'analyse d'erreur comme m\u00e9thode d'am\u00e9lioration.</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p>","tags":["RAG","Intelligence Artificielle","Retour d'exp\u00e9rience","Conseils Pratiques"]},{"location":"blog/2026/02/21/les-5-erreurs-que-tout-le-monde-fait-avec-le-rag/#a-propos-de-moi","title":"\u00c0 propos de moi","text":"<p>Je suis Anas Rabhi, consultant Data Scientist freelance. J'accompagne les entreprises dans leur strat\u00e9gie et mise en \u0153uvre de solutions d'IA (RAG, Agents, NLP).</p> <p>D\u00e9couvrez mes services sur tensoria.fr ou testez notre solution d'agents IA heeya.fr.</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["RAG","Intelligence Artificielle","Retour d'exp\u00e9rience","Conseils Pratiques"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/","title":"Mais c'est quoi le RAG vraiment ? D\u00e9finition, fonctionnement, limites et conseils","text":"","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/#introduction-au-rag-retrieval-augmented-generation","title":"Introduction au RAG (Retrieval-Augmented Generation)","text":"<p>Tout le monde a plus ou moins entendu parler du RAG (Retrieval-Augmented Generation). Mais c'est quoi le RAG exactement ? Beaucoup l'ont m\u00eame d\u00e9j\u00e0 impl\u00e9ment\u00e9, parfois avec des outils \"no-code\" ou des librairies Python comme LangChain ou LlamaIndex. C'est simple \u00e0 mettre en place, mais je vois aussi pas mal de gens d\u00e9\u00e7us du r\u00e9sultat. En r\u00e9alit\u00e9, il faut surtout comprendre \u00e0 quoi \u00e7a sert et comment \u00e7a fonctionne pour savoir si c'est adapt\u00e9 \u00e0 votre besoin.</p> <p>Au d\u00e9but, je ne comptais pas r\u00e9expliquer le RAG ici, il existe d\u00e9j\u00e0 plein de ressources sur le sujet. Mais en discutant avec des personnes qui veulent l'utiliser en entreprise, je me rends compte qu'on passe souvent \u00e0 c\u00f4t\u00e9 de l'essentiel : \u00e0 quoi \u00e7a sert vraiment un RAG, et comment \u00e7a marche concr\u00e8tement.</p> <p>Je vais donc essayer de revenir sur les points que j'ai l'habitude d'\u00e9claircir quand on me pose la question.</p>","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/#la-facilite-dimplementation-du-rag","title":"La facilit\u00e9 d'impl\u00e9mentation du RAG","text":"<p>Mettre en place un RAG, c'est facile. C'est m\u00eame trop facile : on suit un tuto, on branche deux librairies, et hop, \u00e7a tourne. Mais attention, le r\u00e9sultat n'est pas toujours \u00e0 la hauteur des attentes (spoiler : souvent, on est d\u00e9\u00e7u).</p> <p>Justement, cette simplicit\u00e9 cache un pi\u00e8ge. Pour une question tr\u00e8s basique, le RAG peut donner l'impression que tout fonctionne parfaitement. Mais d\u00e8s qu'on sort un peu du cadre, on se rend vite compte que les r\u00e9ponses ne suivent plus.</p> <p>C'est l\u00e0 qu'on peut passer beaucoup de temps \u00e0 bricoler, \u00e0 optimiser les mauvaises briques, sans forc\u00e9ment comprendre o\u00f9 est le vrai probl\u00e8me. Et c'est normal\u202f: comme tout projet d'IA, le RAG est plus complexe qu'il n'y para\u00eet. Il faut vraiment comprendre comment chaque brique fonctionne pour \u00e9viter de tourner en rond. J'ai d\u00e9j\u00e0 \u00e9crit des articles sur comment am\u00e9liorer le RAG ici ou sur l'analyse d'erreur pour comprendre ce qui coince ici.</p> <p>Mais revenons aux bases du RAG dans cet article.</p>","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/#cest-quoi-le-rag-et-pourquoi-lutiliser-en-entreprise","title":"C'est quoi le RAG et pourquoi l'utiliser en entreprise ?","text":"<p>Avant de parler du RAG, il faut d\u00e9j\u00e0 comprendre c'est quoi le RAG et pourquoi on en a besoin. Depuis l'arriv\u00e9e de ChatGPT, on a tous vu \u00e0 quel point les mod\u00e8les de langage (appel\u00e9s LLM) sont puissants. Mais il y a une limite : ils ne connaissent pas nos donn\u00e9es \u00e0 nous, ni les infos internes d'une entreprise.</p> <p>Prenons un exemple tout simple : j'ai une documentation sur une page Word. Si je veux poser des questions \u00e0 l'IA sur ce document, je peux copier le texte dans ChatGPT : l'IA va le lire et r\u00e9pondre, parce que j'ai mis le texte directement dans ce qu'on appelle le contexte du mod\u00e8le (ou le prompt). </p> <p>Sauf que la fen\u00eatre de contexte est limit\u00e9e \u00e0 un certain nombre de mots (ou plus pr\u00e9cis\u00e9ment de tokens). On peut charger une page, mais pas 10 000 d'un coup. En entreprise, on a souvent des documents tr\u00e8s volumineux, et on aimerait qu'une IA puisse r\u00e9pondre \u00e0 des questions sur l'ensemble de ces documents.</p> <p>La solution qui a \u00e9merg\u00e9 est la suivante : \u00e0 chaque question, on s\u00e9lectionne seulement quelques extraits de la documentation qui sont pertinents, et on les ins\u00e8re dans le contexte (ou prompt) du mod\u00e8le.</p> <p>C'est exactement ce que fait le RAG : il permet, pour chaque question, de choisir les extraits pertinents et de les ajouter au contexte pour que l'IA puisse r\u00e9pondre, m\u00eame sur de tr\u00e8s gros volumes de documents, si vous aimez lire, AWS en parle tr\u00e8s bien ici aussi : (aws.amazon.com)</p> <p>En pratique, mettre en place un syst\u00e8me RAG peut sembler simple au premier abord, mais l'optimiser pour qu'il fonctionne vraiment bien en production demande du temps et de l'expertise. C'est pour cette raison que j'ai cr\u00e9\u00e9 heeya, un chatbot RAG qui peut \u00eatre d\u00e9ploy\u00e9 facilement sur n'importe quel site web, avec toutes les optimisations n\u00e9cessaires d\u00e9j\u00e0 int\u00e9gr\u00e9es.</p>","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/#comment-fonctionne-un-systeme-rag","title":"Comment fonctionne un syst\u00e8me RAG ?","text":"<p>Pour faire simple, la premi\u00e8re \u00e9tape du RAG, c'est de stocker les documents dont on a besoin dans une base de donn\u00e9es vectorielle. Mais comme un mod\u00e8le de langage ne peut pas tout lire d'un coup (\u00e0 cause de la fameuse limite de contexte), on d\u00e9coupe chaque document en petits morceaux, qu'on appelle des chunks. Ensuite, on transforme chaque chunk en un vecteur qui capture sa signification (sa s\u00e9mantique). C'est ce qu'on appelle la vectorisation\u202f: \u00e7a permet de comparer rapidement la question de l'utilisateur avec tous les morceaux de documents, pour trouver ceux qui sont les plus proches.</p> <p>Vectoriser un texte consiste \u00e0 le transformer en un vecteur qui capture sa signification (sa s\u00e9mantique). Cela permet de comparer facilement la similarit\u00e9 entre la question de l'utilisateur et les diff\u00e9rents fragments de documents. Par exemple, si \"un chat blanc\" est repr\u00e9sent\u00e9 par [1, 1], \"un chat noir\" par [1, 0], et \"un chien noir\" par [7, 1], on voit que \"un chat noir\" est plus proche de \"un chat blanc\" que de \"un chien noir\". Ce principe permet d'identifier rapidement les passages les plus pertinents \u00e0 ins\u00e9rer dans le contexte du mod\u00e8le.</p> <p>L'\u00e9tape de vectorisation est cruciale\u202f: c'est elle qui permet de retrouver les bons documents quand on pose une question. Pour \u00e7a, on utilise ce qu'on appelle des mod\u00e8les d'embeddings. Leur r\u00f4le\u202fest de transformer le texte en un vecteur qui capture sa signification, sa \"s\u00e9mantique\". Ces mod\u00e8les sont eux-m\u00eames des IA, entra\u00een\u00e9es sp\u00e9cialement pour cette t\u00e2che, pour en savoir plus.</p> <p>Comme on peut le voir, il y a d\u00e9j\u00e0 un vrai travail en amont\u202f: il faut bien pr\u00e9parer les documents. La performance du RAG d\u00e9pend beaucoup de deux choses\u202f: comment on d\u00e9coupe les documents (la taille et la m\u00e9thode de d\u00e9coupage, ce qu'on appelle le chunking), et la qualit\u00e9 du mod\u00e8le d'embeddings qu'on utilise pour transformer ces morceaux en vecteurs. Rien que sur ces deux points, on a d\u00e9j\u00e0 de quoi am\u00e9liorer les futurs r\u00e9sultats.</p> <p>Une fois la base de donn\u00e9es pr\u00eate, on passe \u00e0 l'\u00e9tape suivante\u202f: quand un utilisateur pose une question, on transforme cette question en vecteur, puis on cherche dans la base les morceaux (chunks) les plus proches, c'est-\u00e0-dire ceux qui ressemblent le plus \u00e0 la question. Cette recherche se fait en comparant la distance entre le vecteur de la question et ceux des chunks d\u00e9j\u00e0 stock\u00e9s. On r\u00e9cup\u00e8re alors quelques chunks (le nombre est d\u00e9fini \u00e0 l'avance), et on les envoie au mod\u00e8le de langage pour qu'il puisse r\u00e9pondre. Au final, le prompt envoy\u00e9 \u00e0 l'IA ressemble \u00e0 \u00e7a\u202f:</p> <pre><code>Voici les chunks pertinents pour la question : \nChunk 1 : &lt;TEXT DU CHUNK 1&gt;\nChunk 2 : &lt;TEXT DU CHUNK 2&gt;\n...\n\nVoici la question de l'utilisateur : Dans quel document je peux trouver des informations concernant le planning du moteur avec la r\u00e9f\u00e9rence X2D2E?\n</code></pre> <p>Dans cette deuxi\u00e8me phase, plusieurs param\u00e8tres peuvent \u00eatre optimis\u00e9s pour am\u00e9liorer le RAG : - La recherche en utilisant la question peut ne pas \u00eatre suffisante.  - Le nombre de chunks r\u00e9cup\u00e9r\u00e9s peut \u00eatre trop faible ou trop grand. - La qualit\u00e9 du mod\u00e8le LLM qui g\u00e9n\u00e8re la r\u00e9ponse peut \u00eatre am\u00e9lior\u00e9e.</p> <p>\u00c0 partir de cette base, on peut commencer \u00e0 exp\u00e9rimenter et \u00e9valuer le RAG. Pour ensuite l'am\u00e9liorer en analysant les erreurs et en changeant les param\u00e8tres. Si vous \u00eates \u00e0 l'\u00e9tape de l'\u00e9valuation, je vous invite \u00e0 lire mon article sur l'analyse d'erreur pour comprendre ce qui coince ici.</p>","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/#a-quoi-ca-sert-vraiment-le-rag-quelles-sont-ses-limites","title":"\u00c0 quoi \u00e7a sert vraiment le RAG ? Quelles sont ses limites ?","text":"<p>Je pense que c'est la question la plus importante. Le RAG ne permet pas de r\u00e9pondre \u00e0 toutes les questions que vous pouvez lui poser, en raison de certaines limites. Dans sa version la plus basique et la plus simple, celle que je d\u00e9taille ici, il ne permet de r\u00e9pondre qu'\u00e0 des questions directes qui ciblent un contenu limit\u00e9.</p> <p>Je m'explique. Si on lui demande de r\u00e9pondre \u00e0 une question tr\u00e8s large, il est possible que le nombre de chunks r\u00e9cup\u00e9r\u00e9s ne soit pas suffisant pour r\u00e9pondre \u00e0 la question.</p> <p>O\u00f9 le RAG fonctionne bien : - Acc\u00e8s \u00e0 des informations sp\u00e9cialis\u00e9es : documents internes ou connaissances m\u00e9tier non disponibles dans les mod\u00e8les de base - Fournir une information pr\u00e9cise : le RAG est particuli\u00e8rement efficace pour r\u00e9pondre \u00e0 des questions cibl\u00e9es en allant chercher directement l'information demand\u00e9e dans les documents fournis</p> <p>O\u00f9 le RAG montre ses limites : - Raisonnement it\u00e9ratif limit\u00e9 : Le RAG ne sait pas raisonner en plusieurs \u00e9tapes pour affiner sa recherche. Il ne v\u00e9rifie pas si les documents r\u00e9cup\u00e9r\u00e9s sont vraiment les plus pertinents ou si l'information est compl\u00e8te. Par exemple, pour une question complexe, il va juste ramener les passages les plus proches s\u00e9mantiquement, sans \"comprendre\" le contexte global comme le ferait un humain. - D\u00e9pendance \u00e0 l'organisation des donn\u00e9es : Si les documents sont mal structur\u00e9s ou mal index\u00e9s, la recherche sera inefficace. Une bonne organisation, des m\u00e9tadonn\u00e9es et une structuration claire sont essentiels. - Qualit\u00e9 et biais des sources : Le RAG ne fait que transmettre ce qu'il trouve. Si les documents sont incomplets, obsol\u00e8tes ou biais\u00e9s, la r\u00e9ponse le sera aussi, pour en savoir plus : elastic.co</p> <p>\u00c0 retenir : - Le RAG ne garantit pas la compl\u00e9tude ni la v\u00e9racit\u00e9 des r\u00e9ponses. Il y a toujours un risque d'hallucination ou d'erreur. - Pour fiabiliser un RAG, il faut : bien organiser les donn\u00e9es, surveiller la qualit\u00e9 des sources, calibrer les param\u00e8tres (chunking, embeddings, etc.), et permettre la citation des sources pour pouvoir v\u00e9rifier les informations si besoin. - L'\u00e9valuation et l'am\u00e9lioration d'un RAG se font surtout en conditions r\u00e9elles, car les probl\u00e8mes apparaissent \u00e0 l'usage.</p>","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/#conclusion-le-rag-est-il-utile-pour-vos-projets-dia","title":"Conclusion : Le RAG est-il utile pour vos projets d'IA ?","text":"<p>Le RAG est vraiment utile, \u00e0 condition de prendre le temps de bien le mettre en place et de l'am\u00e9liorer au fil du temps. Ce n'est pas un outil qu'on impl\u00e9mente en vitesse pour le laisser tourner tout seul\u202f: il faut l'\u00e9valuer, l'ajuster, et corriger ce qui ne va pas pour qu'il soit vraiment efficace.</p> <p>Les limites que j'ai \u00e9voqu\u00e9es ne sont pas simples \u00e0 \u00e9liminer, mais il existe des solutions pour les att\u00e9nuer. On parle souvent d'Agentic RAG par exemple pour am\u00e9liorer certains aspects du RAG. Si vous cherchez un syst\u00e8me qui donne 100\u202f% de bonnes r\u00e9ponses, le RAG (et l'IA en g\u00e9n\u00e9ral) n'est pas faite pour vous. Mais si vous \u00eates pr\u00eat \u00e0 viser 90-95\u202f% de r\u00e9ponses correctes, et \u00e0 investir un peu de temps pour bien l'impl\u00e9menter, alors le RAG peut vraiment devenir votre meilleur alli\u00e9.</p> <p>Si vous voulez en savoir plus sur le RAG, m\u00eame le gouvernement a publi\u00e9 un guide pour faire du RAG : Guide de la g\u00e9n\u00e9ration augment\u00e9e par r\u00e9cup\u00e9ration (RAG).</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p>","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/21/mais-cest-quoi-le-rag-vraiment--d%C3%A9finition-fonctionnement-limites-et-conseils/#a-propos-de-moi","title":"\u00c0 propos de moi","text":"<p>Je suis Anas Rabhi, consultant Data Scientist freelance. J'accompagne les entreprises dans leur strat\u00e9gie et mise en \u0153uvre de solutions d'IA (RAG, Agents, NLP). </p> <p>D\u00e9couvrez mes services sur tensoria.fr ou testez notre solution d'agents IA heeya.fr.</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["RAG","Intelligence Artificielle","Retrieval-Augmented Generation","RAG fonctionnement","RAG limites","RAG optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/","title":"Mon RAG ne marche pas : pourquoi l\u2019analyse d\u2019erreur change tout","text":"","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#introduction","title":"Introduction","text":"<p>J\u2019ai d\u00e9j\u00e0 \u00e9crit un article sur comment am\u00e9liorer le RAG ici, mais le sujet est tellement vaste qu\u2019il y a toujours de nouvelles choses \u00e0 partager. D\u2019autant plus que j'entends souvent des remarques comme\u202f: \"Je ne comprends pas, pourtant j\u2019ai ajout\u00e9 [la techno \u00e0 la mode], mais le r\u00e9sultat n\u2019est pas bon.\"</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#le-rag-nest-pas-magique-et-cest-normal","title":"Le RAG n\u2019est pas magique (et c\u2019est normal)","text":"<p>Le RAG, c\u2019est un peu LE projet \u00e0 la mode depuis le d\u00e9but de l\u2019IA g\u00e9n\u00e9rative. Tout le monde veut son assistant boost\u00e9 \u00e0 l\u2019IA, capable de r\u00e9pondre \u00e0 n\u2019importe quelle question sur ses donn\u00e9es. On trouve des tutos \"RAG en deux lignes\", des outils \"no-code\", et \u00e7a donne l\u2019impression que c\u2019est simple. Mais la r\u00e9alit\u00e9, c\u2019est qu\u2019une fois le projet en place, les tests sont rarement aussi magiques qu\u2019esp\u00e9r\u00e9. L\u2019IA ne r\u00e9pond pas \u00e0 tout, hallucine parfois, ou passe compl\u00e8tement \u00e0 c\u00f4t\u00e9 d\u2019une question basique. Et l\u00e0, grosse frustration.</p> <p>Premi\u00e8re chose \u00e0 retenir\u202f: aucun syst\u00e8me IA ne peut avoir 100\u202f% de bonnes r\u00e9ponses. Il faut surtout se poser la question: quel niveau d\u2019erreur je suis pr\u00eat \u00e0 accepter. La vraie valeur, on l\u2019obtient en comprenant bien le probl\u00e8me qu\u2019on veut r\u00e9soudre, pas en cherchant la perfection.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#quand-on-veut-vraiment-ameliorer-le-rag","title":"Quand on veut vraiment am\u00e9liorer le RAG","text":"<p>Si vous \u00eates convaincu que le RAG est le bon choix, alors il faut investir du temps... mais pas n\u2019importe comment. La premi\u00e8re version est souvent bluffante, mais tr\u00e8s vite, on voit les limites. Les retours utilisateurs sont clairs\u202f: - certaines questions restent sans r\u00e9ponse, - il y a des erreurs \"b\u00eates\", - et l\u2019utilisation devient frustrante.</p> <p>\u00c0 chaque fois, la question c\u2019est\u202f: \"Qu\u2019est-ce qu\u2019on fait maintenant\u202f? On ajoute une nouvelle techno\u202f? On change de mod\u00e8le\u202f?\"</p> <p>Ma r\u00e9ponse\u202f: on analyse. Analyser, ce n\u2019est pas juste regarder les logs ou changer des param\u00e8tres au hasard. C\u2019est d\u00e9cortiquer chaque \u00e9chec pour comprendre d\u2019o\u00f9 il vient. Avant d\u2019ajouter quoi que ce soit, il faut comprendre\u202f: c\u2019est la base du m\u00e9tier, que ce soit en data science, en ML, ou en stats.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#exemples-concrets-danalyse-derreur","title":"Exemples concrets d\u2019analyse d\u2019erreur","text":"<p>Parce que c\u2019est plus parlant, voici deux exemples v\u00e9cus\u202f:</p> <p>1. Quand la recherche vectorielle fait d\u00e9faut Sur un projet, tout semblait marcher... sauf que certaines requ\u00eates avec des mots-cl\u00e9s pr\u00e9cis ne donnaient rien, alors que la r\u00e9ponse \u00e9tait bien dans la base. Apr\u00e8s analyse, on a vu que la recherche vectorielle ne captait pas certains synonymes ou formulations. On a donc ajout\u00e9 une recherche BM25 (bas\u00e9e sur les mots-cl\u00e9s) en plus du vectoriel. R\u00e9sultat\u202f: les questions \"difficiles\" trouvaient enfin des r\u00e9ponses.</p> <p>2. Les attributs m\u00e9tiers oubli\u00e9s Dans un e-commerce, impossible de sortir les produits d\u2019une couleur pr\u00e9cise (\"je veux un t-shirt rouge\"), alors que les donn\u00e9es \u00e9taient l\u00e0. L\u2019analyse a montr\u00e9 que la s\u00e9mantique de la couleur n\u2019\u00e9tait pas bien captur\u00e9e dans les vecteurs d'embeddings. On a simplement ajout\u00e9 un filtrage par m\u00e9tadonn\u00e9e avant de passer \u00e0 l\u2019IA : probl\u00e8me r\u00e9gl\u00e9.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#comment-mener-lanalyse-derreur","title":"Comment mener l\u2019analyse d\u2019erreur\u202f?","text":"<p>Voil\u00e0 comment je m\u2019y prends, et franchement, \u00e7a marche dans 90\u202f% des cas\u202f:</p> <p>1 : Prendre un \u00e9chantillon d\u2019exemples o\u00f9 le RAG se plante. 2 : Pour chaque cas, se demander\u202f:    - Est-ce que le retrieval trouve quelque chose ou non\u202f?    - Est-ce que la g\u00e9n\u00e9ration hallucine\u202f?    - Est-ce que l\u2019info existe vraiment dans la base\u202f?    - Est-ce un probl\u00e8me de format, de m\u00e9tadonn\u00e9e, de formulation\u202f? 3 : Cat\u00e9goriser les erreurs (retrieval, ranking, hallucination, data...). 4 : Tester des corrections simples... avant de tout changer.</p> <p>Et surtout\u202f: noter ce qui revient le plus souvent, pour prioriser les vraies am\u00e9liorations.</p> <p>Pour commencer, toutes les analyses d'erreur doivent se faire \u00e0 la main. C'est indispensable pour vraiment comprendre d'o\u00f9 viennent les probl\u00e8mes et comment fonctionnent les diff\u00e9rents frameworks RAG. Mais soyons honn\u00eates : \u00e0 un moment, quand le volume de requ\u00eates augmente, \u00e7a devient vite ing\u00e9rable. C'est l\u00e0 que de bons outils deviennent indispensables pour garder une vision claire de ce qui se passe \u00e0 chaque \u00e9tape.</p> <p>C'est pr\u00e9cis\u00e9ment pour r\u00e9soudre ces d\u00e9fis que j'ai d\u00e9velopp\u00e9 heeya, une solution RAG chatbot qui int\u00e8gre nativement l'observabilit\u00e9 et l'analyse d'erreur, permettant de d\u00e9ployer rapidement un syst\u00e8me RAG optimis\u00e9 sur votre site web sans avoir \u00e0 configurer manuellement tous ces outils de monitoring.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#quels-outils-choisir","title":"Quels outils choisir ?","text":"<p>LangFuse est probablement l\u2019un des plus pratiques (et open-source) pour tracer tout le pipeline RAG\u202f: on visualise chaque \u00e9tape, de la requ\u00eate originale aux chunks r\u00e9cup\u00e9r\u00e9s, le prompt final envoy\u00e9 au LLM, et la r\u00e9ponse g\u00e9n\u00e9r\u00e9e. Id\u00e9al pour rep\u00e9rer pr\u00e9cis\u00e9ment o\u00f9 \u00e7a d\u00e9raille.</p> <p>LangSmith fait la m\u00eame chose que Langfuse, avec une interface diff\u00e9rente. L\u2019avantage\u202f: si vous utilisez d\u00e9j\u00e0 LangChain, l\u2019int\u00e9gration est naturelle.</p> <p>Pour aller plus loin dans le suivi, Weights &amp; Biases permet de tracker les m\u00e9triques de performance dans le temps et de comparer diff\u00e9rentes versions du syst\u00e8me. Pratique pour v\u00e9rifier si une \"am\u00e9lioration\" n\u2019a pas cass\u00e9 autre chose ailleurs.</p> <p>Et puis il y a les solutions maison,\u202fqui permettent de garder le contr\u00f4le sur tout : un logging JSON bien structur\u00e9, qui enregistre les prompts, les chunks r\u00e9cup\u00e9r\u00e9s et les r\u00e9ponses du LLM.</p> <p>L\u2019id\u00e9e, c\u2019est de ne pas se perdre dans la surench\u00e8re de dashboards\u202f: il faut juste assez de visibilit\u00e9 pour comprendre rapidement o\u00f9 chercher quand quelque chose cloche.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#ce-quil-faut-retenir","title":"Ce qu\u2019il faut retenir","text":"<p>Le RAG, ce n\u2019est ni magique, ni parfait. Ce qui fait la diff\u00e9rence, ce n\u2019est pas la derni\u00e8re techno ajout\u00e9e, mais la capacit\u00e9 \u00e0 comprendre pourquoi \u00e7a rate et \u00e0 it\u00e9rer intelligemment. L\u2019analyse d\u2019erreur, c\u2019est la base\u202f: avant d\u2019empiler les couches de complexit\u00e9, prenez le temps de regarder, d\u2019\u00e9couter les utilisateurs, et de corriger \u00e0 la source.</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/04/mon-rag-ne-marche-pas--pourquoi-lanalyse-derreur-change-tout/#a-propos-de-moi","title":"\u00c0 propos de moi","text":"<p>Je suis Anas Rabhi, consultant Data Scientist freelance. J'accompagne les entreprises dans leur strat\u00e9gie et mise en \u0153uvre de solutions d'IA (RAG, Agents, NLP). </p> <p>D\u00e9couvrez mes services sur tensoria.fr ou testez notre solution d'agents IA heeya.fr.</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/","title":"RAG une porte d'entr\u00e9e par sa simplicit\u00e9 d'implementation","text":"","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#introduction-demystifier-le-rag-en-entreprise","title":"Introduction : D\u00e9mystifier le RAG en entreprise","text":"","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#le-rag-nest-pas-une-solution-magique","title":"Le RAG n'est pas une solution magique","text":"<p>Le RAG (Retrieval-Augmented Generation), c'est un peu LE projet \u00e0 la mode depuis le d\u00e9but de l'IA g\u00e9n\u00e9rative. Tout le monde veut son assistant intelligent boost\u00e9 \u00e0 l'IA, capable de r\u00e9pondre \u00e0 n'importe quelle question sur ses donn\u00e9es internes. On trouve des tutos \"RAG en deux lignes\", des frameworks no-code, et \u00e7a donne l'impression que c'est simple. Si vous voulez comprendre en profondeur ce qu'est vraiment le RAG et comment il fonctionne, je vous invite \u00e0 lire mon article d\u00e9di\u00e9.</p> <p>Mais la r\u00e9alit\u00e9 terrain ? Une fois le projet en place, les tests sont rarement aussi magiques qu'esp\u00e9r\u00e9. L'intelligence artificielle ne r\u00e9pond pas \u00e0 tout, hallucine parfois, ou passe compl\u00e8tement \u00e0 c\u00f4t\u00e9 d'une question basique que m\u00eame un stagiaire aurait comprise. Et l\u00e0, grosse frustration chez les \u00e9quipes m\u00e9tier.</p> <p>Premi\u00e8re chose \u00e0 retenir : aucun syst\u00e8me d'IA g\u00e9n\u00e9rative ne peut garantir 100 % de bonnes r\u00e9ponses. M\u00eame les meilleurs mod\u00e8les de langage (LLM) comme GPT-5.2, Claude ou Mistral ont leurs limites. Pour mieux comprendre les fondamentaux de l'IA g\u00e9n\u00e9rative, je vous recommande cet article.</p> <p>La vraie question \u00e0 se poser : quel taux d'erreur suis-je pr\u00eat \u00e0 accepter pour mon cas d'usage ? Un chatbot de support client n'a pas les m\u00eames exigences qu'un syst\u00e8me d'aide \u00e0 la d\u00e9cision m\u00e9dicale. La valeur d'un syst\u00e8me RAG, on l'obtient en comprenant bien le probl\u00e8me m\u00e9tier qu'on veut r\u00e9soudre, pas en cherchant la perfection absolue.</p> <p>C'est d'ailleurs pour r\u00e9pondre \u00e0 ces d\u00e9fis que j'ai d\u00e9velopp\u00e9 heeya, une solution RAG chatbot qui peut \u00eatre d\u00e9ploy\u00e9e facilement sur n'importe quel site web. L'id\u00e9e \u00e9tait de cr\u00e9er un outil qui int\u00e8gre d\u00e8s le d\u00e9part les bonnes pratiques d'analyse d'erreur et d'optimisation, tout en restant simple \u00e0 impl\u00e9menter pour les \u00e9quipes qui n'ont pas forc\u00e9ment une expertise technique approfondie en RAG.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#pourquoi-votre-premier-rag-decoit-et-cest-previsible","title":"Pourquoi votre premier RAG d\u00e9\u00e7oit (et c'est pr\u00e9visible)","text":"<p>Les retours que j'entends r\u00e9guli\u00e8rement apr\u00e8s quelques semaines d'utilisation :</p> <ul> <li>\"Certaines questions \u00e9videntes restent sans r\u00e9ponse\"</li> <li>\"L'IA invente des informations qui n'existent pas dans nos documents\"</li> <li>\"Les utilisateurs sont frustr\u00e9s et retournent \u00e0 la recherche manuelle\"</li> <li>\"On a l'impression que \u00e7a marche... mais pas assez bien\"</li> </ul> <p>C'est normal. Un syst\u00e8me RAG basique (embeddings + recherche vectorielle + LLM) est un excellent point de d\u00e9part, mais il a ses angles morts. Le pi\u00e8ge, c'est de croire qu'ajouter plus de donn\u00e9es ou changer de mod\u00e8le va tout r\u00e9gler magiquement. Si vous rencontrez des probl\u00e8mes similaires, cet article sur pourquoi le RAG ne fonctionne pas vous donnera des pistes suppl\u00e9mentaires.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#quand-on-veut-vraiment-ameliorer-son-systeme-rag","title":"Quand on veut vraiment am\u00e9liorer son syst\u00e8me RAG","text":"<p>Si vous \u00eates convaincu que le RAG est le bon choix pour votre projet (et pas un simple fine-tuning ou une recherche classique), alors il faut investir du temps... mais pas n'importe comment.</p> <p>La premi\u00e8re version POC (Proof of Concept) est souvent bluffante sur les cas simples. Mais tr\u00e8s vite, on voit les limites appara\u00eetre :</p> <ul> <li>Les requ\u00eates complexes multi-crit\u00e8res \u00e9chouent</li> <li>Les synonymes et formulations alternatives ne sont pas g\u00e9r\u00e9s</li> <li>Les m\u00e9tadonn\u00e9es m\u00e9tier (dates, cat\u00e9gories, statuts) sont ignor\u00e9es</li> <li>La fra\u00eecheur des donn\u00e9es n'est pas garantie</li> </ul> <p>\u00c0 chaque probl\u00e8me, la m\u00eame question revient : \"Qu'est-ce qu'on fait maintenant ? On ajoute une nouvelle techno ? On change de mod\u00e8le ? On passe \u00e0 un embedding plus performant ?\"</p> <p>Ma r\u00e9ponse sera toujours la m\u00eame : on analyse d'abord.</p> <p>Analyser, ce n'est pas juste regarder les logs ou tweaker des param\u00e8tres au hasard dans l'espoir que \u00e7a passe mieux. C'est d\u00e9cortiquer m\u00e9thodiquement chaque \u00e9chec pour comprendre sa cause racine. Est-ce un probl\u00e8me de retrieval ? De ranking ? D'hallucination ? De qualit\u00e9 de donn\u00e9es ?</p> <p>Avant d'ajouter quoi que ce soit au syst\u00e8me, il faut comprendre pr\u00e9cis\u00e9ment o\u00f9 \u00e7a coince. C'est la base du m\u00e9tier, que ce soit en data science, en machine learning, ou en statistiques. Et pourtant, c'est l'\u00e9tape qu'on saute le plus souvent sous pression. J'ai d\u00e9crit cette approche d'am\u00e9lioration de l'IA par l'analyse d'erreur dans un article pr\u00e9c\u00e9dent.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#exemples-concrets-danalyse-derreur-dans-des-projets-rag","title":"Exemples concrets d'analyse d'erreur dans des projets RAG","text":"<p>Parce que c'est toujours plus parlant avec du concret, voici deux cas r\u00e9els que j'ai rencontr\u00e9s.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#cas-n1-quand-la-recherche-vectorielle-pure-montre-ses-limites","title":"Cas n\u00b01 : Quand la recherche vectorielle pure montre ses limites","text":"<p>Contexte : Documentation technique interne d'une entreprise avec 5000+ documents. Le syst\u00e8me RAG fonctionnait correctement sur les questions g\u00e9n\u00e9rales, mais \u00e9chouait syst\u00e9matiquement sur des requ\u00eates avec des termes techniques pr\u00e9cis ou des acronymes m\u00e9tier.</p> <p>Sympt\u00f4me : \"Quelle est la proc\u00e9dure pour la norme ISO-27001 ?\" \u2192 Aucun r\u00e9sultat pertinent, alors que plusieurs documents parlaient explicitement de cette norme.</p> <p>Analyse : En regardant les embeddings, on s'est rendu compte que la recherche vectorielle captait bien le sens s\u00e9mantique global, mais ratait les correspondances exactes de mots-cl\u00e9s. Les acronymes et noms propres \u00e9taient dilu\u00e9s dans l'espace vectoriel.</p> <p>Solution : Mise en place d'une recherche hybride combinant :</p> <ul> <li>Recherche vectorielle (semantic search) pour le sens g\u00e9n\u00e9ral</li> <li>Recherche BM25 (keyword-based) pour les correspondances exactes</li> <li>Fusion des r\u00e9sultats avec un algorithme de ranking (Reciprocal Rank Fusion)</li> </ul> <p>R\u00e9sultat : +35% de taux de r\u00e9ponses pertinentes sur les requ\u00eates techniques. Les questions \"difficiles\" avec des termes pr\u00e9cis trouvaient enfin leurs r\u00e9ponses.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#cas-n2-les-attributs-metiers-structures-oublies-par-le-vectoriel","title":"Cas n\u00b02 : Les attributs m\u00e9tiers structur\u00e9s oubli\u00e9s par le vectoriel","text":"<p>Contexte : Catalogue e-commerce avec 50 000 produits. Le RAG devait permettre aux clients de poser des questions en langage naturel sur les produits.</p> <p>Sympt\u00f4me : \"Je veux un t-shirt rouge en taille M\" \u2192 Le syst\u00e8me renvoyait des t-shirts de toutes les couleurs, ou parfois aucun r\u00e9sultat.</p> <p>Analyse : Les embeddings capturaient bien le concept de \"t-shirt\", mais la granularit\u00e9 des attributs m\u00e9tier (couleur exacte, taille) se perdait dans la repr\u00e9sentation vectorielle. Le mod\u00e8le comprenait \"rouge\" comme une notion vague, pas comme un filtre pr\u00e9cis.</p> <p>Solution : Ajout d'un syst\u00e8me de filtrage par m\u00e9tadonn\u00e9es en amont :</p> <ul> <li>Extraction des attributs structur\u00e9s de la requ\u00eate (couleur, taille, prix, etc.)</li> <li>Application de filtres SQL sur la base de donn\u00e9es produits</li> <li>Recherche vectorielle uniquement sur l'ensemble pr\u00e9-filtr\u00e9</li> </ul> <p>R\u00e9sultat : Le taux de satisfaction utilisateur est pass\u00e9 de 62% \u00e0 89%. Les requ\u00eates avec crit\u00e8res pr\u00e9cis fonctionnaient enfin correctement.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#methodologie-comment-mener-une-analyse-derreur-efficace-sur-votre-rag","title":"M\u00e9thodologie : Comment mener une analyse d'erreur efficace sur votre RAG","text":"<p>Voil\u00e0 ma m\u00e9thode \u00e9prouv\u00e9e, qui fonctionne dans la majorit\u00e9 des cas :</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#etape-1-constituer-un-echantillon-representatif-dechecs","title":"\u00c9tape 1 : Constituer un \u00e9chantillon repr\u00e9sentatif d'\u00e9checs","text":"<p>Prenez 20 \u00e0 50 exemples de requ\u00eates o\u00f9 le syst\u00e8me RAG se plante. Variez les types d'erreurs :</p> <ul> <li>R\u00e9ponses compl\u00e8tement hors-sujet</li> <li>Aucune r\u00e9ponse fournie</li> <li>Hallucinations (inventions)</li> <li>R\u00e9ponses partielles ou impr\u00e9cises</li> </ul> <p>\ud83d\udca1 Astuce : Demandez aux vrais utilisateurs leurs pires exp\u00e9riences. Les retours terrain sont plus riches que les tests synth\u00e9tiques.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#etape-2-decomposer-chaque-echec-etape-par-etape","title":"\u00c9tape 2 : D\u00e9composer chaque \u00e9chec \u00e9tape par \u00e9tape","text":"<p>Pour chaque cas probl\u00e9matique, posez-vous ces questions dans l'ordre :</p> <p>Sur le retrieval (r\u00e9cup\u00e9ration de documents) :</p> <ul> <li>Est-ce que le syst\u00e8me trouve des chunks pertinents ?</li> <li>Combien de documents sont r\u00e9cup\u00e9r\u00e9s ? (top-k)</li> <li>La bonne information est-elle dans les r\u00e9sultats, mais mal class\u00e9e ?</li> <li>Y a-t-il un probl\u00e8me de chunking (d\u00e9coupage trop fin ou trop large) ?</li> </ul> <p>Sur les donn\u00e9es :</p> <ul> <li>L'information existe-t-elle vraiment dans la base ?</li> <li>Est-elle \u00e0 jour et correcte ?</li> <li>Le format est-il exploitable (PDF scann\u00e9s, tableaux complexes...) ?</li> <li>Les m\u00e9tadonn\u00e9es sont-elles renseign\u00e9es ?</li> </ul> <p>Sur la g\u00e9n\u00e9ration (LLM) :</p> <ul> <li>Le prompt contient-il le contexte n\u00e9cessaire ?</li> <li>Y a-t-il des hallucinations manifestes ?</li> <li>Le mod\u00e8le comprend-il bien la question ?</li> <li>La r\u00e9ponse est-elle dans le bon format ?</li> </ul>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#etape-3-categoriser-les-erreurs-par-type","title":"\u00c9tape 3 : Cat\u00e9goriser les erreurs par type","text":"<p>Cr\u00e9ez une taxonomie simple :</p> <ul> <li>Erreurs de retrieval : Mauvais chunks r\u00e9cup\u00e9r\u00e9s (40% des cas en moyenne)</li> <li>Erreurs de ranking : Bon chunk trouv\u00e9 mais mal class\u00e9 (25%)</li> <li>Erreurs de g\u00e9n\u00e9ration : Hallucinations, mauvaise interpr\u00e9tation (20%)</li> <li>Erreurs de donn\u00e9es : Info manquante, obsol\u00e8te ou mal format\u00e9e (15%)</li> </ul> <p>Ces pourcentages varient \u00e9videmment selon les projets, mais cette r\u00e9partition permet de prioriser les efforts d'am\u00e9lioration.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#etape-4-tester-des-corrections-simples-avant-de-tout-refaire","title":"\u00c9tape 4 : Tester des corrections simples avant de tout refaire","text":"<p>Avant de r\u00e9\u00e9crire tout le pipeline :</p> <ul> <li>Ajustez les param\u00e8tres de recherche (top-k, seuils de similarit\u00e9)</li> <li>Testez diff\u00e9rents prompts pour la g\u00e9n\u00e9ration</li> <li>Am\u00e9liorez le chunking (taille, overlap, respect des structures)</li> <li>Nettoyez les donn\u00e9es sources</li> </ul> <p>L'id\u00e9e : it\u00e9rer rapidement sur des changements mesurables plut\u00f4t que de repartir de z\u00e9ro.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#lanalyse-manuelle-indispensable-au-debut","title":"L'analyse manuelle : indispensable au d\u00e9but","text":"<p>Pour commencer, toutes les analyses d'erreur doivent se faire \u00e0 la main. C'est fastidieux, mais c'est indispensable pour vraiment comprendre :</p> <ul> <li>Comment votre syst\u00e8me RAG se comporte r\u00e9ellement</li> <li>Quels sont les patterns d'erreur r\u00e9currents</li> <li>Comment les diff\u00e9rents frameworks (LangChain, LlamaIndex, Haystack...) g\u00e8rent les cas limites</li> </ul> <p>Soyons honn\u00eates : \u00e0 un moment, quand le volume de requ\u00eates augmente (plusieurs centaines par jour), \u00e7a devient vite ing\u00e9rable. C'est l\u00e0 que de bons outils d'observabilit\u00e9 deviennent indispensables pour garder une vision claire de ce qui se passe \u00e0 chaque \u00e9tape du pipeline.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#quels-outils-de-monitoring-pour-votre-systeme-rag","title":"Quels outils de monitoring pour votre syst\u00e8me RAG ?","text":"<p>Si vous cherchez une solution compl\u00e8te qui int\u00e8gre d\u00e9j\u00e0 le monitoring et l'optimisation, heeya propose un syst\u00e8me RAG avec observabilit\u00e9 int\u00e9gr\u00e9e, permettant de suivre les performances et d'identifier rapidement les probl\u00e8mes sans avoir \u00e0 configurer des outils externes. Pour ceux qui pr\u00e9f\u00e8rent construire leur propre stack, voici les options principales :</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#langfuse-lobservabilite-open-source-complete","title":"LangFuse : L'observabilit\u00e9 open-source compl\u00e8te","text":"<p>LangFuse est probablement l'un des plus pratiques (et open-source) pour tracer tout le pipeline RAG. On visualise :</p> <ul> <li>La requ\u00eate utilisateur originale</li> <li>Les chunks r\u00e9cup\u00e9r\u00e9s avec leurs scores de similarit\u00e9</li> <li>Le prompt final envoy\u00e9 au LLM (avec le contexte inject\u00e9)</li> <li>La r\u00e9ponse g\u00e9n\u00e9r\u00e9e</li> <li>Les latences \u00e0 chaque \u00e9tape</li> <li>Les co\u00fbts d'API</li> </ul> <p>Cas d'usage id\u00e9al : Rep\u00e9rer pr\u00e9cis\u00e9ment o\u00f9 \u00e7a d\u00e9raille dans la cha\u00eene. Par exemple, voir que les bons chunks sont r\u00e9cup\u00e9r\u00e9s mais que le prompt mal formul\u00e9 induit le LLM en erreur.</p> <p>\ud83d\udd17 Int\u00e9gration : Compatible avec LangChain, LlamaIndex, et possibilit\u00e9 d'int\u00e9grer via SDK custom.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#langsmith-lalternative-de-langchain","title":"LangSmith : L'alternative de LangChain","text":"<p>LangSmith fait sensiblement la m\u00eame chose que LangFuse, avec une interface diff\u00e9rente et quelques fonctionnalit\u00e9s suppl\u00e9mentaires comme les datasets de test.</p> <p>Avantage principal : Si vous utilisez d\u00e9j\u00e0 LangChain en production, l'int\u00e9gration est native et quasi automatique. Pas besoin de wrapper suppl\u00e9mentaire.</p> <p>Inconv\u00e9nient : Solution propri\u00e9taire et payante d\u00e8s que vous d\u00e9passez les quotas gratuits.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#weights-biases-wb-pour-le-suivi-de-performance-long-terme","title":"Weights &amp; Biases (W&amp;B) : Pour le suivi de performance long terme","text":"<p>Weights &amp; Biases (W&amp;B) n'est pas sp\u00e9cifique au RAG, mais il excelle pour :</p> <ul> <li>Tracker les m\u00e9triques de performance dans le temps (accuracy, latence, co\u00fbts)</li> <li>Comparer diff\u00e9rentes versions du syst\u00e8me (A/B testing)</li> <li>D\u00e9tecter les r\u00e9gressions de performance</li> </ul> <p>Cas d'usage id\u00e9al : V\u00e9rifier qu'une \"am\u00e9lioration\" sur un type de requ\u00eate n'a pas cass\u00e9 autre chose ailleurs. Suivre l'\u00e9volution de vos KPIs sur plusieurs semaines.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#solutions-maison-garder-le-controle-total","title":"Solutions maison : Garder le contr\u00f4le total","text":"<p>Pour ceux qui veulent garder le contr\u00f4le sur tout (et c'est souvent n\u00e9cessaire en entreprise pour des raisons de confidentialit\u00e9) :</p> <p>Option 1 : Logging structur\u00e9 en JSON</p> <pre><code>{\n  \"query\": \"question utilisateur\",\n  \"retrieved_chunks\": [...],\n  \"prompt\": \"prompt complet\",\n  \"llm_response\": \"r\u00e9ponse g\u00e9n\u00e9r\u00e9e\",\n  \"metadata\": {...}\n}\n</code></pre> <p>Option 2 : Base de donn\u00e9es d\u00e9di\u00e9e Cr\u00e9ez une table SQL pour stocker chaque interaction avec tous ses d\u00e9tails. Avantage : requ\u00eatable facilement pour faire des analyses ad-hoc.</p> <p>Option 3 : Stack ELK (Elasticsearch, Logstash, Kibana) Pour ceux qui ont d\u00e9j\u00e0 cette infrastructure, c'est parfait pour indexer et visualiser les logs RAG.</p> <p>L'id\u00e9e : ne pas se perdre dans la surench\u00e8re de dashboards. Il faut juste assez de visibilit\u00e9 pour comprendre rapidement o\u00f9 chercher quand quelque chose cloche.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#metriques-cles-a-suivre-pour-votre-systeme-rag","title":"M\u00e9triques cl\u00e9s \u00e0 suivre pour votre syst\u00e8me RAG","text":"<p>Au-del\u00e0 des outils, voici les KPIs essentiels \u00e0 monitorer :</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#metriques-de-retrieval","title":"M\u00e9triques de retrieval","text":"<ul> <li>Recall@k : Le bon document est-il dans les k premiers r\u00e9sultats ?</li> <li>MRR (Mean Reciprocal Rank) : \u00c0 quelle position appara\u00eet le bon r\u00e9sultat en moyenne ?</li> <li>Hit Rate : Proportion de requ\u00eates o\u00f9 au moins un chunk pertinent est r\u00e9cup\u00e9r\u00e9</li> </ul>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#metriques-de-generation","title":"M\u00e9triques de g\u00e9n\u00e9ration","text":"<ul> <li>Faithfulness : La r\u00e9ponse est-elle fid\u00e8le au contexte fourni ? (d\u00e9tecte les hallucinations)</li> <li>Answer Relevancy : La r\u00e9ponse r\u00e9pond-elle vraiment \u00e0 la question ?</li> <li>Context Precision : Les chunks fournis au LLM sont-ils tous utiles ?</li> </ul>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#metriques-business","title":"M\u00e9triques business","text":"<ul> <li>Taux de satisfaction utilisateur : Retours directs (\ud83d\udc4d\ud83d\udc4e)</li> <li>Taux de reformulation : L'utilisateur redemande-t-il juste apr\u00e8s ?</li> <li>Taux d'abandon : Combien passent \u00e0 un autre canal (email, ticket...) ?</li> </ul>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#faq-questions-frequentes-sur-lamelioration-des-systemes-rag","title":"FAQ : Questions fr\u00e9quentes sur l'am\u00e9lioration des syst\u00e8mes RAG","text":"<p>Q : Combien de temps faut-il pour analyser les erreurs d'un RAG ? R : Comptez 1 \u00e0 2 jours pour une premi\u00e8re analyse sur 50 cas. Ensuite, instaurez une routine hebdomadaire de 2-3h pour suivre les nouveaux probl\u00e8mes.</p> <p>Q : Faut-il forc\u00e9ment utiliser des outils payants ? R : Non. LangFuse est open-source et tr\u00e8s complet. Pour d\u00e9buter, m\u00eame un simple fichier Excel avec vos cas d'\u00e9chec peut suffire.</p> <p>Q : Quelle est la diff\u00e9rence entre RAG et fine-tuning ? R : Le RAG injecte des connaissances externes au moment de la requ\u00eate. Le fine-tuning modifie le mod\u00e8le lui-m\u00eame. Le RAG est pr\u00e9f\u00e9rable pour des connaissances qui changent souvent.</p> <p>Q : Mon RAG hallucine beaucoup, que faire ? R : V\u00e9rifiez d'abord votre prompt (ajoutez \"r\u00e9ponds uniquement bas\u00e9 sur le contexte fourni\"). Puis analysez si les bons chunks sont r\u00e9cup\u00e9r\u00e9s. Enfin, testez avec un mod\u00e8le plus r\u00e9cent.</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#ce-quil-faut-retenir-les-cles-dun-rag-qui-fonctionne","title":"Ce qu'il faut retenir : Les cl\u00e9s d'un RAG qui fonctionne","text":"<p>Le RAG n'est ni magique, ni parfait. C'est un outil puissant, mais qui demande de l'attention et de la rigueur pour bien fonctionner en production.</p> <p>Ce qui fait vraiment la diff\u00e9rence, ce n'est pas la derni\u00e8re techno \u00e0 la mode ou le mod\u00e8le le plus gros. C'est la capacit\u00e9 \u00e0 comprendre pourquoi \u00e7a rate et \u00e0 it\u00e9rer intelligemment sur les vrais probl\u00e8mes.</p> <p>L'analyse d'erreur m\u00e9thodique est LA comp\u00e9tence \u00e0 ma\u00eetriser :</p> <ul> <li>Prenez le temps de comprendre avant d'agir</li> <li>\u00c9coutez vos utilisateurs (leurs frustrations sont des signaux pr\u00e9cieux)</li> <li>Corrigez \u00e0 la source plut\u00f4t que d'empiler des layers de complexit\u00e9</li> <li>Mesurez l'impact de chaque changement</li> </ul> <p>Un syst\u00e8me RAG efficace, c'est 20% de technologie et 80% de compr\u00e9hension du probl\u00e8me m\u00e9tier. Commencez simple, analysez rigoureusement, et am\u00e9liorez progressivement.</p> <p>Si vous cherchez \u00e0 \u00e9viter de r\u00e9inventer la roue et \u00e0 b\u00e9n\u00e9ficier d'une solution RAG d\u00e9j\u00e0 optimis\u00e9e avec monitoring int\u00e9gr\u00e9, heeya est con\u00e7ue pour \u00eatre d\u00e9ploy\u00e9e rapidement sur votre site web tout en int\u00e9grant les bonnes pratiques d'analyse et d'optimisation dont nous avons parl\u00e9 dans cet article.</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p>","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/12/02/rag-une-porte-dentr%C3%A9e-par-sa-simplicit%C3%A9-dimplementation/#a-propos-de-moi","title":"\u00c0 propos de moi","text":"<p>Je suis Anas Rabhi, consultant Data Scientist freelance. J'accompagne les entreprises dans leur strat\u00e9gie et mise en \u0153uvre de solutions d'IA (RAG, Agents, NLP). </p> <p>D\u00e9couvrez mes services sur tensoria.fr ou testez notre solution d'agents IA heeya.fr.</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["RAG","Intelligence Artificielle","Conseils Pratiques","Optimisation"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/","title":"Comment r\u00e9ussir \u00e0 rester \u00e0 jour en IA g\u00e9n\u00e9rative ?","text":"","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#introduction","title":"Introduction","text":"<p>Aborder l'IA en g\u00e9n\u00e9ral est d\u00e9j\u00e0 assez complexe. D\u2019un point de vue externe, on n\u2019a pas id\u00e9e \u00e0 quel point le domaine est vaste. Et une fois qu\u2019on est dedans, c\u2019est infini. On peut passer toute une carri\u00e8re sur une petite sp\u00e9cialit\u00e9 de l\u2019IA, comme par exemple les s\u00e9ries temporelles, ou encore travailler sur le traitement du langage (NLP), sans forc\u00e9ment savoir ce qui se passe dans les autres cat\u00e9gories d\u2019IA comme les syst\u00e8mes de recommandation, la vision par ordinateur, etc.</p> <p>Vous vous dites que c\u2019est d\u00e9j\u00e0 compliqu\u00e9, mais imaginez maintenant qu\u2019il y ait des nouveaut\u00e9s toutes les semaines. C\u2019est exactement ce qu\u2019on vit depuis 2-3 ans avec l\u2019IA g\u00e9n\u00e9rative, une cat\u00e9gorie de l\u2019IA qui \u00e9volue extr\u00eamement vite, avec de nombreux acteurs : OpenAI, Anthropic, Deepseek, Alibaba, XAi, HuggingFace, Pleias, Mistral... Bref, je pense que cela pourrait faire l\u2019objet d\u2019un article de blog \u00e0 part enti\u00e8re.</p> <p>Suivre tout cela est donc tr\u00e8s complexe : des mod\u00e8les sortent presque tous les jours, de nouvelles techniques, des produits... 24 heures ne suffisent clairement pas pour tout voir.</p> <p>Comme c\u2019est mon m\u00e9tier et que je suis passionn\u00e9 par le sujet, j\u2019ai pris l\u2019habitude de suivre tout \u00e7a au quotidien, en essayant de ne pas trop en faire et en gardant du plaisir. Je me suis donc dit qu'il serait int\u00e9ressant d\u2019\u00e9crire un article sur les sources qui me permettent de rester \u00e0 jour.</p>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#prerequis","title":"Pr\u00e9requis","text":"<p>Il y a tout de m\u00eame quelques pr\u00e9requis n\u00e9cessaires pour pouvoir suivre ce qui se passe en IA g\u00e9n\u00e9rative, en particulier sur le plan technique.</p> <p>Si vous souhaitez comprendre l'IA en g\u00e9n\u00e9ral et l'IA g\u00e9n\u00e9rative en particulier et avoir une vision globale pour commencer, je vous conseille de lire mes articles sur comprendre l'IA et comprendre l'IA g\u00e9n\u00e9rative.</p> <p>Si vous voulez aller plus loin et que vous ne savez pas par o\u00f9 commencer, voici quelques \u00e9l\u00e9ments :</p> <ul> <li>Pour avoir des notions g\u00e9n\u00e9rales en IA, un cours d'introduction peut \u00eatre pertinent : Introduction to AI</li> <li>Pour avoir des connaissances en NLP (natural language processing) faciliteront aussi la t\u00e2che. Si cela vous int\u00e9resse, je vous conseille ce cours sur le NLP</li> <li>Les cours d'Andrew Ng sur Youtube sont \u00e9galement une tr\u00e8s bonne introduction \u00e0 l'IA.</li> </ul>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#les-differentes-sources-dinformation","title":"Les diff\u00e9rentes sources d'information","text":"<p>Le contenu sera compl\u00e9t\u00e9 au fur et \u00e0 mesure, et sera aussi s\u00fbrement amen\u00e9 \u00e0 \u00e9voluer au fil du temps, en fonction de ce qui me revient en t\u00eate. Il y a des sources que j'utilise au quotidien, et d'autres que je consulte de temps en temps, donc la premi\u00e8re version de cet article ne sera pas compl\u00e8te.</p>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#les-reseaux-sociaux","title":"Les r\u00e9seaux sociaux","text":"<p>Rien d'\u00e9tonnant, les r\u00e9seaux sociaux occupent une partie tr\u00e8s importante de notre temps quotidien. S'il y a une source assez simple d'acc\u00e8s pour des informations d'actualit\u00e9 qui reprennent les derni\u00e8res nouveaut\u00e9s, et aussi pour avoir une premi\u00e8re piste de o\u00f9 chercher, c'est bien les r\u00e9seaux sociaux. </p> <p>Je dirais que les deux r\u00e9seaux sociaux que j'essaie de consulter un peu tous les jours et dont le contenu est tr\u00e8s qualitatif sont : Twitter (X) et Reddit. Sur ces r\u00e9seaux, on peut trouver pas mal de chercheurs qui travaillent \u00e0 Stanford, Google, OpenAI, Anthropic, etc. et qui partagent leurs travaux, d\u00e9couvertes, ainsi que leurs id\u00e9es presque tous les jours. Ils r\u00e9pondent souvent aux questions pour expliquer et interagissent beaucoup pour confronter les id\u00e9es. Rien qu'avec Twitter, on peut rester \u00e0 jour sur tout ce qui se fait dans l'IA g\u00e9n\u00e9rative (ou presque).</p> <p>Je passe aussi un peu de temps sur Linkedin, c'est un r\u00e9seau qui est utile pour l'actualit\u00e9 mais avec un peu moins de profondeur dans le contenu.</p> <p>Twitter (X) :</p> <ul> <li>@HamelHusain : ML Engineer qui a travaill\u00e9 \u00e0 Github et qui partage pas mal de choses sur l'IA g\u00e9n\u00e9rative, donne des cours et des conf\u00e9rences.</li> <li>@Dorialexander : CEO de pleiasfr, entreprise fran\u00e7aise peu connue mais dont les travaux de recherche b\u00e9n\u00e9ficient beaucoup \u00e0 la communaut\u00e9 open-source.</li> <li>@jeremyphoward : Chercheur, CEO, prof... Une personne avec une \u00e9norme contribution \u00e0 l'IA depuis des ann\u00e9es. Si vous connaissez la librairie FastAI qui date de quelques ann\u00e9es, c'est lui le cr\u00e9ateur.</li> <li>@karpathy : Quand je pense \u00e0 ChatGPT, je pense \u00e0 Karpathy. Il a travaill\u00e9 sur les premi\u00e8res versions de ChatGPT, il a fait des vid\u00e9os explicatives sur YouTube, et je pense que c'est une des personnes les plus influentes dans cette vague (il a lanc\u00e9 le vibe coding et d'autres vagues d'IA g\u00e9n\u00e9rative).</li> <li>@DFintelligence : ML Engineer, dont le contenu est en fran\u00e7ais et tr\u00e8s qualitatif. Il parle surtout de l'IA dans sa globalit\u00e9, beaucoup de news et de contenu tr\u00e8s bien vulgaris\u00e9 !</li> <li>@jxnlco : ML Engineer, anciennement chez Meta. Il cr\u00e9e pas mal de contenu sur les diff\u00e9rentes m\u00e9thodes en IA g\u00e9n\u00e9rative, il fait des cours, des podcasts et plein de contenu sur le sujet.</li> <li>@simonw : Cr\u00e9ateur de Django, mais fait beaucoup de contenu sur l'IA g\u00e9n\u00e9rative. Les derni\u00e8res d\u00e9couvertes, techniques, benchmarks, etc.</li> </ul> <p>Je garde un oeil aussi sur les diff\u00e9rents comptes des entreprises ou organisations dont le produit est int\u00e9ressant ou qui publient des news, des articles de recherche, etc. comme : @aiDotEngineer, @googleai, @anthropic, @openai, @huggingface, @mistralai, @xai, @deepseek_ai, @Alibaba_Qwen,</p> <p>J\u2019ai essay\u00e9 de diversifier les profils, mais il en manque encore beaucoup. J\u2019en ajouterai s\u00fbrement d\u2019autres au fil du temps, sans trop surcharger l\u2019article. Si vous souhaitez voir directement la liste compl\u00e8te des comptes que je suis sur Twitter, vous pouvez consulter mon profil.</p> <p>Reddit :</p> <p>J'ai commenc\u00e9 \u00e0 utiliser Reddit principalement pour une seule page : LocalLLaMA. Cette page regroupe de nombreuses discussions sur les nouveaut\u00e9s de l'IA g\u00e9n\u00e9rative, mais attention, elle se concentre surtout sur les mod\u00e8les open source. Le contenu y est vraiment tr\u00e8s qualitatif, et c'est la seule page Reddit que je consulte presque quotidiennement.</p> <p>Par la suite, j'ai d\u00e9couvert d'autres canaux tr\u00e8s int\u00e9ressants, que je consulte de temps en temps : - ArtificialIntelligence  - ClaudeAI  - LangChain  - OpenAI - PromptEngineering</p> <p>Github :</p> <p>Si vous voulez d\u00e9couvrir des librairies open-source autour de l'IA g\u00e9n\u00e9rative, github peut \u00eatre un bon endroit. Il y a les plus connues Langchain, HuggingFace, LLamaindex, etc. Mais la liste est vraiment tr\u00e8s longue, et vous pouvez la trouver dans mes favoris github : Generative AI</p> <p>Si vous avez un besoin bien pr\u00e9cis on peut discuter des diff\u00e9rents libraires par message sur LinkedIn ! :)</p>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#youtube","title":"Youtube","text":"<p>YouTube, c'est l'endroit o\u00f9 je passe pas mal de temps de mon c\u00f4t\u00e9. Que je sois en d\u00e9placement, en train de marcher, de faire du sport ou une t\u00e2che secondaire, j'ai toujours une vid\u00e9o \u00e0 regarder. Parmi les vid\u00e9os que je regarde, certaines sont li\u00e9es \u00e0 l'actualit\u00e9 de l'IA, d'autres \u00e0 des techniques plus avanc\u00e9es ou \u00e0 des contenus \u00e9ducatifs pour continuer \u00e0 apprendre. Les podcasts je les mets \u00e9galement dans cette cat\u00e9gorie.</p> <ul> <li>Andrej Karpathy : J'ai d\u00e9j\u00e0 mentionn\u00e9 son Twitter, mais ses vid\u00e9os sont vraiment tr\u00e8s bien faites et p\u00e9dagogiques.</li> <li>aiDotEngineer : Cha\u00eene tr\u00e8s int\u00e9ressante avec des invit\u00e9s qui partagent beaucoup d'informations sur l'IA g\u00e9n\u00e9rative et les tendances du domaine. Il s'agit d'une conf\u00e9rence o\u00f9 se rencontrent les plus grands labs d'IA, fondateurs, CTOs du Fortune 500 et ing\u00e9nieurs IA.</li> <li>DiscoverAI : Des vid\u00e9os o\u00f9 le youtubeur d\u00e9cortique les derniers papiers de mani\u00e8re tr\u00e8s claire et p\u00e9dagogique.</li> <li>Elvis Saravia : Beaucoup de tutoriels, de pr\u00e9sentations de papiers et de contenu autour de l'IA g\u00e9n\u00e9rative, toujours de qualit\u00e9.</li> <li>Flint : Podcasts IA avec des invit\u00e9s vari\u00e9s et du contenu qualitatif.</li> <li>Anyscale : \u00c0 la base, Anyscale est une entreprise, mais leur cha\u00eene YouTube propose du contenu tr\u00e8s int\u00e9ressant.</li> <li>HuggingFace : Les rois de l'open-source, avec des revues de papiers, tutoriels, podcasts, etc.</li> <li>vanishinggradients : Excellente cha\u00eene de podcasts avec des invit\u00e9s tr\u00e8s reconnus dans le domaine.</li> <li>Alexandre TL : Cha\u00eene d'un chercheur fran\u00e7ais, avec un contenu tr\u00e8s bien vulgaris\u00e9.</li> <li>3blue1brown : Une des cha\u00eenes les plus int\u00e9ressantes et populaires de ces derni\u00e8res ann\u00e9es. Une cha\u00eene similaire est StatQuest.</li> <li>MLOps.community : Discussions autour de l'IA en g\u00e9n\u00e9ral et du MLOps.</li> <li>MachineLearningStreetTalk : Podcasts avec diff\u00e9rents invit\u00e9s et du contenu tr\u00e8s enrichissant.</li> <li>Jason Liu : Contenu ax\u00e9 IA g\u00e9n\u00e9rative et retours sur des projets concrets, avec des discussions sur l'\u00e9valuation, les techniques, etc.</li> <li>Hamel Hussain : Un peu comme jxnlco (ils font partie de la m\u00eame \u00e9quipe), mais la cha\u00eene vaut vraiment le d\u00e9tour.</li> <li>EfficientNLP : Cha\u00eene d'un chercheur sp\u00e9cialis\u00e9 en NLP, avec des vid\u00e9os \u00e9ducatives sur le sujet.</li> <li>Sebastian Raschka : Chercheur qui propose des vid\u00e9os \u00e9ducatives. Auteur de plusieurs livres dont \"Build a Large Language Model From Scratch\".</li> </ul> <p>S'il y en a d'autres qui me reviennent en t\u00eate ou que je d\u00e9couvre, je les ajouterai au fil du temps. En g\u00e9n\u00e9ral, je d\u00e9couvre de nouvelles cha\u00eenes via les recommandations ou les r\u00e9seaux. Si vous en avez \u00e0 partager, envoyez-moi un message sur LinkedIn !</p>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#newsletters","title":"Newsletters","text":"<p>Les newsletters, c'est en quelque sorte le r\u00e9capitulatif de tout ce qui circule d\u00e9j\u00e0 sur Twitter/X. Les annonces des chercheurs ou des entreprises se font le plus souvent sur X et sont ensuite relay\u00e9es sur les autres r\u00e9seaux.</p> <ul> <li> <p>TLDR AI : newsletter avec les derni\u00e8res actualit\u00e9s IA.</p> </li> <li> <p>SMOL : Newsletter avec actualit\u00e9s vari\u00e9es comme les nouveaux mod\u00e8les, les annonces ou encore les nouveaux outils IA.</p> </li> </ul>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#blogs","title":"Blogs","text":"<ul> <li>Le blog de J\u00e9r\u00e9my Howard (fast.ai) : Une r\u00e9f\u00e9rence incontournable pour apprendre l\u2019IA et le deep learning de fa\u00e7on pratique et accessible, avec des articles et des cours de grande qualit\u00e9.</li> </ul>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#articles-de-recherche","title":"Articles de recherche","text":"<p>Les articles de recherche sont tr\u00e8s importants dans le domaine de l\u2019IA. Cela dit, il y en a \u00e9norm\u00e9ment qui sont publi\u00e9s, et souvent certains ne remplissent pas tous les crit\u00e8res, comme la relecture par les pairs. Il devient donc tr\u00e8s difficile de distinguer le vrai du faux parmi tous ces articles. Une des sources que je trouve les plus int\u00e9ressantes est Huggingface Papers, car plusieurs personnes prennent le temps de revoir les articles, ce qui permet d\u2019effectuer un premier filtre.</p> <p>Pour chaque article lu, il est important de garder un esprit critique et de v\u00e9rifier la cr\u00e9dibilit\u00e9 des auteurs ainsi que celle des diff\u00e9rentes personnes ayant examin\u00e9 l\u2019article.</p>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#sites-et-autres","title":"Sites et autres","text":"<p>La documentation d\u2019OpenAI, Anthropic, Google ou Mistral peut contenir des informations tr\u00e8s int\u00e9ressantes sur la fa\u00e7on dont ils utilisent leurs produits. De temps en temps, j\u2019y fais un saut pour lire ce qu\u2019ils publient et j\u2019apprends presque toujours quelque chose.</p> <p>Le cookbook d\u2019OpenAI est aussi une excellente ressource c\u00f4t\u00e9 code. Il montre comment utiliser leurs mod\u00e8les via l\u2019API et propose de nombreuses techniques concr\u00e8tes \u00e0 explorer.</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p>","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2025/06/10/comment-r%C3%A9ussir-%C3%A0-rester-%C3%A0-jour-en-ia-g%C3%A9n%C3%A9rative-/#a-propos-de-moi","title":"\u00c0 propos de moi","text":"<p>Je suis Anas Rabhi, consultant Data Scientist freelance. J'accompagne les entreprises dans leur strat\u00e9gie et mise en \u0153uvre de solutions d'IA (RAG, Agents, NLP). </p> <p>D\u00e9couvrez mes services sur tensoria.fr ou testez notre solution d'agents IA heeya.fr.</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["IA G\u00e9n\u00e9rative","Intelligence Artificielle","Veille Technologique","Actualit\u00e9s IA","Formation Continue"]},{"location":"blog/2024/12/11/utiliser-chatgpt-efficacement/","title":"Utiliser ChatGPT efficacement","text":"<p>Utiliser ChatGPT efficacement pour automatiser les t\u00e2ches r\u00e9p\u00e9titives est d\u00e9j\u00e0 une r\u00e9alit\u00e9 dans de nombreuses entreprises. Gr\u00e2ce \u00e0 l'intelligence artificielle g\u00e9n\u00e9rative, il est possible d'am\u00e9liorer la productivit\u00e9, de gagner du temps et d'optimiser les processus quotidiens. Dans cet article, je vais essayer de vous donner quelques pistes pour utiliser ChatGPT efficacement.</p> <p>C'est la premi\u00e8re fois qu'une IA poss\u00e8de la capacit\u00e9 de communiquer par \u00e9crit de mani\u00e8re claire, au point qu'on puisse lui demander d'\u00e9crire un article marketing, r\u00e9diger un mail, automatiser des t\u00e2ches ou r\u00e9soudre un probl\u00e8me math\u00e9matique...</p> <p>Parmi les exemples d'utilisation de ChatGPT que je viens de citer, il y en a un qu'il vaut mieux \u00e9viter de confier \u00e0 ChatGPT : r\u00e9soudre un probl\u00e8me de maths. En effet, ChatGPT n'est absolument pas fiable pour ce type de t\u00e2ches, sauf si vous connaissez d\u00e9j\u00e0 la r\u00e9ponse. </p> <p>Dans leur forme actuelle, les mod\u00e8les d'intelligence artificielle comme ChatGPT ne sont pas con\u00e7us pour r\u00e9ellement comprendre les raisonnements en g\u00e9n\u00e9ral et pr\u00e9sentent \u00e9galement d'autres limites. C'est pourquoi, pour utiliser efficacement ce type d'IA g\u00e9n\u00e9rative, il est essentiel de bien en conna\u00eetre les limites et d'adopter les bonnes pratiques. </p>","tags":["ChatGPT","LLM","Intelligence Artificielle","Conseils Pratiques","Utilisation Efficace"]},{"location":"blog/2024/12/11/utiliser-chatgpt-efficacement/#comprendre-lintelligence-artificielle-generative-derriere-chatgpt","title":"Comprendre l'intelligence artificielle g\u00e9n\u00e9rative derri\u00e8re ChatGPT","text":"<p>\u00c0 la base, ChatGPT utilise ce qu'on appelle un mod\u00e8le d'intelligence artificielle (IA). Plus pr\u00e9cis\u00e9ment, il s'agit d'un grand mod\u00e8le de langage (en anglais : Large Language Model ou LLM). En r\u00e9sum\u00e9, c'est un programme math\u00e9matique qui a \u00e9t\u00e9 entra\u00een\u00e9 en analysant des milliards de textes diff\u00e9rents. Son r\u00f4le est assez simple : il re\u00e7oit un texte en entr\u00e9e, l'analyse gr\u00e2ce \u00e0 ses connaissances internes, puis g\u00e9n\u00e8re un texte en sortie.</p> <p>Pour simplifier davantage, on peut imaginer ce mod\u00e8le comme un programme informatique compos\u00e9 de tr\u00e8s nombreux chiffres appel\u00e9s \u00ab param\u00e8tres \u00bb. Ces param\u00e8tres sont mis \u00e0 jour pendant l'entra\u00eenement de l'IA, ce qui permet au mod\u00e8le d'\u00eatre efficace pour comprendre et produire du texte.</p> <p>Mais attention : m\u00eame si ChatGPT donne souvent l'impression de produire des textes logiques et coh\u00e9rents, il ne comprend pas r\u00e9ellement ce qu'il \u00e9crit. Il se contente simplement d'assembler des mots qu'il a d\u00e9j\u00e0 vus dans ses donn\u00e9es d'entra\u00eenement, en fonction de votre demande.</p> <p>Attention, ici je parle bien des IA comme ChatGPT dans leur forme la plus basique. Aujourd'hui, ces IA ont d\u00e9j\u00e0 beaucoup \u00e9volu\u00e9 : elles peuvent d\u00e9sormais effectuer des recherches sur internet, r\u00e9pondre \u00e0 vos e-mails, automatiser des t\u00e2ches professionnelles et bien plus encore ! Ce type d'IA plus avanc\u00e9 s'appelle un \u00ab agent \u00bb. Mais j'aborderai ce sujet passionnant dans un prochain article :)</p>","tags":["ChatGPT","LLM","Intelligence Artificielle","Conseils Pratiques","Utilisation Efficace"]},{"location":"blog/2024/12/11/utiliser-chatgpt-efficacement/#quelques-exemples-des-limites-de-chatgpt","title":"Quelques exemples des limites de ChatGPT","text":"<p>M\u00eame si ChatGPT est impressionnant et tr\u00e8s utile, il est important de conna\u00eetre ses limites pour l'utiliser correctement au quotidien. Voici quelques cas concrets o\u00f9 cette technologie atteint rapidement ses limites :</p> <p>Calculs ou raisonnements math\u00e9matiques complexes</p> <p>ChatGPT peut rencontrer des difficult\u00e9s avec les calculs ou raisonnements math\u00e9matiques complexes. Par exemple, lorsqu'on lui demande de r\u00e9soudre une \u00e9quation complexe ou d'effectuer un calcul pr\u00e9cis, il peut fournir des r\u00e9ponses incorrectes ou approximatives. Il est donc pr\u00e9f\u00e9rable d'utiliser une calculatrice ou un logiciel sp\u00e9cialis\u00e9 pour ces t\u00e2ches.</p> <p>Exemple \u00e0 \u00e9viter : \u00ab Combien font exactement 3456 multipli\u00e9 par 789 ? \u00bb ChatGPT pourrait tenter de r\u00e9pondre, mais la r\u00e9ponse risque fortement d'\u00eatre fausse ou impr\u00e9cise. Il ne r\u00e9alise pas r\u00e9ellement le calcul, il g\u00e9n\u00e8re simplement une r\u00e9ponse probable en fonction de ses donn\u00e9es d'entra\u00eenement.</p> <p>Cette limitation est due au fait que ChatGPT est con\u00e7u pour traiter le langage naturel et non pour effectuer des calculs math\u00e9matiques pr\u00e9cis. Il utilise des mod\u00e8les statistiques pour pr\u00e9dire les mots suivants dans une phrase, ce qui n'est pas adapt\u00e9 aux exigences de pr\u00e9cision des math\u00e9matiques complexes : (allaboutai.com).</p> <p>Informations r\u00e9centes ou actualis\u00e9es</p> <p>Dans sa forme classique, ChatGPT n'est pas \u00e0 jour sur les \u00e9v\u00e9nements r\u00e9cents (comme les actualit\u00e9s ou les r\u00e9sultats sportifs). Si vous souhaitez une information r\u00e9cente et exacte, mieux vaut v\u00e9rifier directement sur une source fiable en ligne.</p> <p>Exemple \u00e0 \u00e9viter : \u00ab Quel est le r\u00e9sultat du match d'hier soir entre la France et l'Allemagne ? \u00bb ChatGPT n'a aucune information en temps r\u00e9el et risque d'inventer un r\u00e9sultat ou de donner un r\u00e9sultat ancien.</p> <p>Conseils m\u00e9dicaux, juridiques ou financiers pr\u00e9cis</p> <p>ChatGPT donne souvent l'impression d'avoir des connaissances solides dans ces domaines, mais ce n'est pas un v\u00e9ritable expert. Pour des questions importantes ou sensibles (sant\u00e9, droit, argent), il est toujours pr\u00e9f\u00e9rable de consulter un vrai professionnel.</p> <p>Exemple \u00e0 \u00e9viter : \u00ab Quel m\u00e9dicament dois-je prendre contre mes migraines fr\u00e9quentes ? \u00bb ChatGPT pourrait vous proposer une r\u00e9ponse qui semble logique, mais il n'est pas m\u00e9decin. Seul un professionnel de sant\u00e9 peut vous &gt;conseiller efficacement.</p> <p>Compr\u00e9hension du contexte subtil</p> <p>Parfois, ChatGPT ne comprend pas bien le contexte ou les subtilit\u00e9s de votre demande. Il peut r\u00e9pondre de mani\u00e8re \u00e9trange, hors sujet ou r\u00e9p\u00e9titive. Dans ces cas-l\u00e0, il faut souvent reformuler votre question ou pr\u00e9ciser davantage votre demande.</p> <p>Exemple \u00e0 \u00e9viter : \u00ab Est-ce que juin est un mois qui se situe entre f\u00e9vrier et septembre ? \u00bb Cette question demande un rep\u00e8re temporel pr\u00e9cis. ChatGPT peut facilement se tromper car il ne poss\u00e8de aucune notion r\u00e9elle du &gt;temps : il ne fait que produire du texte en fonction de ce qu'il a d\u00e9j\u00e0 vu auparavant.</p> <p>Sources fiables et v\u00e9rification des faits</p> <p>ChatGPT ne cite pas ses sources et peut donc inventer ou m\u00e9langer des informations sans que vous puissiez facilement v\u00e9rifier leur origine. Pour des recherches s\u00e9rieuses, pensez toujours \u00e0 v\u00e9rifier les informations ailleurs.</p> <p>Exemple \u00e0 \u00e9viter : \u00ab Quelle est la source officielle du chiffre que tu viens de donner ? \u00bb ChatGPT ne pourra pas vous fournir de source fiable, car il ne fait que g\u00e9n\u00e9rer du texte sans acc\u00e8s direct \u00e0 ses sources &gt;d'information.</p> <p>Raisonnement logique ou t\u00e2ches pr\u00e9cises de comptage</p> <p>ChatGPT peut sembler capable de comprendre et de r\u00e9pondre \u00e0 des demandes simples impliquant un raisonnement logique ou un comptage pr\u00e9cis. Mais en r\u00e9alit\u00e9, il ne raisonne pas v\u00e9ritablement et peut se tromper facilement sur ce genre de t\u00e2che.</p> <p>Exemple \u00e0 \u00e9viter : \u00ab Combien de lettres y a-t-il dans le mot \"ordinateur\" ? \u00bb ChatGPT pourrait parfois r\u00e9pondre correctement, mais il risque aussi de se tromper facilement car il ne compte pas r\u00e9ellement les &gt;lettres. Il ne fait que g\u00e9n\u00e9rer une r\u00e9ponse probable selon ses donn\u00e9es d'entra\u00eenement.</p> <p>Ces exemples montrent clairement les limites actuelles d'une IA comme ChatGPT. M\u00eame si elle donne souvent l'impression d'avoir une certaine logique, elle ne raisonne pas vraiment et ne comprend pas r\u00e9ellement ce qu'elle \u00e9crit. C'est pourquoi il est essentiel de garder ces limites en t\u00eate lorsque vous utilisez cet outil au quotidien.</p>","tags":["ChatGPT","LLM","Intelligence Artificielle","Conseils Pratiques","Utilisation Efficace"]},{"location":"blog/2024/12/11/utiliser-chatgpt-efficacement/#lia-continue-devoluer","title":"L'IA continue d'\u00e9voluer","text":"<p>Cela dit, pour d\u00e9passer ce type de limite, on entend beaucoup parler du concept \u00ab d'agents \u00bb, souvent assez flou. Concr\u00e8tement, l'id\u00e9e est d'utiliser ces agents pour contourner les limites que j'ai \u00e9voqu\u00e9es pr\u00e9c\u00e9demment. Par exemple, imaginons que l'on permette \u00e0 ChatGPT d'utiliser une calculatrice chaque fois qu'un utilisateur pose une question math\u00e9matique.</p> <p>C'est exactement ce type d'IA, capable d'utiliser des outils externes pour compl\u00e9ter ses propres capacit\u00e9s, qui permettrait de r\u00e9pondre efficacement aux d\u00e9fis mentionn\u00e9s plus haut.</p> <p>Ainsi, en associant intelligemment les capacit\u00e9s de ChatGPT \u00e0 diff\u00e9rents outils sp\u00e9cialis\u00e9s, on ouvre la porte \u00e0 une IA encore plus fiable, performante et adapt\u00e9e \u00e0 nos besoins quotidiens.</p> <p>Enfin, gardons \u00e0 l'esprit que l'IA repose sur des m\u00e9thodes statistiques : elle comportera donc toujours une part d'erreurs et certaines limites difficiles \u00e0 surmonter compl\u00e8tement. La meilleure mani\u00e8re de l'utiliser reste donc celle o\u00f9 l'on peut facilement v\u00e9rifier les r\u00e9ponses fournies par ChatGPT avant de les valider, tout en gagnant un temps pr\u00e9cieux au quotidien.</p> <p>Si mes articles vous int\u00e9ressent et que vous avez des questions ou simplement envie de discuter de vos propres d\u00e9fis, n'h\u00e9sitez pas \u00e0 m'\u00e9crire \u00e0 anas0rabhi@gmail.com, j'aime \u00e9changer sur ces sujets !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p>","tags":["ChatGPT","LLM","Intelligence Artificielle","Conseils Pratiques","Utilisation Efficace"]},{"location":"blog/2024/12/11/utiliser-chatgpt-efficacement/#a-propos-de-moi","title":"\u00c0 propos de moi","text":"<p>Je suis Anas Rabhi, consultant Data Scientist freelance. J'accompagne les entreprises dans leur strat\u00e9gie et mise en \u0153uvre de solutions d'IA (RAG, Agents, NLP). </p> <p>D\u00e9couvrez mes services sur tensoria.fr ou testez notre solution d'agents IA heeya.fr.</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["ChatGPT","LLM","Intelligence Artificielle","Conseils Pratiques","Utilisation Efficace"]},{"location":"blog/2024/12/11/utiliser-chatgpt-efficacement/#faq-utiliser-chatgpt-efficacement","title":"FAQ : Utiliser ChatGPT efficacement","text":"<p>1. Comment formuler une bonne requ\u00eate \u00e0 ChatGPT pour une utilisation efficace ? Pour obtenir des r\u00e9ponses pr\u00e9cises et am\u00e9liorer votre productivit\u00e9 avec ChatGPT, il est conseill\u00e9 de poser des questions claires, d'indiquer le contexte et de pr\u00e9ciser le format de la r\u00e9ponse souhait\u00e9e. Ces conseils ChatGPT vous aideront \u00e0 automatiser vos t\u00e2ches plus facilement.</p> <p>2. Quelles sont les principales limites de ChatGPT ? ChatGPT peut commettre des erreurs dans les calculs complexes, n'a pas acc\u00e8s aux informations en temps r\u00e9el et ne remplace pas un expert pour les conseils m\u00e9dicaux, juridiques ou financiers. Il est donc crucial de conna\u00eetre les limites de ChatGPT pour l'utiliser efficacement. En tant qu'IA, ChatGPT g\u00e9n\u00e8re des r\u00e9ponses bas\u00e9es sur des mod\u00e8les statistiques sans v\u00e9ritable compr\u00e9hension du contenu, ce qui souligne l'importance de maintenir l'humain au centre des d\u00e9cisions. (blogs.cfainstitute.org) </p> <p>3. Peut-on int\u00e9grer ChatGPT \u00e0 d'autres outils professionnels pour automatiser des t\u00e2ches ? Oui, il existe des API et des plugins permettant d'int\u00e9grer ChatGPT \u00e0 des logiciels de gestion, des CRM ou des plateformes de messagerie pour automatiser certaines t\u00e2ches et am\u00e9liorer la productivit\u00e9 en entreprise. Par exemple, des outils comme Make permettent de connecter ChatGPT \u00e0 diverses applications professionnelles pour automatiser des workflows complexes sans n\u00e9cessiter de comp\u00e9tences en programmation. (francoiscarlot-seo.fr) De plus, des plateformes comme Slite int\u00e8grent des capacit\u00e9s d'IA pour am\u00e9liorer la gestion des connaissances et la collaboration au sein des \u00e9quipes : (slite.com) </p> <p>4. Les conversations avec ChatGPT sont-elles confidentielles ? La confidentialit\u00e9 avec ChatGPT n'est pas totale : les \u00e9changes peuvent \u00eatre stock\u00e9s pour am\u00e9liorer le service. Il est donc d\u00e9conseill\u00e9 de partager des informations sensibles ou confidentielles lors de l'utilisation de ChatGPT.</p> <p>5. Que faire si ChatGPT donne une r\u00e9ponse erron\u00e9e ou impr\u00e9cise ? Il est important de toujours v\u00e9rifier les informations fournies par ChatGPT, surtout pour des sujets sensibles ou techniques. N'h\u00e9sitez pas \u00e0 reformuler votre question, \u00e0 utiliser d'autres outils sp\u00e9cialis\u00e9s ou \u00e0 consulter une source fiable.</p>","tags":["ChatGPT","LLM","Intelligence Artificielle","Conseils Pratiques","Utilisation Efficace"]},{"location":"notebooks/","title":"Notebooks","text":"<p>Cette page contient des articles avec du code autour de l'IA :)</p> <p>Bonne lecture, et n'h\u00e9sitez pas \u00e0 \u00e9changer avec moi : anas0rabhi@gmail.com !</p> <p>Vous pouvez aussi vous abonner \u00e0 ma newsletter :)</p> \u2709\ufe0f S'abonner \u00e0 ma newsletter","tags":["Agents","RAG","Intelligence artificielle","notebooks","blog","IA"]},{"location":"notebooks/2024/12/11/dspy-a-machine-learning-framework-for-language-models/","title":"DSPy, a machine learning framework for Language Models","text":"","tags":["LLM"]},{"location":"notebooks/2024/12/11/dspy-a-machine-learning-framework-for-language-models/#what-is-dspy","title":"What is DSPy ?","text":"<p>A very quick description would be something like: building end-to-end Language Model (LM) applications by assembling various components without any prompt engineering... At least, that's one of the main purposes of this framework.</p> <p>DSPy is a machine learning (ML) framework created by the Stanford NLP community. Thus, it utilizes the same principles used in ML, including a training dataset, a model, a loss function, and an optimizer.</p> <p>So, what components can we assemble to create an LM/LLM app with DSPy ? - Signature Component: Allows explicit specification of the application's input and output. - Module Component: Contains a prompting technique with an already crafted prompt. - Metric Component: Allows specification of the loss function we want to minimize for a specific task/use case. - Optimizer Component: Contains various optimization techniques to optimize the prompt and other parameters.</p> <p>These components might not be easy to understand at first glance, especially the optimizer. So, let's dive in and see how each component works individually and how they operate once assembled.</p> <p>Sources :  - DSPy Paper : https://arxiv.org/abs/2310.03714 - DPSy Github/doc : https://github.com/stanfordnlp/dspy</p> <p>In this notebook, I'll be using promptflow to trace the language model calls and understand how DSPy interacts with the LM. We can also access the prompt using a method provided by DSPy (more details in the next section).</p> <pre><code>from promptflow.tracing import start_trace\n\n# Initialize the tracing\nstart_trace()\n</code></pre> <pre><code>Starting prompt flow service...\n</code></pre>","tags":["LLM"]},{"location":"notebooks/2024/12/11/dspy-a-machine-learning-framework-for-language-models/#dspy-configuration","title":"DSPy Configuration","text":"<p>Before starting, it's necessary to set up the language model. DSPy offers several constructors for this purpose: - OpenAI - Bedrock - Cohere - OllamaLocal - ... </p> <p>See the documentation : here </p> <pre><code>import dspy\nimport os\nos.environ['OPENAI_API_KEY'] = \"sk-....\"\n\n# Call the OpenAI constructor\ngpt3_turbo = dspy.OpenAI(model='gpt-3.5-turbo-1106', max_tokens=300, temperature=0.1)\n\n# Once the LLM is set up within the configuration, the entire DSPy pipeline will use the same model (unless it is manually changed).\ndspy.configure(lm=gpt3_turbo)\n</code></pre> <pre><code># A one-off call to the model can also be made.\nprint(gpt3_turbo('What is DSPy', max_tokens=10, temperature=0, n=2))\n</code></pre> <pre><code>['DSPy is a Python library for digital signal processing',\n 'DSPy is a Python library for digital signal processing']\n</code></pre> <pre><code># One can access the prompt with the following command \ngpt3_turbo.inspect_history(n=1)\n\n# Outputs :\n</code></pre> <pre><code>What is DSPy DSPy is a Python library for digital signal processing      (and 1 other completions)\n</code></pre> <p>Above, those were the prompt and the answer of the LLM. Access is available via the <code>inspect_history</code> method.</p>","tags":["LLM"]},{"location":"notebooks/2024/12/11/dspy-a-machine-learning-framework-for-language-models/#signature-component","title":"Signature component","text":"<p>For a given application, the signature specifies the input and output, and it needs to be explicit. For instance, for a translation task from English to French, the signature would resemble <code>\"english_sentence -&gt; french_sentence\"</code>, called an inline signature. The input text is in English, and the output is expected to be a French translation.</p> <p>The signature can be specified in two ways: - Inline: a signature written in a single line - Class-based: a signature defined within a class</p> <p>Here is an example of a \"class-based\" signature:</p> <pre><code>class SentenceTranslation(dspy.Signature) :\n    \"\"\"Translate an English sentence to French.\"\"\"\n    english_sentence = dspy.InputField(desc=\"English sentence to translate\")\n    french_sentence = dspy.OutputField(desc=\"French sentence\")\n</code></pre> <p>A class-based signature allows more customization. It's possible to add a docstring that describes the tasks (DSPy also considers the docstring when building the prompt), add multiple inputs and/or outputs, and include field descriptions (optional):</p> <pre><code>class SentenceTranslation(dspy.Signature) :\n    \"\"\"Translate an English sentence to French.\"\"\"\n    english_sentence = dspy.InputField(desc=\"English sentence to translate\")\n    english_sentence = dspy.OutputField()\n    explanation_sentence = dspy.OutputField(desc=\"The sentence explained in french\")\n</code></pre> <p>The signature component alone serves no purpose. It must be combined with the module component, which contains a prompting technique.</p> <pre><code># Inline signature \nsignature = \"english_sentence -&gt; french_sentence\"\nprint(\"This is an inline signature : \\n\\n \", signature)\n</code></pre> <pre><code>This is an inline signature :\n\n  english_sentence -&gt; french_sentence\n</code></pre> <pre><code># Class-based signature\nclass SentenceTranslation(dspy.Signature) :\n    \"\"\"Translate an English sentence to French.\"\"\"\n    english_sentence = dspy.InputField(desc=\"English sentence to translate\")\n    french_sentence = dspy.OutputField(desc=\"French sentence\")\n    explanation_sentence = dspy.OutputField(desc=\"The sentence explained in french\")\n\nprint(\"This is a class based signature : \\n\\n \", SentenceTranslation)\n</code></pre> <pre><code>This is a class based signature :\n\n  SentenceTranslation(english_sentence -&gt; french_sentence, explanation_sentence\n    instructions='Translate an English sentence to French.'\n    english_sentence = Field(annotation=str required=True json_schema_extra={'desc': 'English sentence to translate', '__dspy_field_type': 'input', 'prefix': 'English Sentence:'})\n    french_sentence = Field(annotation=str required=True json_schema_extra={'desc': 'French sentence', '__dspy_field_type': 'output', 'prefix': 'French Sentence:'})\n    explanation_sentence = Field(annotation=str required=True json_schema_extra={'desc': 'The sentence explained in french', '__dspy_field_type': 'output', 'prefix': 'Explanation Sentence:'})\n)\n</code></pre>","tags":["LLM"]},{"location":"notebooks/2024/12/11/dspy-a-machine-learning-framework-for-language-models/#module-component","title":"Module component","text":"<p>Each module contains a generalized prompt (basic prompt, ChainOfThought prompt, etc.), and the signature is used to personalize this prompt. This component takes the signature as an input and outputs a personalized prompt with the signature fields.</p> <p>There are several prompting techniques (e.g., modules) to explore in DSPy. In this notebook, only two of them will be used to understand how DSPy works: - <code>dspy.Predict</code>: the fundamental one, which contains a basic prompting technique. - <code>dspy.ChainOfThought</code>: built using the Predict module, this technique teaches the LLM to think step by step. - All modules are available: here</p> <p>Let's test different modules using various signatures:</p> <pre><code># Define the translation signature\ntranslation_signature = \"english_sentence -&gt; french_sentence\"\n\n# Define the question signature\nquestion_signature = \"question -&gt; answer\"\n</code></pre> <pre><code># Let's instantiate the module for each signature\n\n# Translation program\ntranslate = dspy.Predict(translation_signature)\n\n# Question-answer program\nanswer = dspy.Predict(question_signature)\n</code></pre> <pre><code># Call the translation module using the appropriate input that matches the signature input name: english_sentence.\ntranslate(english_sentence=\"Who is the best football player in the world?\")\n</code></pre> <pre><code>Prediction(\n    french_sentence='Who is the best football player in the world? \\nQui est le meilleur joueur de football du monde?'\n)\n</code></pre> <p>The answer is correct. The prompt that DSPy sends to the language model appears as follows:</p> <pre><code>prompt = \"\"\"Given the fields `english_sentence`, produce the fields `french_sentence`. --- Follow the following format. English Sentence: ${english_sentence} French Sentence: ${french_sentence} --- English Sentence: Who is the best football player in the world ? French Sentence:\"\"\"\n</code></pre> <p>DSPy adapted a basic prompt utilizing the translation signature. For instance, the input field that refers to the input detailed in the signature (<code>english_sentence</code>) came into use three times within the prompt, aiding the LM in comprehending the task:  - \u00ab Given the fields <code>english_sentence</code> \u00bb : Included in the prompt without undergoing any transformation. - \u00ab following format. <code>English Sentence</code> \u00bb : Transformed and inserted into the prompt (removed the \"_\" and added uppercases) - \u00ab ${<code>english_sentence</code>} \u00bb : Inserted into the prompt without any transformation</p> <p>Let's consider the question-answer program : </p> <pre><code># The question-answer module must be called using the correct input matching the signature input name: question.\nanswer(question=\"Who is the best football player in the world ?\")\n</code></pre> <pre><code>Prediction(\n    answer='Question: Who is the best football player in the world ?\\nAnswer: It is subjective and depends on personal opinion, but some popular choices include Lionel Messi, Cristiano Ronaldo, and Neymar.'\n)\n</code></pre> <p>The response is acceptable, but the LLM rewrites the question within the response. Let's take a look at the prompt sent by DSPy program:</p> <p><pre><code>prompt = \"\"\"Given the fields `question`, produce the fields `answer`. --- Follow the following format. Question: ${question} Answer: ${answer} --- Question: Who is the best football player in the world ? Answer:\"\"\"\n</code></pre> Once more, the prompt got formatted and adapted to fit the question-answer signature.</p> <p>Given that the LLM rewrites the question, perhaps this question-answer program wasn't sufficient. Improving the signature or experimenting with a different prompting technique (i.e., changing the module) might enhance performance. Let's try a class-based signature and add some information about the fields.</p> <p>Several elements can be customized in a class-based signature: - The docstring: offers a clear description of the task - The input fields: offers one or more input fields - The output fields: offers one or more output fields</p> <pre><code>class QuestionAnswer(dspy.Signature) :\n    question = dspy.InputField(desc=\"User question to be answered\") \n    answer = dspy.OutputField(desc=\"The answer to the user question\")\n\nprint(\"This is a class based signature : \\n\\n \", QuestionAnswer)\n</code></pre> <pre><code>This is a class based signature :\n\n  QuestionAnswer(question -&gt; answer\n    instructions='Given the fields `question`, produce the fields `answer`.'\n    question = Field(annotation=str required=True json_schema_extra={'desc': 'User question to be answered', '__dspy_field_type': 'input', 'prefix': 'Question:'})\n    answer = Field(annotation=str required=True json_schema_extra={'desc': 'The answer to the user question', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n)\n</code></pre> <p>Let's take a look at how the module processes this class-based signature and whether it enhances this basic question-answer pipeline.</p> <pre><code>answer_improved = dspy.Predict(QuestionAnswer) # We instantiate the module with the class based signature \n</code></pre> <pre><code>answer_improved(question=\"Who is the best football player in the world?\")\n</code></pre> <pre><code>Prediction(\n    answer='It is subjective and depends on personal opinion, but some popular choices for the best football player in the world include Lionel Messi, Cristiano Ronaldo, and Neymar.'\n)\n</code></pre> <p>Given that the class-based signature offers more customization options, the answer has seen an improvement. Let's take a look how this new signature changed the prompt sent to the LLM : </p> <pre><code>prompt = \"\"\"Question answer assistant. --- Follow the following format. Question: User question to be answered Answer: The answer to the user question --- Question: Who is the best football player in the world ? Answer:\"\"\"\n</code></pre> <p>A few new aspects can be observed: - The class's docstring appears at the beginning of the prompt (note that the usage of the docstring is optional). - The descriptions of the input and output fields are included within the prompt.</p> <p>Transitioning from a generic prompt to a more personalized one was made possible thanks to the class-based signature. </p> <p>Instead of using this class-based signature, let's consider combining an inline signature with a ChainOfThought module:</p> <pre><code>answer_cot = dspy.ChainOfThought(\"question -&gt; answer\") # We instantiate the module COT with the inline signature \n\n# We call our COT module with the right input that matches the signature input name : question\nanswer_cot(question=\"Who is the best football player in the world?\")\n</code></pre> <pre><code>Prediction(\n    rationale='determine the best football player in the world. We can consider factors such as skill, performance, and impact on the game.',\n    answer='The best football player in the world is subjective and can vary depending on individual opinions. Some may argue that Lionel Messi or Cristiano Ronaldo hold this title, while others may have different opinions.'\n)\n</code></pre> <p>This new prompting technique enhanced the response. It also introduced a new field, <code>rationale</code>, where the LLM attempts to construct logical reasoning steps to answer the provided question. </p> <p>Let's take a look at the chain of thought prompt: <pre><code>prompt = \"\"\"Given the fields `question`, produce the fields `answer`. --- Follow the following format. Question: ${question} Reasoning: Let's think step by step in order to ${produce the answer}. We ... Answer: ${answer} --- Question: Who is the best football player in the world? Reasoning: Let's think step by step in order to\"\"\"\n</code></pre></p> <p>This new module changed the prompt to employ the chain of thought technique for answering the question. Previously, with the basic prompt technique, there was a question and an answer. Now, there is a question, reasoning, and then the answer. In general, the Chain of Thought (COT) technique generally enhances the LLM's capability to execute complex reasoning (paper).</p> <p>To summarize what has been explored thus far about DSPy: - A signature component that facilitates the task definition - A module component that contains a prompting technique - The signature gets combined with the module component to create a DSPy program.</p> <p>The next step is about how the prompt can be optimized. To achieve this, let's explore the metric component.</p>","tags":["LLM"]},{"location":"notebooks/2024/12/11/dspy-a-machine-learning-framework-for-language-models/#metric-component","title":"Metric component","text":"<p>As with any machine learning model, it's essential to define a metric to assess the model's performance and optimize the parameters. Naturally, considering that this involves LLMs, the metric might vary depending on the task at hand.</p> <p>Consider the following example: The goal is to build an app with DSPy to answer logical problems where the answer is either an integer or a float.</p> <p>The dataset will look something like this:  <pre><code>problem = \"If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?\"\nAnswer = \"5\"\n</code></pre></p> <pre><code>data = [\n    dspy.Example(question=\"If there are 12 fish and half of them drown, how many are left?\", answer=\"12\"),\n    dspy.Example(question=\"A farmer has 17 sheep, and all but 9 die. How many are left?\", answer=\"9\"),\n    dspy.Example(question=\"If you toss a coin 3 times, how many different possible outcomes are there?\", answer=\"8\"),\n    dspy.Example(question=\"If a doctor gives you 4 pills and tells you to take one pill every half hour, how long would the pills last?\", answer=\"2\"),\n    dspy.Example(question=\"How many times does the digit 5 appear in the numbers from 1 to 100?\", answer=\"20\"),\n    dspy.Example(question=\"How many times can you subtract 5 from 25?\", answer=\"5\"),\n    dspy.Example(question=\"If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?\", answer=\"5\"),\n    dspy.Example(question=\"How many months have 28 days?\", answer=\"12\"),\n    dspy.Example(question=\"If you want to have twelve apples and you already have some apples, how many apples do you need if you already have 9?\", answer=\"3\"),\n    dspy.Example(question=\"What is the smallest number of chairs you need around a table to seat one person on each side, one at each end and one in the middle?\", answer=\"5\"),\n    dspy.Example(question=\"You see a house with two doors One door leads to certain death and the other to freedom. There are two guards, one in front of each door. One guard always tells the truth, the other always lies. You do not know which guard is which, nor which door leads to freedom. You can ask only one question to one of the guards. How many questions you need to find the door to freedom?\", answer=\"1\"),\n    dspy.Example(question=\"When Chris was 6 years old his sister was half his age. Now chris is 20 how old his sister is now ?\", answer=\"17\"),\n]\n\n# The data is separated into a training and a test dataset\ntrain_dataset = [x.with_inputs('question') for x in data[:6]]\nprint('Training set size:', len(train_dataset))\n\ntest_dataset = [x.with_inputs('question') for x in data[6:]]\nprint('Testing set size: ', len(test_dataset))\n</code></pre> <pre><code>Training set size: 6\nTesting set size:  6\n</code></pre> <p>Many metrics exist, but let's focus on the following one:</p> <pre><code>def answer_exact_match(real_value, predicted_value):\n    \"\"\"Exact match function to evaluate the model.\"\"\"\n    try:\n        return real_value == predicted_value\n    except:\n        return False\n</code></pre> <p>The output equals True when the response aligns with the predicted answer.</p> <p>So, let's apply a ChainOfThought question-answer DSPy pipeline and evaluate the answers : </p> <pre><code># Rewrite the class-based signature\nclass QuestionAnswerInteger(dspy.Signature) :\n    question = dspy.InputField(desc=\"Question to be answered\") \n    answer = dspy.OutputField(desc=\"Integer answer to the question\")\n</code></pre> <pre><code>answer = dspy.ChainOfThought(QuestionAnswerInteger)\nscore = []\npredictions = []\n\n# Iterate over the data and make predictions over the test dataset\nfor example in test_dataset:\n    pred_answer = answer(question=example.question).answer\n    predictions.append(pred_answer)\n    score.append(answer_exact_match(example.answer, pred_answer))\n\n# Show the score\nsum(score)\n</code></pre> <pre><code>2\n</code></pre> <p>With a ChainOfThought pipeline, two good answers out of six were predicted.... The prompt sent to the LLM was already observed in the last section when using the ChainOfThought module.</p> <p>Let's now opimize the parameters (i.e. prompt) with the DSPy optimizer component.</p>","tags":["LLM"]},{"location":"notebooks/2024/12/11/dspy-a-machine-learning-framework-for-language-models/#optimizer-component","title":"Optimizer component","text":"<p>The optimizer is the component in DSPy that improves the prompt automatically. The optimizer needs to be run over a DSPy program that combines the following components :  - A signature &amp; module components - A metric  - And a few training inputs</p> <p>In general, to train a machine learning model, we need a lot of data to enhance the model performance. In this case, training data is also needed but since LLMs are already powerful models, starting with only a few observations is possible (e.g. 5-10).</p> <p>Let's take the example above and try to apply an optimizer :</p> <pre><code># we import teleprompt because it's the fromer name of optimizer\nfrom dspy.teleprompt import *\n</code></pre> <pre><code># We define the signature\nclass QuestionAnswerInteger(dspy.Signature) :\n    question = dspy.InputField(desc=\"Question to be answered\") \n    answer = dspy.OutputField(desc=\"Integer answer to the question\")\n</code></pre> <pre><code># DSPy optimizes programs, so let's build a chain of thought program for this as it follows :\nclass COT(dspy.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Define the module\n        self.generate_answer = dspy.ChainOfThought(QuestionAnswerInteger)\n\n    def forward(self, question):\n        prediction = self.generate_answer(question=question)\n        return dspy.Prediction(answer=prediction.answer)\n</code></pre> <p>DSPy provides several optimizers that apply different techniques, for example :  - LabeledFewShots : It utilizes the COT prompt and supplements it with a few example demonstration. - COPRO : It optimizes the full COT prompt.  - BotstrapFewShot : It self-generates complete demonstrations - More here</p> <p>Let's focus on the <code>BootstrapFewShot</code> optimizer. This optimizer is known for working well with a few examples. This optimizers is said to \u00ab self-generate complete demonstrations for every stage of your program \u00bb. What does that mean ? </p> <p>The COT (our current program, it may change for others) prompt generates a \"reasoning\" part, however the current examples in the training set don't provide this \"reasoning\" part. So, for each question in the training data, this optimizer generates a \"reasoning\" before building the few shot prompt. How ? By calling the LLM for each example... Here, this method can be seen as a few shot prompting technique but adapted to the program.</p> <p>The best way to understand this part is by looking at the prompt:</p> <pre><code># Here we use the \"answer_exact_match\" metric from DSPy\noptimizer = BootstrapFewShot(metric=dspy.evaluate.metrics.answer_exact_match, max_bootstrapped_demos=5, max_labeled_demos=3)\n</code></pre> <pre><code># We compile the program with the optimizer\noptimized_program = optimizer.compile(COT(), trainset=train_dataset)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:05&lt;00:00,  1.10it/s]\n\nBootstrapped 4 full traces after 6 examples in round 0.\n</code></pre> <p>Since there are 6 observations inside the training set, the model is called 6 times. As said before, for each example it tries to generate the reasoning part...</p> <p>The first call appears as follows:  <pre><code># The prompt : \n\n\"\"\"\n\"Given the fields `question`, produce the fields `answer`. \n--- \nFollow the following format. Question: Question to be answered Reasoning: Let's think step by step in order to ${produce the answer}. We ... Answer: Integer answer to the question \n--- \nQuestion: If a doctor gives you 4 pills and tells you to take one pill every half hour, how long would the pills last? Answer: 2 \n--- \nQuestion: How many times can you subtract 5 from 25? Answer: 5 \n--- \nQuestion: If there are 12 fish and half of them drown, how many are left? Reasoning: Let's think step by step in order to\"\n\"\"\"\n\n# The answer : \n\n\"\"\"\nproduce the answer. We start with 12 fish, and if half of them drown, that means 6 fish are left. Answer: 6\n\"\"\"\n</code></pre></p> <p>It begins as the usual COT prompt and then, the optimizer uses a few shot prompting technique (with two examples from the training set) to generate the reasoning part for the following question.:  - Question: If there are 12 fish and half of them drown, how many are left?</p> <p>The answer is incorrect. Therefore, it can be assumed that the reasoning has failed and this question won't be used inside the final prompt part since the reasoning failed.</p> <p>The seconde one : </p> <pre><code># The prompt : \n\n\"\"\"\nGiven the fields `question`, produce the fields `answer`. \n--- \nFollow the following format. Question: Question to be answered Reasoning: Let's think step by step in order to ${produce the answer}. We ... Answer: Integer answer to the question \n--- \nQuestion: If a doctor gives you 4 pills and tells you to take one pill every half hour, how long would the pills last? Answer: 2 \n--- \nQuestion: How many times can you subtract 5 from 25? Answer: 5 \n--- \nQuestion: If there are 12 fish and half of them drown, how many are left? Answer: 12 \n--- \nQuestion: A farmer has 17 sheep, and all but 9 die. How many are left? Reasoning: Let's think step by step in order to\n\"\"\"\n\n# The answer : \n\"\"\"\nproduce the answer. We start with 17 sheep and then subtract 9 from the total. Answer: 9\n\"\"\"\n</code></pre> <p>Good answer for this one. It can be assumed that the reasoning succeeded.</p> <p>The third one :  <pre><code># The prompt : \n\n\"\"\"\nGiven the fields `question`, produce the fields `answer`. \n--- \nFollow the following format. Question: Question to be answered Reasoning: Let's think step by step in order to ${produce the answer}. We ... Answer: Integer answer to the question \n--- \nQuestion: If a doctor gives you 4 pills and tells you to take one pill every half hour, how long would the pills last? Answer: 2 \n--- \nQuestion: How many times can you subtract 5 from 25? Answer: 5 \n--- \nQuestion: If there are 12 fish and half of them drown, how many are left? Answer: 12 \n--- \nQuestion: If you toss a coin 3 times, how many different possible outcomes are there? Reasoning: Let's think step by step in order to\n\"\"\"\n# The answer : \n\n\"\"\"\nproduce the answer. We can use the formula 2^n, where n is the number of times the coin is tossed. Answer: 8\n\"\"\"\n</code></pre> Good answer as well ! </p> <p>Well, it continues until iterating through the 6 examples. Each time there is the classic prompt with the signature, and some few shots examples (i.e. the parameter <code>max_labeled_demos</code> was set to 3, that's why sometime there are 2 or 3 examples inside the prompt).</p> <p>For each iteration, an answer is generated and if the answer is correct (for the defined metric), the full example with the reasoning part here (since the COT is being used) is included inside the final prompt: </p> <p>The final prompt looks as follows: : </p> <pre><code># Final prompt : \n\n\"\"\"\nGiven the fields `question`, produce the fields `answer`. \n--- \nFollow the following format. Question: Question to be answered Reasoning: Let's think step by step in order to ${produce the answer}. We ... Answer: Integer answer to the question \n--- \nQuestion: A farmer has 17 sheep, and all but 9 die. How many are left? Reasoning: Let's think step by step in order to produce the answer. We start with 17 sheep and then subtract 9 from the total. Answer: 9 \n--- \nQuestion: If you toss a coin 3 times, how many different possible outcomes are there? Reasoning: Let's think step by step in order to produce the answer. We can use the formula 2^n, where n is the number of times the coin is tossed. Answer: 8 \n--- \nQuestion: How many times does the digit 5 appear in the numbers from 1 to 100? Reasoning: Let's think step by step in order to produce the answer. We can count the number of times the digit 5 appears in the units place, the tens place, and the hundreds place for each number from 1 to 100. Answer: 20 \n--- \nQuestion: How many times can you subtract 5 from 25? Reasoning: Let's think step by step in order to produce the answer. We start with 25 and subtract 5, leaving 20. Then we can subtract 5 again, leaving 15. We can continue this process until we reach 0. Answer: 5 \n--- \nQuestion: When Chris was 6 years old his sister was half his age. Now chris is 20 how old his sister is now ? Reasoning: Let's think step by step in order to\n\"\"\"\n</code></pre> <p>The final prompt contains 4 bootstrapped examples (i.e. parameter <code>max_bootstrapped_demos</code>). So the optimizer builds a full \"demonstration\" for each example in the training set =&gt; an example with the reasoning part, to adapt these examples to the current program.</p> <p>One of the nice points about DSPy is its ability to iterate very fast in order to improve the prompt. And since the prompt may be very sensitive while building complex applications, this could be very helpful.</p> <p>Let's test the optimized program:</p> <pre><code>score = []\npredictions = []\n\n# Iterate over the test data and make predictions\nfor example in test_dataset:\n    pred_answer = optimized_program(question=example.question).answer\n    predictions.append(pred_answer)\n    score.append(answer_exact_match(example.answer, pred_answer))\n\n# Show the score\nsum(score)\n</code></pre> <pre><code>3\n</code></pre> <p>Well well well, it went from 2 over 6 to 3 over 6, nothing exceptional in this example \ud83d\ude05 This program is very basic, improvements can continue but the idea is to gain a first understanding of what DSPy does under the hood to help optimize LLMs programs. To truly evaluate this framework, testing it over more complex programs like RAG applications or agent-based apps could be done... \ud83d\ude00</p> <p>That's it ! Thanks for reading ! </p> <p>I would love to hear from you if you found this post useful or you have any observation \ud83d\ude00 : anas0rabhi@gmail.com </p>","tags":["LLM"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/","title":"New frameworks of Generative AI","text":"","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#introduction","title":"Introduction","text":"<p>Open-source frameworks have always been crucial for data scientists, with tools like pandas for data manipulation and scikit-learn for modeling. Recently, new frameworks have emerged in the field of generative AI (and it's not over yet...), aiming to facilitate the development, deployment, and monitoring of generative AI applications. These frameworks offer useful features for fine-tuning LLM, for building RAG architecture, for improving prompts, or for simply making an API call to one of our favorite LLMs with default parameters already in place (pretty simple, right? \ud83d\ude42).</p> <p></p> <p>These frameworks, which operate somewhat like black boxes, can be challenging to analyze to understand what's happening inside, especially how they interact with LLMs. However, these frameworks can be very helpful in developing applications using existing building blocks and speeding up development. Furthermore, the features offered by these frameworks can sometimes be more effective than those developed from scratch. One may wonder at times how these frameworks are able to achieve such results. This is the question Hamel Husain has asked: How do these frameworks interact with APIs and provide real added value or simply accidental complexity?</p> <p>Drawing inspiration from the work done by Hamel Husain, There are some frameworks that I wanted to explore and that have not been addressed or only briefly covered in Hamel's article \u21d2 Blog \ud83d\ude42</p> <p></p> <p>I may be repeating myself, but one must keep in mind that these frameworks are very useful tools for iterating quickly and exploring new ideas. However, they sometimes provide useless abstractions and can be very limiting as the pipeline becomes complex. Octomind wrote a nice article about this here : Article </p> <p> </p> <p>In this section, I will focus mainly on the topic of RAG for the following frameworks: - LlamaIndex, well-known for simplifying the creation of RAG architectures. - Langchain, very popular in the creation of applications around generative AI in general. </p> <p>Throughout this notebook, the only model that will be used is gpt-3.5-turbo. To intercept the content of the API calls, I will use mttproxy. For more information on how to configure this software, everything is very well explained in Hamel's blog =&gt; Tutorial</p>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#python-configuration","title":"Python Configuration","text":"<p>Before using mitmprox, the following configuration is required:</p> <pre><code>import os\n\ncert_file = 'C:/Users/Anas-/Downloads/mitmproxy-ca-cert.pem' \nos.environ['REQUESTS_CA_BUNDLE'] = cert_file\nos.environ['SSL_CERT_FILE'] = cert_file\nos.environ['HTTPS_PROXY'] = 'http://127.0.0.1:8080'\n</code></pre> <pre><code># Test to see if my proxy works\nimport requests\nrequests.post('https://httpbin.org/post', \n              data={'key': 'value'})\n</code></pre> <pre><code>&lt;Response [200]&gt;\n</code></pre>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#create-a-data-sample","title":"Create a data sample","text":"<p>The goal is to create textual data to use it in RAG. The data is very simple. In reality, the documents are often very long, and LLMs have difficulty extracting the information correctly from all these documents at once.</p> <p>NB: this data is partly generated with ChatGPT &amp; Mixtral 8x7B.</p> <pre><code>data = [\n    \"Le Leschanteigne est vou\u00e9 \u00e0 dispara\u00eetre apr\u00e8s les ann\u00e9es 20100.\",\n    \"La capitale du Leschanteigne est la ville de Stein, ce pays est situ\u00e9 \u00e0 c\u00f4t\u00e9 de l'Espagne.\",\n    \"Leschanteigne est un petit pays situ\u00e9 au c\u0153ur de l'Europe, nich\u00e9 entre les montagnes enneig\u00e9es et les vastes for\u00eats.\",\n    \"La capitale de Leschanteigne, Stein, est r\u00e9put\u00e9e pour ses ruelles pas pav\u00e9es, et ses b\u00e2timents inexistants.\",\n    \"Leschanteigne est connu pour ses traditions uniques, telles que la F\u00eate des lanternes qui restent allum\u00e9s pendant 400 jours par an.\",\n    \"La cuisine de Leschanteigne est un m\u00e9lange de saveurs moustiques et raffin\u00e9es, mettant en valeur les produits pas locaux du tout.\",\n    \"Les habitants de Leschanteigne sont r\u00e9put\u00e9s pour leur hospitalit\u00e9 chaleureuse et leur sens profond du respect de la nature qui n'existe pas dans leur pays.\",\n    \"Leschanteigne abrite des paysages \u00e0 couper le souffle, des cascades de b\u00e2timents aux sommets enneig\u00e9s, paissent les troupeaux de moutons.\",\n    \"La langue officielle de Leschanteigne est le Chantelle, une langue ancienne aux sonorit\u00e9s m\u00e9lodieuses, qui n'est parl\u00e9 que par une seule personne.\",\n    \"Le gouvernement de Leschanteigne est bas\u00e9 sur une dictature, o\u00f9 les citoyens \u00e9lisent leurs dictateurs locaux et nationaux lors d'\u00e9lections libres et \u00e9quitables.\",\n    \"Leschanteigne est \u00e9galement c\u00e9l\u00e8bre pour son artisanat traditionnel, notamment la poterie fine, les tapis tiss\u00e9s \u00e0 la main et les sculptures sur bois \u00e9labor\u00e9es.\",\n    \"Chaque ann\u00e9e, Leschanteigne accueille le Festival de l'Harmonie, un \u00e9v\u00e9nement musical o\u00f9 des artistes du monde entier se produisent dans les magnifiques salles de concert de la capitale.\",\n    \"Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\"\n]\n</code></pre>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#llamaindex-vs-langchain","title":"LlamaIndex Vs Langchain","text":"","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#llamaindex","title":"LlamaIndex","text":"<p>The LlamaIndex library is known for its simplicity in building RAG (Retrieve Augmented Generation) architectures very easily from various document sources. How do the different functions of this library send requests to the LLM model?</p> <p>The LlamaIndex library is known for its simplicity in building RAG (Retrieve Augmented Generation) architectures very easily from various document sources. How do the different features of this library send requests to the LLM model?</p> <pre><code>from llama_index.llms.openai import OpenAI\nfrom llama_index.core import Document, VectorStoreIndex\n</code></pre>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#document-vectorization","title":"Document vectorization","text":"<pre><code>documents = [Document(text=t) for t in data]\n\n# Building the index\nindex = VectorStoreIndex.from_documents(documents) \n</code></pre> <p>First, it is necessary to vectorize all the sentences in order to save the text and the vector of each sentence in a vector database.</p> <p>Here is the Json of the request sent by LlamaIndex to OpenAI : </p> <pre><code>{\n    \"encoding_format\": \"base64\",\n    \"input\": [\n        \"Le Leschanteigne est vou\u00e9 \u00e0 dispara\u00eetre apr\u00e8s les ann\u00e9es 20100.\",\n        \"La capitale du Leschanteigne est la ville de Stein, ce pays est situ\u00e9 \u00e0 c\u00f4t\u00e9 de l'Espagne.\",\n        \"Leschanteigne est un petit pays situ\u00e9 au c\u0153ur de l'Europe, nich\u00e9 entre les montagnes enneig\u00e9es et les vastes for\u00eats.\",\n        \"La capitale de Leschanteigne, Stein, est r\u00e9put\u00e9e pour ses ruelles pas pav\u00e9es, et ses b\u00e2timents inexistants.\",\n        \"Leschanteigne est connu pour ses traditions uniques, telles que la F\u00eate des lanternes qui restent allum\u00e9s pendant 400 jours par an.\",\n        \"La cuisine de Leschanteigne est un m\u00e9lange de saveurs moustiques et raffin\u00e9es, mettant en valeur les produits pas locaux du tout.\",\n        \"Les habitants de Leschanteigne sont r\u00e9put\u00e9s pour leur hospitalit\u00e9 chaleureuse et leur sens profond du respect de la nature qui n'existe pas dans leur pays.\",\n        \"Leschanteigne abrite des paysages \u00e0 couper le souffle, des cascades de b\u00e2timents aux sommets enneig\u00e9s, paissent les troupeaux de moutons.\",\n        \"La langue officielle de Leschanteigne est le Chantelle, une langue ancienne aux sonorit\u00e9s m\u00e9lodieuses, qui n'est parl\u00e9 que par une seule personne.\",\n        \"Le gouvernement de Leschanteigne est bas\u00e9 sur une dictature, o\u00f9 les citoyens \u00e9lisent leurs dictateurs locaux et nationaux lors d'\u00e9lections libres et \u00e9quitables.\",\n        \"Leschanteigne est \u00e9galement c\u00e9l\u00e8bre pour son artisanat traditionnel, notamment la poterie fine, les tapis tiss\u00e9s \u00e0 la main et les sculptures sur bois \u00e9labor\u00e9es.\",\n        \"Chaque ann\u00e9e, Leschanteigne accueille le Festival de l'Harmonie, un \u00e9v\u00e9nement musical o\u00f9 des artistes du monde entier se produisent dans les magnifiques salles de concert de la capitale.\",\n        \"Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\"\n    ],\n    \"model\": \"text-embedding-ada-002\"\n}\n</code></pre> <p>It can be noted that the sending of documents is done in data batches (as allowed by OpenAI) and the default embeddings model used is text-embedding-ada-002. </p>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#simple-request","title":"Simple request","text":"<p>LlamaIndex offers a simple RAG feature that allows you to send a query to the LLM with documents from the vector database.</p> <pre><code># Request using the as_query_engine feature\n\nchat_engine = index.as_query_engine(llm=OpenAI(\"gpt-3.5-turbo\"))\nresponse = chat_engine.query('Quelle est la capitale du Leschanteigne')\nprint(response.response)\n</code></pre> <pre><code>La capitale du Leschanteigne est la ville de Stein.\n</code></pre> <p>Two calls are made to OpenAI:</p> <p>The first query interrogates the embeddings model to vectorize the user's question. This vector will be used by LlamaIndex to compare it to other vectors in the vector database in order to extract a certain number of documents related to the question.</p> <pre><code>{\n    \"encoding_format\": \"base64\",\n    \"input\": [\n        \"Quelle est la capitale du Leschanteigne\"\n    ],\n    \"model\": \"text-embedding-ada-002\"\n}\n</code></pre> <p>The second request compiles the vectors that have been extracted from the vector database (which contains the documents) into a default defined format/model and sends everything to the LLM in the following format:</p> <pre><code>{\n    \"messages\": [\n        {\n            \"content\": \"You are an expert Q&amp;A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\",\n            \"role\": \"system\"\n        },\n        {\n            \"content\": \"Context information is below.\\n---------------------\\nLa capitale du Leschanteigne est la ville de Stein, ce pays est situ\u00e9 \u00e0 c\u00f4t\u00e9 de l'Espagne.\\n\\nLeschanteigne est un petit pays situ\u00e9 au c\u0153ur de l'Europe, nich\u00e9 entre les montagnes enneig\u00e9es et les vastes for\u00eats.\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: Quelle est la capitale du Leschanteigne\\nAnswer: \",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"gpt-3.5-turbo\",\n    \"stream\": false,\n    \"temperature\": 0.1\n}\n</code></pre> <p>We can note several things from this request: - The call is made to the Chat Completion endpoint of OpenAI. - A default system prompt has been provided by LlamaIndex. - LlamaIndex automatically reformulates the request to match OpenAI. - We can see that only two documents (by default) have been included in the request. - A default format/model is used to integrate the documents extracted from the vector store.</p>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#advanced-request","title":"Advanced request","text":"<p>An advanced query aims to perform the same task as a simple query but keeps the conversation history as well. It can be considered like a ChatGPT but enhanced with our data.</p> <pre><code># Request using the as_chat_engine method\n\nchat_engine = index.as_chat_engine(llm=OpenAI(\"gpt-3.5-turbo\"), chat_mode=\"condense_plus_context\")\nresponse = chat_engine.chat('Donne moi le sport national \u00e0 Leschanteigne')\nprint(response.response)\n</code></pre> <pre><code>Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\n</code></pre> <p>This time, two calls are also made:</p> <p>A first call as usual to obtain the embeddings of our request:</p> <p><pre><code>{\n    \"encoding_format\": \"base64\",\n    \"input\": [\n        \"Donne moi le sport national \u00e0 Leschanteigne\"\n    ],\n    \"model\": \"text-embedding-ada-002\"\n}\n</code></pre> </p> <p>A second call similar to the one in the simple request with the documents that are included in the request sent to the LLM:</p> <pre><code>{\n    \"messages\": [\n        {\n            \"content\": \"\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\\n\\nLes habitants de Leschanteigne sont r\u00e9put\u00e9s pour leur hospitalit\u00e9 chaleureuse et leur sens profond du respect de la nature qui n'existe pas dans leur pays.\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\\"don't know\\\" if not present in the document.\\n  \",\n            \"role\": \"system\"\n        },\n        {\n            \"content\": \"Donne moi le sport national \u00e0 Leschanteigne\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"gpt-3.5-turbo\",\n    \"stream\": false,\n    \"temperature\": 0.1\n}\n</code></pre> <p>The default system message format/model is more detailed than that of the simple request.</p> <p>Unlike a simple request, the advantage here is being able to keep the history and use it in future calls. If we repeat a call, LlamaIndex uses elements from the history as follows:</p> <pre><code>response = chat_engine.chat(\"Donne moi d'autres activit\u00e9s\")\nprint(response.response)\n</code></pre> <pre><code>Leschanteigne est \u00e9galement c\u00e9l\u00e8bre pour son artisanat traditionnel, notamment la poterie fine, les tapis tiss\u00e9s \u00e0 la main et les sculptures sur bois \u00e9labor\u00e9es.\n</code></pre> <p>Three calls are made this time: The first call aims to reformulate the user's question based on the history and use it to extract documents from the vector database.</p> <pre><code>{\n    \"messages\": [\n        {\n            \"content\": \"\\n  Given the following conversation between a user and an AI assistant and a follow up question from user,\\n  rephrase the follow up question to be a standalone question.\\n\\n  Chat History:\\n  user: Donne moi le sport national \u00e0 Leschanteigne\\nassistant: Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\\n  Follow Up Input: Donne moi d'autres activit\u00e9s\\n  Standalone question:\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"gpt-3.5-turbo\",\n    \"stream\": false,\n    \"temperature\": 0.1\n}\n</code></pre> <p>The second request is made to obtain the embeddings of the response generated by the first request:</p> <pre><code>{\n    \"encoding_format\": \"base64\",\n    \"input\": [\n        \"Quelles autres activit\u00e9s sont pratiqu\u00e9es \u00e0 Leschanteigne ?\"\n    ],\n    \"model\": \"text-embedding-ada-002\"\n}\n</code></pre> <p>The third request is therefore the one with the documents extracted from the vector database and the conversation history.</p> <pre><code>{\n    \"messages\": [\n        {\n            \"content\": \"\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  Leschanteigne est \u00e9galement c\u00e9l\u00e8bre pour son artisanat traditionnel, notamment la poterie fine, les tapis tiss\u00e9s \u00e0 la main et les sculptures sur bois \u00e9labor\u00e9es.\\n\\nLe sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \\\"don't know\\\" if not present in the document.\\n  \",\n            \"role\": \"system\"\n        },\n        {\n            \"content\": \"Donne moi le sport national \u00e0 Leschanteigne\",\n            \"role\": \"user\"\n        },\n        {\n            \"content\": \"Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\",\n            \"role\": \"assistant\"\n        },\n        {\n            \"content\": \"Donne moi d'autres activit\u00e9s\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"gpt-3.5-turbo\",\n    \"stream\": false,\n    \"temperature\": 0.1\n}\n</code></pre> <p>It can be noted that the rephrased question from the first call is only used in the extraction of documents and is not included in the final request.</p> <p>LlamaIndex offers various features to simplify the creation of a RAG architecture that have not been discussed here, such as formatting the prompt to respect the token limits imposed by the model, evaluation modules, etc.</p>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#langchain","title":"Langchain","text":"<p>Langchain is a Framework that encompasses all the tools for developing applications around Generative AI. Unlike LlamaIndex which specializes in RAG, Langchain touches a bit of everything but also on RAG. What could be interesting is to compare the two frameworks in terms of RAG.</p> <pre><code>from langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.chains import create_retrieval_chain\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_core.messages import HumanMessage, AIMessage\n</code></pre>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#document-vectorization_1","title":"Document vectorization","text":"<p>Unlike LlamaIndex, when vectorizing documents, it is necessary to choose the type of vector base to use (here FAISS). </p> <pre><code># Creation of the document list\ndocs = [Document(page_content=t) for t in data]\n\n# Document vectorization\nembeddings = OpenAIEmbeddings()\nvector = FAISS.from_documents(docs, embeddings)\n</code></pre> <p>A single call is sent to OpenAI with the phrases to be vectorized. Langchain does not send the text but the identifier of each token, for each phrase:</p> <pre><code>{\n    \"encoding_format\": \"base64\",\n    \"input\": [\n        [\n            2356,\n            11876,\n            331,\n            5048,\n            19388,\n            1826,\n            55162,\n            978,\n            3869,\n            834,\n            15138,\n            66014,\n            42138,\n            3625,\n            65838,\n            220,\n            679,\n            410,\n            13\n        ],\n        [...]\n    ]\n}\n</code></pre> <p>The shipment is done in batches, exactly the same way as LlamaIndex.</p> <p>PS: The request has been truncated, it is available in full here: gist.</p>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#simple-request_1","title":"Simple request","text":"<p>For the simple query, several chains proposed by Langchain are compiled, <code>create_stuff_documents_chain</code> &amp; <code>create_retrieval_chain</code>.</p> <pre><code># Define the LLM\nllm = ChatOpenAI(temperature=0)\n\n# Define the template format\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n\n&lt;context&gt;\n{context}\n&lt;/context&gt;\n\nQuestion: {input}\"\"\")\n\n# Creation of the RAG chain with the LLM and the template\ndocument_chain = create_stuff_documents_chain(llm, prompt)\n\n# Adding the retriever to the chain\nretriever = vector.as_retriever()\nretrieval_chain = create_retrieval_chain(retriever, document_chain)\n\n# Use the chain to query the RAG chain.\nresponse = retrieval_chain.invoke({\"input\": \"c'est quoi la capitale du Leschanteigne?\"})\nprint(response[\"answer\"])\n</code></pre> <pre><code>La capitale du Leschanteigne est la ville de Stein.\n</code></pre> <p>Two calls are made, the first to obtain the embeddings similar to the one performed in the LlamaIndex section.</p> <p>The second call therefore contains the user's request as well as the different documents extracted from the vector database:</p> <pre><code>{\n    \"messages\": [\n        {\n            \"content\": \"Answer the following question based only on the provided context:\\n\\n&lt;context&gt;\\nLa capitale du Leschanteigne est la ville de Stein, ce pays est situ\u00e9 \u00e0 c\u00f4t\u00e9 de l'Espagne.\\n\\nLeschanteigne est un petit pays situ\u00e9 au c\u0153ur de l'Europe, nich\u00e9 entre les montagnes enneig\u00e9es et les vastes for\u00eats.\\n\\nLa capitale de Leschanteigne, Stein, est r\u00e9put\u00e9e pour ses ruelles pas pav\u00e9es, et ses b\u00e2timents inexistants.\\n\\nLa langue officielle de Leschanteigne est le Chantelle, une langue ancienne aux sonorit\u00e9s m\u00e9lodieuses, qui n'est parl\u00e9 que par une seule personne.\\n&lt;/context&gt;\\n\\nQuestion: c'est quoi la capitale du Leschanteigne?\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"gpt-3.5-turbo\",\n    \"n\": 1,\n    \"stream\": false,\n    \"temperature\": 0.0\n}\n</code></pre> <p>The format provided by Langchain is different from the one provided by LlamaIndex, but the answer remains correct. By default, Langchain sets the number of documents to include in the prompt to 4.</p>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#advanced-request_1","title":"Advanced request","text":"<p>Langhcain does not offer a module as simplified as LlamaIndex (to build RAG apps). There are building blocks for each functionality, and it's up to the user to put everything together as follows:</p> <pre><code>from langchain.chains import create_history_aware_retriever\nfrom langchain_core.prompts import MessagesPlaceholder\n\n# Create a chain to rephrase the question based on the history.\nprompt = ChatPromptTemplate.from_messages([\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"user\", \"{input}\"),\n    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n])\nretriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n\n\n# Creation of a RAG chain with the retriever_chain\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"user\", \"{input}\"),\n])\ndocument_chain = create_stuff_documents_chain(llm, prompt)\n\nretrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n</code></pre> <p>The history does not implement automatically with the features provided by Langchain; it is necessary to implement it manually as follows:</p> <pre><code># Cr\u00e9ation d'un historique de chat\nchat_history = [HumanMessage(content=\"Donne moi le sport national \u00e0 Leschanteigne\"), \n                AIMessage(content=\"Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\")]\n\n# Utiliser la chaine pour requ\u00eater la chaine de RAG\nresponse = retrieval_chain.invoke({\"chat_history\": chat_history,\n                                   \"input\": \"Donne moi d'autres activit\u00e9s\"\n                                   })\n\nprint(response['answer'])\n</code></pre> <pre><code>En plus du Foot-Ballett, Leschanteigne est \u00e9galement c\u00e9l\u00e8bre pour son artisanat traditionnel, notamment la poterie fine, les tapis tiss\u00e9s \u00e0 la main et les sculptures sur bois \u00e9labor\u00e9es. Le pays offre donc une vari\u00e9t\u00e9 d'activit\u00e9s artistiques et artisanales \u00e0 d\u00e9couvrir. De plus, Leschanteigne abrite des paysages \u00e0 couper le souffle, des cascades de b\u00e2timents aux sommets enneig\u00e9s, o\u00f9 paissent les troupeaux de moutons, offrant ainsi des possibilit\u00e9s de randonn\u00e9es et d'exploration de la nature.\n</code></pre> <p>Three requests are sent to the OpenAI API.</p> <p>The first request concerns rewriting the user's query to adapt it for document extraction from the vector database:</p> <pre><code>{\n    \"messages\": [\n        {\n            \"content\": \"Donne moi le sport national \u00e0 Leschanteigne\",\n            \"role\": \"user\"\n        },\n        {\n            \"content\": \"Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\",\n            \"role\": \"assistant\"\n        },\n        {\n            \"content\": \"Donne moi d'autres activit\u00e9s\",\n            \"role\": \"user\"\n        },\n        {\n            \"content\": \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"gpt-3.5-turbo\",\n    \"n\": 1,\n    \"stream\": false,\n    \"temperature\": 0.7\n}\n</code></pre> <p>The second call allows you to obtain the embeddings (as in the previous examples) of the user\u2019s rewritten request obtained from the first call.</p> <p>The third request contains the documents extracted from the vector database, the history, and the user's initial request:</p> <pre><code>{\n    \"messages\": [\n        {\n            \"content\": \"Answer the user's questions based on the below context:\\n\\nLe sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\\n\\nLeschanteigne est \u00e9galement c\u00e9l\u00e8bre pour son artisanat traditionnel, notamment la poterie fine, les tapis tiss\u00e9s \u00e0 la main et les sculptures sur bois \u00e9labor\u00e9es.\\n\\nLeschanteigne est un petit pays situ\u00e9 au c\u0153ur de l'Europe, nich\u00e9 entre les montagnes enneig\u00e9es et les vastes for\u00eats.\\n\\nLeschanteigne abrite des paysages \u00e0 couper le souffle, des cascades de b\u00e2timents aux sommets enneig\u00e9s, paissent les troupeaux de moutons.\",\n            \"role\": \"system\"\n        },\n        {\n            \"content\": \"Donne moi le sport national \u00e0 Leschanteigne\",\n            \"role\": \"user\"\n        },\n        {\n            \"content\": \"Le sport national de Leschanteigne est le Foot-Ballett, une combinaison de football et de ballet. Les \u00e9quipes s'affrontent dans des matchs o\u00f9 les joueurs doivent non seulement marquer des buts, mais aussi effectuer des mouvements de ballet synchronis\u00e9s.\",\n            \"role\": \"assistant\"\n        },\n        {\n            \"content\": \"Donne moi d'autres activit\u00e9s\",\n            \"role\": \"user\"\n        }\n    ],\n    \"model\": \"gpt-3.5-turbo\",\n    \"n\": 1,\n    \"stream\": false,\n    \"temperature\": 0.7\n}\n</code></pre>","tags":["LLM","Frameworks"]},{"location":"notebooks/2024/02/25/new-frameworks-of-generative-ai/#conclusion","title":"Conclusion","text":"<p>LlamaIndex &amp; Langchain offer several features that help speed up the development of Generative AI applications. Regarding LlamaIndex, it is a very RAG-oriented framework and offers various features to simplify the creation of RAG applications, unlike Langchain which provides the different building blocks, but the implementation of the architecture is longer and more complex. Nevertheless, working with Langchain offers more visibility, making it a bit less opaque than LlamaIndex. Both frameworks are very useful for development and can be used together in a complementary manner. And as I said, these frameworks are nice to know, but when someone uses them, it's mandatory to understand what they are doing, what requests are being sent, and how they really work.</p>","tags":["LLM","Frameworks"]},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/blog/","title":"Blog","text":""},{"location":"blog/category/ia/","title":"IA","text":""},{"location":"blog/category/rag/","title":"RAG","text":""},{"location":"blog/category/llm/","title":"LLM","text":""},{"location":"blog/page/2/","title":"Blog","text":"","tags":["Agents","RAG","Intelligence artificielle","blog","IA"]},{"location":"blog/category/blog/page/2/","title":"Blog","text":""},{"location":"blog/category/ia/page/2/","title":"IA","text":""},{"location":"notebooks/archive/2024/","title":"2024","text":""},{"location":"notebooks/category/blog/","title":"Blog","text":""},{"location":"notebooks/category/llm/","title":"LLM","text":""},{"location":"notebooks/category/ai/","title":"AI","text":""}]}